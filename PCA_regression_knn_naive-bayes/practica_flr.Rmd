---
title: "Práctica 1 Machine Learning 2023/2024"
subtitle: "Master en Bioinformática, Universidad de Murcia"
author: "Fernando Lucas Ruiz (fernando.lucas@um.es)"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    highlight: kate
    number_sections: no
    theme: spacelab
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Tarea a realizar

En esta práctica se pretenda analizar 3 datasets (BostonHousingData, LetterRecognition, Diabetes en el BRFSS). En cada uno de ellos vamos a responder una serie de preguntas. Se pone en práctica desde la asociación de variables pasando por PCA, el uso de algoritmos básicos como regresión lineal, logística, knn y naive bayes, y preproceso en tres datasets diferentes.

En primer lugar vamos a responder las preguntas del primer dataset BostonHousingData.

## Librerias

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(gridExtra)
library(viridis)  # Para la paleta de colores viridis
library(caret)
library(kknn)
library(pheatmap) # heatmap 
library(REdaS) # Para test esfericidad de Bartlett
library(factoextra) #Gráficas PCA
library(REdaS) # Análisis PCA
library(plotly) # Gráficas PCA 3D
library(fastDummies) # codificación variables dummy

library(doParallel)
num_cores <- detectCores()
registerDoParallel(cores=num_cores)
```

# BostonHousingData

Los datos de este dataset se localizan en el paquete R mlbench.

```{r}
library(mlbench)
data(BostonHousing)
```

## Pregunta 1. ¿Qué tamaño tiene? ¿De qué tipo son las variables?

```{r}
str(BostonHousing)
```

El dataset contiene 506 filas y 14 variables.

Todas las variables son de tipo numérico excepto la variable chas o Charles River dummy variable.

Un primer vistazo a los datos numéricos para ver si tiene nulos, la distribución, búsqueda de variables categóricas, etc

```{r}
Hmisc::describe(BostonHousing)
```

No tiene valores nulos y tienen distribuciones dispares entre variables.


## Pregunta 2. Explica qué representan los ejemplos

Este dataset incluye información sobre diferentes zonas de Boston, detalladas a través de varios parámetros. Cada registro en este dataset representa una zona de Boston, con características que influyen en la estimación del valor medio de las viviendas en esa zona. La variable dependiente es medv, que se refiere al valor medio de las casas habitadas por sus propietarios en miles de dólares.

Las variables del dataset son:

1.  crim: Tasa de criminalidad per cápita por ciudad.
2.  zn: Proporción de terreno residencial zonificado para lotes de más de 25,000 pies cuadrados.
3.  indus: Proporción de acres de negocio no minorista por ciudad.
4.  chas: Variable ficticia Charles River (= 1 si el tramo limita con el río; 0 de lo contrario).
5.  nox: Concentración de óxidos nítricos (partes por 10 millones).
6.  rm: Número promedio de habitaciones por vivienda.
7.  age: Proporción de unidades ocupadas por sus propietarios construidas antes de 1940.
8.  dis: Distancias ponderadas a cinco centros de empleo de Boston.
9.  rad: Índice de accesibilidad a autopistas radiales.
10. tax: Tasa de impuesto a la propiedad de valor total por \$10,000.
11. ptratio: Ratio alumno-profesor por ciudad.
12. black: 1000(Bk - 0.63)/^2 donde Bk es la proporción de personas negras por ciudad.
13. lstat: Porcentaje de población de estatus bajo.
14. medv: Valor mediano de las casas ocupadas por sus propietarios en miles de dólares.

Para ver un poco más en detalle los datos vamos a representarlos en gráficas para ver cómo son sus distribuciones o si podemos sacar alguna conclusión previa.

```{r}
plots <- list()
for (i in 1:14) {
  # Crea una gráfica para cada variable
  p <- ggplot(BostonHousing, aes_string(x = names(BostonHousing)[i])) +
    geom_density(adjust=1.5, alpha=.7, fill="#f1e6b2") +
    theme_minimal() +
    labs(title = names(BostonHousing)[i],
         x = names(BostonHousing)[i]) 

  plots[[colnames(BostonHousing)[i]]] <- p
}

plots$chas <- ggplot(BostonHousing, aes(x=chas)) +
  geom_bar(alpha=.7, fill="#f1e6b2", color = "black")+
  theme_minimal() +
  labs(title = "chas",
       x = "chas")

grid.arrange(plots$crim , plots$zn, plots$indus, plots$nox, ncol=2)
grid.arrange(plots$rm, plots$age, plots$dis, plots$rad, ncol=2)
grid.arrange(plots$tax, plots$ptratio, plots$b, plots$lstat, ncol = 2)
plots$chas
```

1.  Crim: Observamos que la tasa de criminalidad es baja aunque existen zonas en los datos que se llega a tasas muy altas de indice de criminalidad de hasta más de 80, que pueden ser los barrios problemáticos de la ciudad de Boston

2.  Zn: Esta gráfica nos quiere decir que no hay muchas zonas residenciales, aunque hay dos picos intermedios sobre 23 y 80 que puede ser barrios residenciales.

3.  Indus: Hay dos zonas de Boston con negocios de minoristas

4.  Nox: La mayoria de zonas de Boston tienen una densidad de 0.4 y 0.6 ppm de oxido nitrico aunque hay otras menos que llegan hasta 0.9 ppm.

5.  Rm: El número promedio de habitaciones en Boston son de aproximadamente 6 habitaciones por vivienda.

6.  Age: Como vemos, la matoria de casas de Boston están construidas antes 1940. La cola a la izquierda nos puede sugerir que se están construyendo casas nuevas en Boston.

7.  Dis : Esta gráfica nos indica que la mayoría de zonas se encuentran cerca de centros de empleo indicando que podría ser el centro de la ciudad, aunque la cola de la derecha nos indica que puede que los barrios residenciales se encuentren a las afueras de la ciudad.

8.  Rad: Esta gráfica nos muestra que la mayoría de zonas se encuentran bien comunicadas con autopistas, pero hay una cierta densidad de zonas lejanas a autopistas por lo que podría indicar que son zonas residenciales o marginadas.

9.  Tax: Esto nos indica que hay una distribución normal de las tasas excepto unas zonas que pagan más impuestos. Nos sugiere que puede ser una zona residencial de lujo.

10. Ptratio: La mayoría se situa sobre 20 alumnos por profesor. La distribución se ensancha en la cola izquierda porque en colegios privados el ratio suele disminuir.

11. Black: Esta grádica nos puede sugerir que hay pocas zonas en la que la mayoría de personas son de raza caucásica.

12. Lstat: La tasa de personas con estatus bajo ronda el 8% pero la cola de la derecha nos sugiere que hay zonas donde el porcentaje sube por lo que puede ser zonas marginales.

13. Chas: La mayoría de zonas están alejadas del rio Charles

## Pregunta 3. ¿Es un problema de clasificación, regresión o cualquier otro? Explica por qué. Si fuera un problema de clasificación, ¿qué tienes que decir sobre la distribución de los ejemplos de cada clase? Si fuera de regresión, ¿qué tienes que decir de la distribución de valores de la variable dependiente?

En regresión, la variable dependiente o variable objetivo es continua, lo que significa que puede tomar cualquier valor dentro de un rango. En el caso de BostonHousing, la variable medv es un claro ejemplo de una variable continua, ya que el valor de las viviendas puede variar en un amplio espectro, teóricamente sin límites específicos dentro de los rangos observados en los datos.

El objetivo principal de un problema de regresión es predecir valores específicos de la variable dependiente basándose en una o más variables independientes (predictores). En este contexto, estamos interesados en predecir el valor de las viviendas (medv) a partir de otras características del dataset, como la tasa de criminalidad (crim), el número promedio de habitaciones por vivienda (rm), etc.

Los problemas de regresión buscan modelar la relación entre la variable objetivo y las variables predictoras, que puede ser lineal o no lineal. El análisis y modelado de estas relaciones permiten entender cómo las características de las viviendas y sus alrededores afectan su valor de mercado.

Vemos la distribución de los datos medv para tener una imagén inicial de esta variable.

```{r}
p1 <- ggplot(BostonHousing, aes(x = medv)) +
    geom_density(adjust=1.5, alpha=.7, fill="#f1e6b2") +
    theme_minimal() +
    labs(title = "medv", x ="") +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

p2<- ggplot(BostonHousing, aes(x=medv))+
  geom_boxplot(alpha=.7, fill="#f1e6b2")+
    theme_minimal() +
  labs(y ="")

grid.arrange(p1, p2, ncol=1)
```

La mayoría de casas valen alrededor de 20.000 a 23.000 dolares. Vemos algunos casos interesantes que podemos tener en cuenta que hay varias casas que no siguen una tendencia normal ya que hay un pico de unas observaciones cuyos valores son de 50.000 dolares.

Vamos a ver ahora cómo se correlaciona la variable medv con las variables predictoras

```{r, warning=FALSE}
plots <- list()
for (i in 1:13) {
  # Crea una gráfica para cada variable
  p <- ggplot(BostonHousing, aes_string(x = names(BostonHousing)[i], y = names(BostonHousing)[14])) +
  geom_point(color ="#8B1A1A", alpha = 0.7) +
    labs(title = names(BostonHousing)[i],
         x = names(BostonHousing)[i], y = names(BostonHousing)[14]) +
#    scale_color_viridis_c(alpha = 0.7) +
    xlab(names(BostonHousing)[i]) +
    theme_minimal() +
    ylab("medv") +
    guides(color = FALSE) +
    theme(axis.title.x=element_blank(),
        axis.ticks.x=element_blank())

  plots[[colnames(BostonHousing)[i]]] <- p
}

plots$chas <- ggplot(BostonHousing, aes(y = medv, x = chas, fill = chas)) +
  geom_boxplot() +
  geom_jitter(color ="#8B1A1A", size = 1, alpha = 0.7) +
  theme_minimal() +
  scale_fill_viridis_d() + 
  theme(legend.position = "none")

grid.arrange(plots$crim , plots$zn, plots$indus, plots$nox, ncol=2)
grid.arrange(plots$rm, plots$age, plots$dis, plots$rad, ncol=2)
grid.arrange(plots$tax, plots$ptratio, plots$b, plots$lstat, ncol = 2)
plots$chas
```

Un resumen de lo visto en los gráficos de puntos:

1.  Crim: el precio de los barrios con mayor indice de crimininalidad es menor.
2.  zn: Se aprecia una ligera relación positiva en esta variable
3.  indus: A mayor proporcion de negocios no minorista, el precio de la vivienda cae.
4.  nox: Se ve una ligera tenendecia a que cuanto mayor es la cantidad de oxido nitrico menor es el precio de las viviendas. Es decir, las zonas más alejadas de la ciudad tendrán un valor mayor en el mercado.
5.  rm: Se ve una clara correlación positiva entre el número de habitaciones y el precio de la vivienda.
6.  age: Se intuye que en los barrios más modernos el precio es ligeramente superior al casco antiguo.
7.  dis: Las zonas más alejadas de la ciudad parece ser que tienen los precios de viviendas más caros
8.  rad: Parece ser que hay barrios con poca accesibilidad a la autopista y son aquellos con un precio más bajo. Hay algunos barrios donde el precio de la vivienda es muy alto y también tiene poca accesibilidad. Puede ser que sean zonas de residencia de lujo.
9.  tax: Se ve una ligera correlacion negativa en esta variable.
10. ptratio: Las zonas con las viviendas más caras tienen un ratio más bajo de alumno-profesorado.
11. b: No se una relación clara
12. lstat: Esta relación es bastante clara, ya que las personas con estatus bajo viven en zonas con el precio de la vivienda bajo
13. chas: La descompesación de las clases no nos deja dar una conclusión aunque parece que las zonas cercanas al rio Charles aumentan el precio de la vivienda

```{r}
correlations <- cor(BostonHousing[,c(-4)])

cor_df <- data.frame(correlations["medv", ])

cor_df$Variable <- rownames(cor_df)
cor_df <- cor_df %>% filter(Variable != "medv")
names(cor_df)[1] <- "Correlation"
cor_df <- cor_df %>% arrange(desc(Correlation))

cor_df$Variable <- factor(cor_df$Variable, levels = cor_df$Variable[order(cor_df$Correlation, decreasing = TRUE)])

ggplot(cor_df, aes(x = Variable, y = Correlation, fill= Correlation)) +
  geom_col() +
  coord_flip() +
   labs(title = "Correlations with Medv", x = "", y = "Correlation", fill = "medv") +
  scale_fill_viridis_c() 
```

En esta gráfica de correlación vemos que la variable mas correlacionado con la variable medv es aquella variable que mide el numero de personas con estatus bajo (lstat) con una correlación negativa. Luego hay otras como ptratio, indus o tax. La variable con mayor correlación positiva es el número de habitaciones (rm), seguida de la variable con mayor zonas residenciales (zn).

## Pregunta 4. Supón que los posibles sesgos pudieran estar representados por los predictores del dataset. ¿Podríamos determinar visual y numéricamente, con ayuda de PCA si los hay?

El PCA es una técnica estadística que permite reducir la dimensionalidad de los datos, manteniendo al mismo tiempo la mayor cantidad de variación posible. Esto se logra identificando las direcciones (componentes principales) en las cuales los datos varían más. Además de reducir la dimensionalidad, el PCA puede ofrecer insights sobre la estructura subyacente de los datos, incluidos los sesgos potenciales.

Para contrastar si nuestros datos son adecuados para hacer el análisis de componentes principales hay que pasar el test de esferecidad de Bartlett y el test de Kaiser-Meyer-Olkin.

En ambos tests rechazamos la hipotesis nula por lo que nuestros datos son adecuados para realizar el PCA Además el test KMO nos da un criterio de 0.86 que es un criterio alto de adecuación para el PCA.

```{r}
bart_spher(BostonHousing[,c(-4,-14)])
KMOS(BostonHousing[,c(-4,-14)])
```

Para aplicar el PCA de manera correcta, es esencial determinar si nuestros datos requieren un escalado apropiado.

```{r}
datos_long_boston <- pivot_longer(BostonHousing, cols = -chas, names_to = "Variable", values_to = "Valor")

ggplot(datos_long_boston, aes(x = Variable, y = Valor, fill = Variable)) +
  geom_boxplot() +
  geom_jitter(color = "black", size = 0.4, alpha = 0.6) +
  scale_fill_viridis(discrete = TRUE, alpha = 0.6) +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, hjust = 0.5),  
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),  
    axis.text.y = element_text(size = 8),  
    axis.title = element_text(size = 10),  
    axis.title.x = element_text(margin = margin(t = 10), hjust = 0.5),  
    axis.title.y = element_text(margin = margin(r = 10), hjust = 0.5)  
  ) +
  ggtitle("Variables") +
  xlab("Variables") +
  ylab("Count")
```

Se debe prestar atención a variables como "tax" y "b", las cuales se caracterizan por tener valores significativamente mayores en comparación con el resto de las variables en nuestro conjunto de datos. Este desequilibrio en la magnitud de los valores puede influir en el resultado del PCA, haciendo indispensable un ajuste de escala para asegurar una interpretación precisa y equitativa de los componentes principales.

```{r}
pca_result_boston <- prcomp(BostonHousing[,c(-4,-14)], scale = TRUE)
```

```{r}
var_exp <- pca_result_boston$sdev^2
prop_var_exp <- var_exp / sum(var_exp)
cum_var_exp <- cumsum(prop_var_exp)

df_var_exp <- data.frame(Comp = 1:length(prop_var_exp), VarExp = prop_var_exp)
df_cum_var_exp <- data.frame(Comp = 1:length(cum_var_exp), CumVarExp = cum_var_exp)

# Scree plot con los datos no escalados
ggplot(df_var_exp[1:10,], aes(x = Comp, y = VarExp)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_line(aes(group = 1), color = "blue") +
  geom_point(color = "blue") +
  theme_minimal() +
  labs(x = "Principal components", y = "Variance", title = "Scree Plots") +
  ylim(c(0,1)) +
  geom_line(data = df_cum_var_exp[1:10,], aes(x = Comp, y = CumVarExp), color="#8B1A1A") +
  geom_point(data = df_cum_var_exp[1:10,], aes(x = Comp, y = CumVarExp), color = "red") +
  geom_bar(data = df_cum_var_exp[1:10,], aes(x = Comp, y = CumVarExp), stat = "identity", fill = "red", alpha= 0.25) +
  annotate("text", x = 9, y = 0.85, label = "Cumulative Scree Plot", color = "#8B1A1A", size = 4) +
  geom_text(data = df_cum_var_exp[1:10,], aes(x=Comp, y = CumVarExp +0.04, label = round(CumVarExp, 2)))
```

Las cinco componentes principales primeras explican el 85% de la varianza total, lo que nos permite avanzar con este conjunto de datos para investigar la presencia de sesgos mediante un mapa de calor que visualiza las correlaciones. Es notable que la primera componente principal por sí sola aclare más del 50% de la varianza, destacando su significativa contribución a la comprensión de la estructura de los datos.

En el siguiente heatmap vemos la correlación de las componentes principales con las variables.

```{r}
sesgos <- cor(pca_result_boston$x[,1:5], BostonHousing[,c(-4,-14)])

pheatmap(sesgos,
         clustering_method = "ward.D2",
         color = colorRampPalette(c("#00AFBB", "white", "#FC4E07"))(255), # Paleta de colores
         cellwidth = 20,
         cellheight = 20,
         breaks = seq(-1, 1, length.out = 256),
         scale = "none", 
         annotation_legend = TRUE,
         angle_col = 45, cutree_cols = 2)
```

Observamos que la componente principal 1 desempeña un papel fundamental en la diferenciación de los clusters, destacándose como el principal factor de separación. Curiosamente, las variables que presentan una menor correlación con la Componente Principal 1 (b, rm, zn, dis) coinciden con aquellas que muestran una mayor correlación con la variable medv, y viceversa, en el apartado de la pregunta 3 de este documento. Este patrón sugiere que dichas variables son especialmente adecuadas para inferir el precio medio de la vivienda, lo que indica su relevancia y utilidad en el análisis de los factores que determinan el valor medio de las casas en Boston.

```{r, warning=FALSE}
pca <- as.data.frame(pca_result_boston$x[,1:5], stringsAsFactors=F)
pca <- cbind(BostonHousing, pca)

plots <- list()
for (i in 1:14) {
  # Crea una gráfica para cada variable
  p <- ggplot(pca, aes_string(x = names(pca)[15], y = names(pca)[16], color = names(pca)[i])) +
    geom_point() +
    labs(title = names(pca)[i],
         x = names(pca)[15], y = names(pca)[16], color = names(pca)[i]) +
    scale_color_gradient(low = "#D3D3D3", high = "#FC4E07") +
    geom_point() +
    xlab("PC 1 ") +
    ylab("PC 2")+
    theme_minimal()

  plots[[colnames(pca)[i]]] <- p
}

plots$chas <- ggplot(pca, aes(x = PC1, y = PC2, color = chas)) +  
  geom_point() +
  labs(title = "chas", x = "PC 1", y = "PC 2", color = "CHAS") +
  scale_color_manual(values = c("0" = "#D3D3D3", "1" = "#FC4E07")) +  
  theme_minimal()  

grid.arrange(plots$crim , plots$zn, plots$indus, plots$nox, ncol=2)
grid.arrange(plots$rm, plots$age, plots$dis, plots$rad, ncol=2)
grid.arrange(plots$tax, plots$ptratio, plots$b, plots$lstat, ncol = 2)
grid.arrange(plots$chas, plots$medv, ncol=2, nrow=2)
```

Como ya veiamos en el heatmap, hay un sesgo claro en nuestros datos ya que nuestros datos del PCA generan dos clusters grandes.Observamos que seis barrios se distancian significativamente del resto, ubicados en la zona inferior derecha del gráfico.

El cluster con forma redondeada, que podría interpretarse como el núcleo urbano de Boston, se caracteriza por su óptima accesibilidad a las autopistas (rad), un régimen tributario más elevado (tax), una concentración superior de edificaciones antiguas (age), niveles más altos de óxidos de nitrógeno (nox), y una mayor densidad de zonas industriales (indus).

Por contraste, el segundo cluster alargado sugiere una transición hacia la periferia, como lo indica el aumento en la distancia media a centros de empleo (dis). Este patrón se ve reflejado en la reducción progresiva de zonas industriales (indus) hacia las afueras, dando paso a un incremento de áreas residenciales (zn), especialmente evidente en la extensión del cluster elongado. Adicionalmente, la presencia de viviendas con un mayor número promedio de habitaciones (rm) hacia la izquierda del cluster refuerza esta noción de expansión urbana.

Un detalle revelador es la existencia de un barrio céntrico con un porcentaje inusualmente bajo de residentes negros (b). Sin embargo, los datos no proporcionan más características distintivas de este barrio.

La proximidad al río Charles (chas) en el cluster redondeado parece delinear una zona de exclusividad en el corazón de la ciudad, donde los valores inmobiliarios (medv) son sustancialmente más altos y las viviendas ostentan un número mayor de habitaciones en comparación con el resto del centro urbano.

Enfocando la atención en los seis outliers en la esquina inferior derecha, se aprecian índices de criminalidad (crim) más elevados y un porcentaje más alto de personas de estatus socioeconómico bajo (lstat). Estos barrios presentan una marcada diversidad racial ya que, o son exlusivamente personas negras o personas blancas (b), y están situados cerca de las autopistas, caracteristica de barrios marginales por el tráfico de drogas.

Finalmente, al examinar la variable dependiente medv, se deduce que la distancia del núcleo urbano correlaciona con un incremento en el valor de las viviendas, que tienden a ser más espaciosas y situadas en zonas predominantemente residenciales. El sector adyacente al río Charles en el núcleo urbano se destaca por su alto valor inmobiliario. En contraste, las áreas con una tasa de criminalidad más alta tienden a coincidir con los precios de vivienda más bajos, lo que señala una relación inversa entre seguridad y valor inmobiliario.

Vamos a ver ahora la importancia de las variables a cada una de las componentes principales.

```{r}
p1 <- fviz_cos2(pca_result_boston, choice = "var", axes = 1, top= 5, fill = "#00AFBB", ggtheme = theme_minimal()) + labs(y = "Cos2", title = "Dim 1")
p2 <- fviz_cos2(pca_result_boston, choice = "var", axes = 2, top= 5, fill = "#00AFBB", ggtheme = theme_minimal()) + labs(y = "Cos2", title = "Dim 2")
p3 <- fviz_cos2(pca_result_boston, choice = "var", axes = 3, top= 5, fill = "#00AFBB", ggtheme = theme_minimal()) + labs(y = "Cos2", title = "Dim 3")
p4 <- fviz_cos2(pca_result_boston, choice = "var", axes = 4, top= 5, fill = "#00AFBB", ggtheme = theme_minimal()) + labs(y = "Cos2", title = "Dim 4")

grid.arrange(p1, p2, p3, p4, ncol=2)
```

Observamos que la primera y segunda componentes principales tiene varias variables que explican su varianza. Conforme vamos profundizando, la varianza se va explicando por algunas variables específicas.

```{r}
data.frame(pca_result_boston$rotation[,1:4])
```

Examinando los vectores de carga, observamos que la componente 1 parece venir definida por variables relacionadas con el centro urbano. La segunda viene definida por la antigüedad de los edificios. La tercera por el número de habitaciónes por casa, que sabemos que estaba muy relacionada con el precio de las viviendas.

```{r}
fviz_pca_biplot(pca_result_boston,
                geom.ind = "point", 
                geom_var = c("arrow", "text"), 
                col.var = "cos2",
                col.ind = "cos2",
                gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
                repel = F, 
                )
```

Vemos que la componente principal 1 aglomera el 51% de la varianza con valores altos de cos2. Un valor alto de cos2 para una variable en una componente principal indica que la componente explica una gran parte de la varianza de esa variable. En otras palabras, la variable está bien representada en esa dimensión. Esto significa que la dirección y magnitud de la variable tienen una fuerte correlación con la componente principal. Vemos que la variable del número de habitaciones (rm) tiene un cos2 bajo pero como vimos en la gráfica de barras anteriores, es porque viene definida por la componente principal 3.

Similarmente, para los individuos, un alto cos2 en una componente principal sugiere que la posición del individuo en el espacio de las componentes principales está fuertemente determinada por esa componente. Esto puede interpretarse como que el individuo tiene características que son bien capturadas o explicadas por esa componente principal.

## Pregunta 5. ¿Qué algoritmo de entre los que conoces aplicarías según el problema para el que lo has identificado?

Dada la naturaleza de los datos de Bostonhousing, considero que se debe utilizar un algoritmo de regresión para predecir la variable medv con respecto a las demás variables. Debido al tamaño de los datos los mejores algoritmos para esta práctica es algoritmo de k-vecinos más cercanos (KNN).

KNN es un algoritmo no paramétrico y puede capturar relaciones no lineales entre las variables sin una formulación específica de un modelo. KNN puede ser útil para identificar patrones locales y no lineales que la regresión lineal podría pasar por alto, especialmente en áreas donde las propiedades tienen características distintivas que solo se aplican a un vecindario específico.

El algoritmo KNN es intuitivo y fácil de entender ya que predice el valor de una nueva observación promediando los valores de los K vecinos más cercanos. KNN tiene en cuenta la localidad de los datos, lo que podría ser particularmente relevante para datos inmobiliarios donde la ubicación y el vecindario juegan un papel crucial en el precio de las viviendas.


## Pregunta 6. ¿Sería necesario aplicar alguna transformación a los datos? ¿Por qué? Si la respuesta es afirmativa, explica qué tendría de beneficioso hacerlo.

Es importante prestar atención al preprocesamiento de los datos al utilizar KNN. Dado que KNN depende de la distancia entre las observaciones, las variables deben ser normalizadas o estandarizadas para que tengan la misma escala. De lo contrario, las variables con rangos más grandes podrían influir desproporcionadamente en la predicción.

También debemos pasar la variable categórica chas a numérica mediante codificación Dummy. En este caso, como solamente hay dos niveles, sería solamente pasar la variable a numerica entre 0 y 1 sin añadir columnas adicionales.

```{r}
BostonHousing.scaled <- data.frame(scale(BostonHousing[,-4]))
BostonHousing.scaled <- cbind(BostonHousing.scaled, chas = BostonHousing$chas)
BostonHousing.scaled$chas <- as.numeric(BostonHousing.scaled$chas) - 1

datos_long_boston <- pivot_longer(BostonHousing.scaled, cols = -chas, names_to = "Variable", values_to = "Valor")

ggplot(datos_long_boston, aes(x = Variable, y = Valor, fill = Variable)) +
  geom_boxplot() +
  geom_jitter(color = "black", size = 0.4, alpha = 0.6) +
  scale_fill_viridis(discrete = TRUE, alpha = 0.6) +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, hjust = 0.5),  
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),  
    axis.text.y = element_text(size = 8),  
    axis.title = element_text(size = 10),  
    axis.title.x = element_text(margin = margin(t = 10), hjust = 0.5),  
    axis.title.y = element_text(margin = margin(r = 10), hjust = 0.5)  
  ) +
  ggtitle("Variables") +
  xlab("Variables") +
  ylab("Count")

```

## Pregunta 7. Aplica el algoritmo designado en la pregunta 5 y optimiza el estadístico o estadísticos que consideres pertinente para medir su rendimiento mediante la selección de un modelo óptimo. Comenta los resultados

En primer, lugar vamos a generar los datos de entrenamiento y test de los datos de BostonHousing escalados para entrenar el algoritmo de KNN. 

```{r}
set.seed(1234)

boston.TrainIdx.80<- createDataPartition(BostonHousing.scaled$medv,
                                       p=0.8, #Genera un 80% para train, 20% para test
                                       list = FALSE, # resultados en una matriz
                                       times = 1) #Genera solamente una partición 80/20

trainSet.boston <- BostonHousing.scaled[boston.TrainIdx.80, ]
testSet.boston <- BostonHousing.scaled[-boston.TrainIdx.80, ]
```

La función kknn es parte del paquete kknn, que está integrado con caret. Algunos de los hiperparámetros que se pueden ajustar al utilizar la función kknn a través de caret incluyen:

1. kmax: El número de vecinos más cercanos a considerar en la votación o el promedio. Es el hiperparámetro principal del KNN. Un k pequeño puede hacer que el modelo sea sensible al ruido de los datos (sobreajuste), mientras que un k grande puede hacer que el modelo sea demasiado general (subajuste).

2. distance: Define la métrica de distancia a utilizar para calcular la cercanía entre puntos. Por ejemplo, una distancia de 2 se refiere a la distancia euclidiana, mientras que 1 se refiere a la distancia de Manhattan. Distancias fraccionarias también son posibles y pueden ser útiles para capturar estructuras no lineales.

3. kernel: Especifica la función de ponderación a aplicar a los vecinos. Algunas opciones incluyen "rectangular" (todos los vecinos ponderan igual), "triangular", "epanechnikov", "biweight", "triweight", "cos", entre otros. La ponderación puede afectar cómo influyen los vecinos en la predicción final, dando más peso a los vecinos más cercanos.

Es importante destacar que el ajuste de hiperparámetros debe hacerse cuidadosamente. Por ejemplo, seleccionar k puede requerir la implementación de técnicas como la validación cruzada para encontrar un valor que equilibre bien el sesgo y la varianza. La elección de la métrica de distancia y la función de kernel debe basarse en el conocimiento del problema y el dominio de aplicación, así como en la experimentación y validación.

```{r}
modelLookup(("kknn"))
```

Debido a que el numero de muestras que tenemos no es muy grande, he decidido optar por un entrenamiento de validación cruzada repetitiva con 3 repeticiones y 10 particiones. Permito el análisis en paralelo para aumentar el rendimiento

```{r}
set.seed(1234)
kknnControl.boston <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 3,
                           seeds = NULL,
                           returnResamp = "final", 
                           allowParallel = T)
```

En un primer intento de buscar el mejor modelo realizo una parrilla de hiperparámetros con los siguientes valores dando 21 posibilidades combinatorias:
- kmax: 5, 7, 9 , 11
- distance: 1, 2, 3
- kernel: rectangular, triangular, optimal

```{r}
set.seed(1234)
y = trainSet.boston$medv
x = trainSet.boston[,-13]

mygrid.boston = expand.grid(kmax = seq(from=5,to=11,2),
                     distance = seq(1, 3, 1),
                     kernel = c("rectangular","triangular","optimal" 
                                 # ,"epanechnikov", "biweight", "triweight" 
                                 # ,"cos", "inv", "gaussian","rank"
                                ))

myfit.boston.cv10.rp3 <- train(y=y, x = x, 
               method = "kknn",
               metric = "Rsquared",
               trControl = kknnControl.boston,
               tuneGrid=mygrid.boston
               )

(myfit.boston.cv10.rp3.best <- subset(myfit.boston.cv10.rp3$results, kmax == myfit.boston.cv10.rp3$bestTune$kmax & distance == myfit.boston.cv10.rp3$bestTune$distance & kernel == myfit.boston.cv10.rp3$bestTune$kernel))
plot(myfit.boston.cv10.rp3)

```
En este primer modelo vemos que los mejores hiperparámetros son con una kmax de 5, distancia Manhattan de 1 y el kernel optimal. La RMSE y Rsquared dan valores muy buenos para este modelo.

Vamos a probar con otros tipos de kernel para ver si mejoramos el modelo. Los hiperparámetros son:
- kmax: 5, 7, 9 , 11
- distance: 1
- kernel: optimal, epanechnikov, biweight, triweight, cos, gaussian, rank

```{r}
set.seed(1234)

mygrid.boston = expand.grid(kmax = seq(from=5,to=11,3),
                     distance = 1,
                     kernel = c(#"rectangular","triangular", 
                                "optimal"
                                ,"epanechnikov", "biweight", "triweight"
                                 ,"cos", "gaussian","rank"
                                ))

myfit.boston.cv10.rp3.2 <- train(y=y, x = x, 
               method = "kknn",
               metric = "Rsquared",
               trControl = kknnControl.boston,
               tuneGrid=mygrid.boston,
               #preProcess = c("center", "scale")
               )

(myfit.boston.cv10.rp3.2best <- subset(myfit.boston.cv10.rp3.2$results, kmax == myfit.boston.cv10.rp3.2$bestTune$kmax & distance == myfit.boston.cv10.rp3.2$bestTune$distance & kernel == myfit.boston.cv10.rp3.2$bestTune$kernel))
plot(myfit.boston.cv10.rp3.2)
```
El mejor modelo posible lo seguimos encontrando con los hiperparámetros anteriores, por lo que no mejoramos el modelo. Como vemos en las siguiente gráficas, casi todas las iteraciones de nuestro modelo de entrenamiento nos da como resultado unas Rsquared de entorno 0.85 y un RMSE de 0.39 sin grandes cambios.

```{r}
resample <- myfit.boston.cv10.rp3.2$resample
p1 <- ggplot(data = resample, aes(x = RMSE)) +
  geom_density(size = 2,color = "#8B1A1A") +
  theme_minimal() +
  labs(title = "RMSE",
       x = "RMSE")+
  geom_vline(xintercept = median(resample$RMSE),
             linetype = "dashed") +
  annotate("text", x = 0.5, y = 4, 
           label = paste("RMSE", "\n", round(median(resample$RMSE), 2)), color = "#8B1A1A", size = 4) 
  
p2 <- ggplot(data = resample, aes(x = Rsquared)) +
  geom_density(size = 2, color = "#f1e6b2") +
  theme_minimal() +
  labs(title = "Rsquared",
       x = "Rsquared")+
  geom_vline(xintercept = median(resample$Rsquared),
             linetype = "dashed") +
   annotate("text", x = 0.75, y = 6, 
           label = paste("Rsquared", "\n", round(median(resample$Rsquared), 2)), color = "#c0b88e", size = 4) 

grid.arrange(p1, p2, ncol = 2)
```

En la siguiente tabla vemos las variables más importantes para nuestro modelo situando a lstat y rm como las variables más importantes.

```{r}
varImp(myfit.boston.cv10.rp3.2)
```

Vemos que los valores predecidos caen en lugares parecidos a los valores de train y de test sin que haya ningún outlier aparente.

```{r}
preds.boston <- predict(myfit.boston.cv10.rp3.2$finalModel, newdata = testSet.boston)

df.train <- trainSet.boston
df.test <- testSet.boston
df.train$trainORtest <- "train"
df.test$trainORtest <- "test"
boston_df_plot.boston <- rbind(df.train, df.test)

prediction_test.boston <- cbind(testSet.boston, preds.boston)

plots <- list()
for (i in 1:13){
  col_name <- names(boston_df_plot.boston)[i]
  
  p <- ggplot(boston_df_plot.boston, aes_string(x = col_name, y = "medv")) + 
    geom_point(aes(color = trainORtest)) + 
    geom_point(data = prediction_test.boston, aes_string(x = col_name, y = "preds.boston", color = "'Predicciones'"), show.legend = TRUE) +
    scale_color_manual(values = c("train" = "#f1e6b2", "test" = "#b2d8b2", "Predicciones" = "black"),
                     name = "", labels = c("Predicciones", "Test", "Train")) + 
    theme_minimal()
    
    plots[[col_name]] <- p
}

plots$chas <- ggplot(boston_df_plot.boston, aes(y = medv, x = chas, color = trainORtest)) +
  geom_jitter() +
  geom_jitter(data = prediction_test.boston, aes(x = chas, y = preds.boston, color = "Predictions"), show.legend = TRUE) +
    scale_color_manual(values = c("train" = "#f1e6b2", "test" = "#b2d8b2", "Predictions" = "black"),
                     name = "", labels = c("Predictions", "Test", "Train")) + 
    theme_minimal() +
  geom_vline(xintercept = 0.5, color = "#8B1A1A", linetype = "dashed")

grid.arrange(plots$crim , plots$zn, plots$indus, plots$nox, ncol=2)
grid.arrange(plots$rm, plots$age, plots$dis, plots$rad, ncol=2)
grid.arrange(plots$tax, plots$ptratio, plots$b, plots$lstat, ncol = 2)
plots$chas

```

El análisis del error de predicción en relación con los valores reales de la variable medv en el conjunto de test revela una tendencia diferenciada en función del rango de precios de las viviendas. Observamos que para las viviendas con un valor cercano a la mediana de medv, las predicciones son notablemente precisas, reflejando un error bajo. No obstante, para las viviendas con un valor de medv superior al promedio, el modelo tiende a tener un rendimiento menos exacto, resultando en errores de predicción más elevados.

```{r}
testSet.boston %>%
  cbind(preds.boston) %>%
  mutate(RMSE = sqrt((preds.boston - medv)^2)) %>%
  ggplot(aes(x = medv, y = RMSE)) +
  geom_line(color ="#8B1A1A") + 
  geom_point(color ="#c0b88e") + 
  theme_minimal() +
  labs(title = "Predictive error test data", x = "Medv test data")
```

Finalmente, para ver si incurrimos en overfitting o underfitting, calculamos el RMSE de los valores predichos con los datos de entrenamiento y con lo datos de test. Observamos que en los datos de entrenamiento el error es menor que en los datos de test pero la diferencia no es muy grande, siendo el error muy bajo. Encontramos un modelo bueno para predecir el precio de la vivienda con estas variables en la ciudad de Boston.

```{r, warning=FALSE}
preds.boston.train <- predict(myfit.boston.cv10.rp3.2$finalModel, newdata = trainSet.boston)

cat("El error RMSE de los datos de entrenamiento es",sqrt((1/nrow(trainSet.boston)) * sum((trainSet.boston$medv - preds.boston.train)^2)),"\n")

cat("El error RMSE de los datos de evaluación es", sqrt((1/nrow(testSet.boston)) * sum((testSet.boston$medv - preds.boston)^2)),"\n")
```


# LetterRecognition

Los datos de este dataset se localizan en el paquete R mlbench.

```{r}
library(mlbench)
data(LetterRecognition)
```

## Pregunta 1. ¿Qué tamaño tiene? ¿De qué tipo son las variables?

```{r}
str(LetterRecognition)
```

El dataset contiene 20.000 filas y 17 variables.

Todas las variables son de tipo numérico excepto la variable lettr que es la variable dependiente.

Un primer vistazo a los datos numéricos para ver si tiene nulos, la distribución, búsqueda de variables categóricas, etc

```{r}
Hmisc::describe(LetterRecognition)
```

No tiene valores nulos y las distribuciones van de 0 a 15 en todas las variables menos en la variable dependiente lettr.

## Pregunta 2. Explica qué representan los ejemplos

Los datos de LetterRecognition del paquete mlbench en R están diseñados para la tarea de identificar letras del alfabeto inglés en mayúsculas a partir de imágenes en píxeles blanco y negro. Cada imagen de letra se basa en 20 fuentes diferentes, y cada letra de estas fuentes se distorsionó aleatoriamente para producir un conjunto de 20,000 estímulos únicos. Estos estímulos se convirtieron en 16 atributos numéricos primitivos, como momentos estadísticos y recuentos de bordes, que se escalaron para ajustarse a un rango de valores enteros de 0 a 15.

El dataset contiene 20,000 observaciones con 17 variables. La primera variable es categórica y tiene niveles de la A a la Z, representando cada una de las letras del alfabeto. Las 16 variables restantes son numéricas y representan diversas características estadísticas y geométricas de las imágenes de las letras.

## Pregunta 3. ¿Es un problema de clasificación, regresión o cualquier otro? Explica por qué. Si fuera un problema de clasificación, ¿qué tienes que decir sobre la distribución de los ejemplos de cada clase? Si fuera de regresión, ¿qué tienes que decir de la distribución de valores de la variable dependiente?

Este conjunto de datos es un problema de clasificación. La razón es que el objetivo es categorizar cada representación digital de una letra en una de varias clases predefinidas, que son las letras del alfabeto. Por lo tanto, el conjunto de datos es un ejemplo clásico de un problema de clasificación multiclase, donde el modelo de aprendizaje automático debe predecir cuál de las múltiples categorías (lettr) corresponde a las características dadas de una imagen.

Para un problema de clasificación, es crucial examinar la distribución de los ejemplos entre las clases. Una distribución equilibrada de ejemplos asegura que el modelo de aprendizaje automático tenga suficientes datos para aprender las características distintivas de cada clase. Por otro lado, una distribución desequilibrada puede llevar a un modelo sesgado que prefiera las clases con más ejemplos.

A continuación mostramos la distribucione de las frecuencias de cada clase de la variable lettr. Vemos que la distribución es bastante homogénea entre clases ya que ronda las 750 instancias de cada letra.

```{r}
table(LetterRecognition$lettr)
```

```{r}
LetterRecognition %>%
  group_by(lettr) %>%
  summarize(Frequency = n()) %>%
  ungroup() %>%
  ggplot(aes(x = lettr, y = Frequency)) +
  geom_segment(aes(x = lettr, xend = lettr, y = 0, yend = Frequency)) +
  geom_point(size = 5, aes(fill = lettr), shape = 21, stroke = 2, alpha = 0.7) +
  scale_fill_viridis_d() +
  theme_minimal() + 
  guides(fill = FALSE) + 
  ylim(c(0, 1000)) +
  labs(title = "Letters") 
```



## Pregunta 4. Supón que los posibles sesgos pudieran estar representados por los predictores del dataset. ¿Podríamos determinar visual y numéricamente, con ayuda de PCA si los hay?

Para contrastar si nuestros datos son adecuados para hacer el analisis de componentes principales hay que pasar el test de esferecidad de Bartlett y el test de Kaiser-Meyer-Olkin.

En ambos tests rechazamos la hipotesis nula por lo que nuestros datos son adecuados para realizar el PCA Además el test KMO nos da un criterio de 0.68 que es un criterio adecuado para el PCA.

```{r}
bart_spher(LetterRecognition[,-1])
KMOS(LetterRecognition[,-1])
```

Vamos a ver la correlación que hay entre las variables predictoras para ver cómo se agrupan. Vemos que hay 2 grupos de variables muy correlados entre sí por lo que podriamos pensar que son candidatos a ser importantes en distintas componentes principales. Luego hay otro cluster más diverso.

```{r}
correlations <- cor(LetterRecognition[,c(-1)])

pheatmap(correlations,
         clustering_method = "ward.D2",
         color = colorRampPalette(c("#00AFBB", "white", "#FC4E07"))(255), # Paleta de colores
         cellwidth = 12,
         cellheight = 12,
         breaks = seq(-1, 1, length.out = 256),
         scale = "none", 
         annotation_legend = TRUE,
         angle_col = 45, cutree_cols = 3)
```

Para aplicar el PCA de manera correcta, es esencial determinar si nuestros datos requieren un escalado apropiado.

```{r}
datos_long_letter <- pivot_longer(LetterRecognition, cols = -lettr, names_to = "Variable", values_to = "Valor")

ggplot(datos_long_letter, aes(x = Variable, y = Valor, fill = Variable)) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha = 0.6) +
  scale_color_viridis(discrete = TRUE, alpha = 0.6) + 
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, hjust = 0.5),  
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    axis.text.y = element_text(size = 8),
    axis.title = element_text(size = 10),
    axis.title.x = element_text(margin = margin(t = 10), size = 14, hjust = 0.5),
    axis.title.y = element_text(margin = margin(r = 10), size = 14, hjust = 0.5)
  ) +
  ggtitle("Variables") +
  xlab("Variables") +
  ylab("Value")
```

La distribución no es homogénea pero vemos que todos los datos están entre 0 y 15 por lo que no considero escalar los datos ya que todos los valores están restringidos entre esos valores y entiendo que tienen la misma escala.

```{r}
pca_result.letter <- prcomp(LetterRecognition[,-1])
```

```{r}
var_exp <- pca_result.letter$sdev^2
prop_var_exp <- var_exp / sum(var_exp)
cum_var_exp <- cumsum(prop_var_exp)

df_var_exp <- data.frame(Comp = 1:length(prop_var_exp), VarExp = prop_var_exp)
df_cum_var_exp <- data.frame(Comp = 1:length(cum_var_exp), CumVarExp = cum_var_exp)

# Scree plot con los datos no escalados
ggplot(df_var_exp[1:10,], aes(x = Comp, y = VarExp)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_line(aes(group = 1), color = "blue") +
  geom_point(color = "blue") +
  theme_minimal() +
  labs(x = "Principal components", y = "Variance", title = "Scree Plot") +
  ylim(c(0,1)) +
  geom_line(data = df_cum_var_exp[1:10,], aes(x = Comp, y = CumVarExp), color="#8B1A1A") +
  geom_point(data = df_cum_var_exp[1:10,], aes(x = Comp, y = CumVarExp), color = "red") +
  geom_bar(data = df_cum_var_exp[1:10,], aes(x = Comp, y = CumVarExp), stat = "identity", fill = "red", alpha= 0.25) +
  annotate("text", x = 9, y = 0.75, label = "Cumulative Scree Plot", color = "#8B1A1A", size = 4) +
  geom_text(data = df_cum_var_exp[1:10,], aes(x=Comp, y = CumVarExp +0.04, label = round(CumVarExp, 2)))
```

La primera componente solamente explica el 29% de la varianza. Con la segunda se explica hasta un 44% por lo que entendemos que las caracteristicas explicadas de nuestras variables están bien distribuidas y cada una explica algo importante en la clasificación de nuestras clases.

Para ver si hay sesgos claros en nuestros datos vamos a hacer una correlacion de las variables con los componentes principales.

```{r}
sesgos.letter <- cor(pca_result.letter$x[,1:7], LetterRecognition[,-1])

pheatmap(sesgos.letter,
         clustering_method = "ward.D2",
         color = colorRampPalette(c("#00AFBB", "white", "#FC4E07"))(255), # Paleta de colores
         cellwidth = 20,
         cellheight = 20,
         breaks = seq(-1, 1, length.out = 256),
         scale = "none", 
         annotation_legend = TRUE,
         cutree_cols = 3)
```

Cemos que las variables "x.ege", "y.box", "high", "onpix", "x.box" y "width" tienen una correlacion muy fuerte con la componente principal 1. Curiosamente, son las mismas variables que en el anterior mapa de correlación entre variables tenian una correlación muy fuerte entre ellas.

Vamos a ver la distribución de nuestra variables en los dos componentes principales 1 y 2

```{r, warning=FALSE}
pca.letter <- as.data.frame(pca_result.letter$x[,1:7], stringsAsFactors=F)
pca.letter <- cbind(LetterRecognition, pca.letter)

plots <- list()
for (i in 1:17) {
  # Crea una gráfica para cada variable
  p <- ggplot(pca.letter, aes_string(x = names(pca.letter)[18], y = names(pca.letter)[19], color = names(pca.letter)[i])) +
    geom_point() +
    labs(x = names(pca.letter)[18], y = names(pca.letter)[19], color = names(pca.letter)[i]) +
    scale_color_gradient(low = "#D3D3D3", high = "#FC4E07") +
    theme_minimal()

  plots[[colnames(pca.letter)[i]]] <- p
}

plots$lettr <- ggplot(pca.letter, aes(x = PC1, y = PC2, color = lettr)) +
    geom_point() +
    labs(x = "PC 1", y = "PC 2", color = "lettr") +
    theme_minimal()

grid.arrange(plots$x.box , plots$y.box, plots$width, plots$high, ncol=2)
grid.arrange(plots$onpix, plots$x.bar, plots$y.bar, plots$x2bar, ncol=2)
grid.arrange(plots$y2bar, plots$xybar, plots$x2ybr, plots$xy2br, ncol = 2)
grid.arrange(plots$x.ege, plots$xegvy, plots$y.ege, plots$yegvx, ncol = 2)
plots$lettr
```

Como vemos no se ven clusters separables en las dos primeras componentes principales pero sí vemos que cada variable se situa en unas zonas específicas, agrupandose entre ellas y con las variables más relacionadas entre ellas. Por ejemplo las variables "x.box", "y.box", "width" y "high" se encuentran en zonas parecidas ya que se correlacionan entre ellas.

Con respecto a la variable dependiente "lettr" vemos que podemos intuir que cada letra se separa de las otras en estas dos primeras componentes principales. En las siguientes ilustraciones muestro mejor la distribución de cada clase entre las dos primeras componentes principales.

```{r}
unique_letters <- unique(LetterRecognition$lettr)

plots <- list()

for (letter in unique_letters) {
  pca.letter$highlight <- ifelse(pca.letter$lettr == letter, "Highlighted", "Other")
  
  plots[[letter]] <- ggplot(pca.letter, aes(x = PC1, y = PC2)) +
    geom_point(aes(color = highlight), alpha = ifelse(pca.letter$highlight == "Highlighted", 1, 0.05)) +
    scale_color_manual(values = c("Highlighted" = "#FC4E07", "Other" = "grey")) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    geom_vline(xintercept = 0, linetype = "dashed") +
    labs(title = letter) +
    theme_minimal() +
    guides(color = FALSE, alpha = FALSE)
}

grid.arrange(plots$A, plots$B,plots$C,plots$D,plots$E,plots$F, plots$G, plots$H, plots$I,plots$J,ncol = 5)
grid.arrange(plots$K,plots$L, plots$M,plots$N,plots$O,plots$P,plots$Q,plots$R,plots$S,plots$T,ncol = 5)
grid.arrange( plots$U,plots$V,plots$W,plots$X, plots$Z, ncol = 3)

```

Por último, otra forma de ver representada la distribución de las clases de letra pero esta vez entre las 3 primeras componentes principales. Se ve claramente que la clasificación de las clases de letras por medio de estas variables es muy prometedora.

```{r, warning=FALSE}
plot <- plot_ly(data = pca.letter, x = ~PC1, y = ~PC2, z = ~PC3, color = ~lettr, type = 'scatter3d', mode = 'markers')
plot <- plot %>% layout(title = "3D PCA Letter Recongnition", 
                        scene = list(xaxis = list(title = "PC1"),
                                     yaxis = list(title = "PC2"),
                                     zaxis = list(title = "PC3")))

plot
```

La gráfica anterior en 3 dimensiones puede ser explicada por la siguiente en la que mostramos el peso que tiene cada variable en cada dimensión. Por lo que "y.box" tiene el mayor peso a la hora de explicar la varianza de los datos en la componente principal 1. Las variables "x2ybr" y "y.bar" lo son importantes para la componente principal 2. Y las variables "x2bar" y "y2bar" para la componente principal 3.

```{r}
p12 <- fviz_pca_var(pca_result.letter, col.var="cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, axes = c(1,2), label = "var", title="PC1-2") + theme(legend.position = "none")

p23 <- fviz_pca_var(pca_result.letter, col.var="cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, axes = c(2,3), label = "var", title="PC2-3") + theme(legend.position = "none")

grid.arrange(p12, p23, ncol= 2)

```

Ahora lo muestro numéricamente los vectores de carga de cada variable a cada componente principal. También muestro la importancia de varianza explicada en cada componente mediante gráficas de barras de las 3 componentes principales.

```{r}
pca_result.letter$rotation[,1:3]
```

```{r}
p1 <- factoextra::fviz_cos2(pca_result.letter, choice = "var", axes = 1, top= 5, fill = "#00AFBB", ggtheme = theme_minimal()) + labs(y = "Cos2", title = "Dim 1")
p2 <- factoextra::fviz_cos2(pca_result.letter, choice = "var", axes = 2, top= 5, fill = "#00AFBB", ggtheme = theme_minimal())+ labs(y = "Cos2", title = "Dim 2")
p3 <- factoextra::fviz_cos2(pca_result.letter, choice = "var", axes = 3, top= 5, fill = "#00AFBB", ggtheme = theme_minimal())+ labs(y = "Cos2", title = "Dim 3")

grid.arrange(p1, p2, p3, ncol=3)

```

## Pregunta 5. ¿Qué algoritmo de entre los que conoces aplicarías según el problema para el que lo has identificado?

KNN es una elección razonable para el conjunto de datos letterRecognition debido a su simplicidad, su capacidad para manejar clasificaciones multiclase y su utilidad en situaciones donde la clasificación depende de la similitud de las instancias en un espacio de características. Su robustez frente a las variaciones complejas en los datos lo hace adecuado para reconocer patrones visuales, lo cual es esencial en la clasificación de letras basada en atributos visuales.

Además, KNN no hace suposiciones sobre la distribución subyacente de los datos, lo cual es ventajoso en el procesamiento de imágenes, donde las distribuciones pueden ser complicadas y no lineales. La capacidad de KNN para adaptarse a los datos localmente, ajustando el número de vecinos, permite una gran flexibilidad para encontrar estructuras en el espacio de características que son significativas para la clasificación.

Sin embargo, es importante destacar que KNN tiene sus desventajas. La principal es su naturaleza computacionalmente intensiva, especialmente en conjuntos de datos grandes con muchas características, debido a la necesidad de calcular la distancia entre pares para cada instancia de prueba contra todas las instancias de entrenamiento. Además, KNN puede ser sensible a características irrelevantes o redundantes, por lo que un buen preprocesamiento y posiblemente la selección de características son pasos cruciales para su rendimiento óptimo.


## Pregunta 6. ¿Sería necesario aplicar alguna transformación a los datos? ¿Por qué? Si la respuesta es afirmativa, explica qué tendría de beneficioso hacerlo.

En este caso, los datos fueron escalados previamente para situarlos en un rango entre 0 y 15, por lo que no consideraría escalar los datos. 

Por otro lado, podría reducir la dimensionalidad a 7 u 8 dimensiones ya que explican más del 80% de la varianza de los datos en el caso de que la capacidad computacional se viese comprometida. En este caso, no voy a reducir la dimensionalidad y voy a seguir con los datos sin transformar.

## Pregunta 7. Aplica el algoritmo designado en la pregunta 5 y optimiza el estadístico o estadísticos que consideres pertinente para medir su rendimiento mediante la selección de un modelo óptimo. Comenta los resultados

En primer, lugar vamos a generar los datos de entrenamiento y test de los datos de letterRecognition para entrenar el algoritmo de KNN. Vemos que el porcentaje de cada letra en entrenamiento y test se mantiene.

```{r}
set.seed(1234)
letterRecognition.TrainIdx.80<- createDataPartition(LetterRecognition$lettr,
                                       p=0.8, #Genera un 80% para train, 20% para test
                                       list = FALSE, #Dame los resultados en una matriz
                                       times = 1) #Genera solamente una partición 80/20

trainSet.letter <- LetterRecognition[letterRecognition.TrainIdx.80, ]
testSet.letter <- LetterRecognition[-letterRecognition.TrainIdx.80, ]

table(trainSet.letter$lettr)
table(testSet.letter$lettr)
```

En el apartado anterior de los datos BostonHousing explico la información del paquete kknn de caret y los hiperparámetros a utilizar (kmax, distance y kernel).

```{r}
modelLookup(("kknn"))
```

Para este modelo he elegido entrenar los datos con validación cruzada sin repetición, ya que la dimensionalidad de estos datos es bastante grande y a nivel computacional es más costoso. Permito el análisis en paralelo para aumentar el rendimiento

```{r}
set.seed(1234)
kknnControl.letter<- trainControl(method = "cv",
                           number = 10 ,
                           returnResamp = "final",
                           seeds = NULL,
                           allowParallel = T)
```

En un primer intento de buscar el mejor modelo realizo una parrilla de hiperparámetros con los siguientes valores dando 9 posibilidades combinatorias:
- kmax: 5, 7, 9 
- distance: 1, 2, 3
- kernel: optimal

```{r}
set.seed(1234)
y <- trainSet.letter$lettr
x <- trainSet.letter[,-1]

mygrid.letter <- expand.grid(kmax = seq(5,9,2),
                      distance = c(1,2,3),
                      kernel = c(#"rectangular","triangular",
                                 "optimal" 
                                 # ,"epanechnikov", "biweight", "triweight" 
                                 # ,"cos", "inv", "gaussian","rank"
                                ))

myfit.letter.cv10 <- train(y=y, x=x, 
               method = "kknn",
               metric = "Kappa",
               trControl = kknnControl.letter,
               tuneGrid=mygrid.letter
               )

(myfit.letter.cv10.best <- subset(myfit.letter.cv10$results, kmax == myfit.letter.cv10$bestTune$kmax & distance == myfit.letter.cv10$bestTune$distance & kernel == myfit.letter.cv10$bestTune$kernel))
plot(myfit.letter.cv10)

```

En este primer modelo vemos que los mejores hiperparámetros son con una kmax de 9, distancia Manhattan de 1 y el kernel optimal. La Accuracy (0.95) y Kappa (0.95) dan valores muy buenos para este modelo.

Vamos a probar con kmax más altos ya que parece que la tendencia del kmax es a subir. Tambien probar otros tipos de kernel para ver si mejoramos el modelo. Los hiperparámetros son:
- kmax: 9, 10, 11
- distance: 1
- kernel: rectangular, triangular, optimal

```{r}
mygrid.letter <- expand.grid(kmax = seq(9,11,1),
                      distance = 1,
                      kernel = c("rectangular","triangular",
                                 "optimal" 
                                 # ,"epanechnikov", "biweight", "triweight" 
                                 # ,"cos", "inv", "gaussian","rank"
                                ))

myfit.letter.cv10.2 <- train(y=y, x=x, 
               method = "kknn",
               metric = "Kappa",
               trControl = kknnControl.letter,
               tuneGrid=mygrid.letter
               )

(myfit.letter.cv10.2.best <- subset(myfit.letter.cv10.2$results, kmax == myfit.letter.cv10.2$bestTune$kmax & distance == myfit.letter.cv10.2$bestTune$distance & kernel == myfit.letter.cv10.2$bestTune$kernel))
plot(myfit.letter.cv10.2)
```

En este segundo modelo vemos que los mejores hiperparámetros sigue siendo una kmax de 9 y el kernel triangula. La Accuracy (0.96) y Kappa (0.96) dan valores muy buenos para este modelo, mejorando los valores anteriores.

Vamos a probar con kmax estable en 9 pero con otros tipos de kernel para ver si mejoramos el modelo. Los hiperparámetros son:
- kmax: 9,
- distance: 1
- kernel: triangular, epanechnikov, biweight, triweight, cos, inv, gaussian, rank

```{r}
mygrid.letter <- expand.grid(kmax = 9,
                      distance = 1,
                      kernel = c(#"rectangular",
                                 "triangular"
                                 #,"optimal" 
                                  ,"epanechnikov", "biweight", "triweight" 
                                  ,"cos", "inv", "gaussian","rank"
                                ))

myfit.letter.cv10.3 <- train(y=y, x=x, 
               method = "kknn",
               metric = "Kappa",
               trControl = kknnControl.letter,
               tuneGrid=mygrid.letter
               )

(myfit.letter.cv10.3.best <- subset(myfit.letter.cv10.3$results, kmax == myfit.letter.cv10.3$bestTune$kmax & distance == myfit.letter.cv10.3$bestTune$distance & kernel == myfit.letter.cv10.3$bestTune$kernel))
plot(myfit.letter.cv10.3)
```

Seguimos viendo que otros kernel no mejoramos el modelo así que el mejor modelo para este dataset es con una kmax de 9, distancia Manhattan de 1 y kernel triangular.

Como vemos en las siguiente gráficas, casi todas las iteraciones de nuestro mejor modelo de entrenamiento nos da como resultado un accuracy de entorno 0.96 y un Kappa de 0.96. Estos valores son muy altos y muy buenos, por lo que deberiamos mirar si nuestro modelo ha caido en overfitting.
 
```{r}
resample <- myfit.letter.cv10.3$resample
p1 <- ggplot(data = resample, aes(x = Accuracy)) +
  geom_density(size = 2,color = "#8B1A1A") +
  theme_minimal() +
  labs(title = "Accuracy",
       x = "Accuracy")+
  geom_vline(xintercept = median(resample$Accuracy),
             linetype = "dashed") +
  annotate("text", x = 0.95, y = 4, 
           label = paste("Accuracy", "\n", round(median(resample$Accuracy), 3)), color = "#8B1A1A", size = 4)  +
  xlim(c(0.93, 0.97))
  
p2 <- ggplot(data = resample, aes(x = Kappa)) +
  geom_density(size = 2, color = "#f1e6b2") +
  theme_minimal() +
  labs(title = "Kappa",
       x = "Kappa")+
  geom_vline(xintercept = median(resample$Kappa),
             linetype = "dashed") +
   annotate("text", x = 0.95, y = 6, 
           label = paste("Kappa", "\n", round(median(resample$Kappa), 3)), color = "#c0b88e", size = 4) +
  xlim(c(0, 1))

grid.arrange(p1, p2, ncol = 2)
```

En la siguiente tabla vemos las variables más importantes para nuestro modelo para cada una de las letras de nuestra variable dependiente lettr.

```{r}
varImp(myfit.letter.cv10.3)
```

Vamos ahora a predecir las clases de los datos que tenemos de test para este dataset que hemos generado anteriormente. Despues de eso vamos mostrar la información de la matriz de confusión de estos resultados.

Una matriz de confusión es una herramienta poderosa utilizada en el campo del aprendizaje automático para evaluar el rendimiento de los modelos de clasificación. Es una tabla específica que permite la visualización del desempeño de un algoritmo de clasificación, mostrando de manera explícita cuándo las clases son confundidas por el modelo durante la predicción. 

La matriz de confusión compara las etiquetas predichas por el modelo contra las etiquetas reales (verdaderas) que se encuentran en el conjunto de datos de prueba. La matriz de confusión permite calcular métricas importantes como la precisión, la sensibilidad, la especificidad, y el valor predictivo. Tambien ayuda a entender los tipos de errores que está cometiendo el modelo (por ejemplo, si está prediciendo falsos positivos más a menudo que falsos negativos).

```{r}
preds.letter <- predict(myfit.letter.cv10.3$finalModel, newdata = testSet.letter)

conf.letter <- confusionMatrix(preds.letter, testSet.letter$lettr)
conf.letter
```

La accuracy global del modelo es del 95.64%, indicando un alto nivel de precisión en la clasificación de las instancias en sus respectivas categorías. El valor de Kappa es de 0.9546, lo que indica una muy buena concordancia entre las predicciones del modelo y los valores reales, más allá de lo que se esperaría por casualidad.

Tambien cada clase muestra valores altos de sensibilidad (la capacidad del modelo para identificar correctamente los verdaderos positivos) y especificidad (la capacidad del modelo para identificar correctamente los verdaderos negativos), lo que demuestra que el modelo es capaz de clasificar correctamente la mayoría de las instancias en todas las categorías.

Aunque el modelo es super robusto y comete muy pocos errores, vamos a ver, mediante el siguiente heatmap, cuáles son los fallos que comete el modelo.

```{r}
conf_matrix_long <- as.data.frame(conf.letter$table)

ggplot(data = conf_matrix_long, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  scale_fill_gradient2(low = "white", high = "#FC4E07", mid ="#00AFBB", midpoint = 5, limit = c(0, 10)) +
  theme_minimal() +
  labs(x = "Predicted", y = "Reference", fill = "Count") +
  geom_text(aes(x=Reference, y = Prediction, label=ifelse(Freq > 0, paste(Reference, Prediction), "")), size = 2)+ coord_flip()


```

Lo que vemos en el heatmap son los fallos cometimos de nuestro modelo. Por ejemplo, ha cometido casi 10 fallos al predecir una letra P en lugar de una F. Otro fallo que ha cometido alguna que otra vez es predecir una Q en lugar de una O. 

Aun con estos pocos fallos, nuestro modelo predice perfectamente con este modelo de entrenamiento las distintas clases. Vamos a ver a continuación sin este modelo ha incurrido en overfitting.

```{r}
# Evaluar el rendimiento en el conjunto de entrenamiento
trainPredictions <- predict(myfit.letter.cv10.3$finalModel, trainSet.letter)
conf_train <- confusionMatrix(trainPredictions, trainSet.letter$lettr)

# Evaluar el rendimiento en el conjunto de prueba
testPredictions <- predict(myfit.letter.cv10.3$finalModel, testSet.letter)
conf_test <- confusionMatrix(testPredictions, testSet.letter$lettr)

conf_train$overall
conf_test$overall


```

Al calcular el accuracy de los valores predichos con los datos de entrenamiento y el accuracy de los datos de test, observamos que en los datos de entrenamiento la precisión es mayor que en los datos de test pero la diferencia no es muy grande, siendo el accuracy también muy alto. Encontramos un modelo muy bueno para predecir el tipo de letra según las variables recogidas en este dataset.

# Diabetes BRFSS

Los datos de este dataset se localizan en el archivo csv "diabetesBRFSS2015.csv".

```{r}
diabetesBRFSS2015 <- read.csv("diabetesBRFSS2015.csv")
```

## Pregunta 1. ¿Qué tamaño tiene? ¿De qué tipo son las variables?

```{r}
str(diabetesBRFSS2015)
```

Es un dataset con 70,692 muestras y 22 variables casi todas numéricas aunque es un problema de la carga de los datos. Paso las variables categoricas a factorizarlas.

```{r}
diabetesBRFSS2015[] <- lapply(diabetesBRFSS2015, as.factor)
diabetesBRFSS2015$BMI <- as.numeric(diabetesBRFSS2015$BMI)
diabetesBRFSS2015$MentHlth <- as.numeric(diabetesBRFSS2015$MentHlth)
diabetesBRFSS2015$PhysHlth <- as.numeric(diabetesBRFSS2015$PhysHlth)
diabetesBRFSS2015$Diabetes_binary <- factor(diabetesBRFSS2015$Diabetes_binary, levels= c(0,1), labels = c("Healthy", "Diabetic"))

Hmisc::describe(diabetesBRFSS2015)
```

No hay datos nulos en nuestro dataset. Hay algunas variables categóricas bastante balanceadas pero otras no. Parece que hay algunas variables con varias clases e incluso otras que pueden ser numéricas.

## Pregunta 2. Explica qué representan los ejemplos

Los datos del dataset de diabetesBRFSS2015 representa información de encuestas sobre estilos de vida de personas en general, incluyendo su diagnóstico de diabetes. Cada instancia en este conjunto de datos representa a una persona que participó en este estudio. El objetivo principal de este conjunto de datos es entender mejor la relación entre el estilo de vida y la diabetes en EE.UU.

El conjunto de datos incluye 22 variables que abarcan desde información demográfica hasta resultados de exámenes de laboratorio y respuestas a preguntas de encuestas. Aquí hay un resumen de algunas de las variables clave:

1. Diabetes_binary: Variable objetivo binaria que indica si el paciente tiene diabetes (incluyendo prediabetes) o no.
2. HighBP: Presión arterial alta, indicada binariamente 0 = no 1 = si.
3. HighChol: Colesterol alto, también indicado de manera binaria 0 = no 1 = si.
4. CholCheck: Indica si el paciente ha tenido un chequeo de colesterol en los últimos 5 años 0 = no 1 = si.
5. BMI: Índice de masa corporal, una medida cuantitativa.
6. Smoker: Indica si el paciente ha fumado al menos 100 cigarrillos en su vida 0 = no 1 = si.
7. Stroke: Indica si al paciente le han diagnosticado un derrame cerebral 0 = no 1 = si.
8. HeartDiseaseorAttack: Presencia de enfermedad coronaria o ataque cardíaco, indicado binariamente 0 = no 1 = si.
9. PhysActivity: Actividad física en los últimos 30 días, excluyendo el trabajo 0 = no 1 = si, indicado binariamente.
10. Fruits: Consumo diario de frutas, indicado binariamente 0 = no 1 = si.
11. Veggies: Consumo diario verduras, indicado binariamente 0 = no 1 = si.
12. HvyAlcoholConsump: Consumo excesivo de alcohol 0 = no 1 = si.
13. AnyHealthcare: Cobertura de salud de cualquier tipo 0 = no 1 = si.
14. NoDocbcCost: Si hubo un momento en los últimos 12 meses en que el paciente necesitaba ver a un doctor pero no pudo debido al costo 0 = no 1 = si.
15. GenHlth: Estado general de salud, medido en una escala de 1 a 5, 1 = excelente 2 = muy buena 3 = buena 4 = poca 5 = muy poca. 
16. MentHlth:  Salud mental indicada por el número de días en los últimos 30 días que no fueron buenos.
17. PhysHlth: Salud física indicada por el número de días en los últimos 30 días que no fueron buenos.
18. DiffWalk: Dificultad para caminar o subir escaleras. 0 = no 1 = si.
19. Sex: Género del paciente. 0 = mujer 1 = hombre
20. Age: Edad, categorizada en 13 niveles de edad. 1 = 18-24, 9 = 60-64, 13 = 80 o mayores.
21. Education: Nivel de educación, en una escala de 1 a 6, 1 = Nunca fue a la escuela, 2 = Grados 1 a 8 (Elemental), 3 = Grados 9 a 11 (Algo de instituto) 4 = Grados 12 or graduación (Graduado de instituto) 5 = Universidad de 1 a 3 años (Asistió a la universidad) 6 = Universidad 4 años o más (Graduado universitario).
22. Income: Ingresos, categorizados en una escala de 1 a 8, 1 = menos de 10,000 USD, 5 = menos de 35,000 USD, 8 = 75,000 USD o mas.

Estas variables ofrecen una visión integral de los factores de riesgo, condiciones de salud y estilos de vida de los participantes, permitiendo realizar análisis detallados sobre la relación entre estos factores y la diabetes.

<https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators>

## Pregunta 3. ¿Es un problema de clasificación, regresión o cualquier otro? Explica por qué. Si fuera un problema de clasificación, ¿qué tienes que decir sobre la distribución de los ejemplos de cada clase? Si fuera de regresión, ¿qué tienes que decir de la distribución de valores de la variable dependiente?

El problema planteado en este conjunto de datos se enmarca claramente como un problema de clasificación. Esto se debe a que la variable objetivo, Diabetes_binary, es categórica y denota la presencia o ausencia de diabetes en los participantes. En los problemas de clasificación, el objetivo es predecir una etiqueta de clase para cada instancia en el conjunto de datos basándose en las características observadas. La distinción entre tener o no tener diabetes representa una clasificación fundamental entre dos estados de salud distintos, lo cual es un objetivo común en el análisis de datos de salud.

Vamos a ver la distribución de los datos de la variable dependiente diabetes_binary.

```{r}
n<-nrow(diabetesBRFSS2015)

diabetesBRFSS2015 %>%
  group_by(Diabetes_binary) %>%
  summarize(Frequency = n()/n *100) %>%
  ungroup() %>%
  ggplot(aes(x = Diabetes_binary, y = Frequency, fill =Diabetes_binary)) +
  geom_col(position = position_dodge(), alpha = 0.7) +
  scale_fill_viridis_d() +
  theme_minimal() +
  guides(fill = FALSE) +
  ylim(c(0,100)) +
  xlab("")+ ylab("Percentage %") + ggtitle("Percentage of Diabetic and Healthy people")
```

El 50% de nuestros datos son personas sanas y el otro 50% son personas diabéticas. Esto es algo muy importante para tener un modelo robusto con una alta capacidad predictiva.

Vamos a estudiar las variables categóricas con respecto a la variable dependiente para observar los datos y para ver si vemos alguna tendencia.

```{r}
data_long <- pivot_longer(diabetesBRFSS2015[,c(-5, -15, -16, -17, -20, -21, -22)], cols = -c(Diabetes_binary), 
                          names_to = "Variable", values_to = "Value")

plots <- list()

for(variable in unique(data_long$Variable)) {
  data_long$Diabetes_binary <- factor(data_long$Diabetes_binary)
  
  data_filtered <- data_long %>%
    filter(Variable == variable) %>%
    group_by(Value, Diabetes_binary) %>%
    summarize(Frequency = n(), .groups = 'drop') %>%
    mutate(Percentage = Frequency / sum(Frequency) * 100) %>%
    ungroup()
  
  p <- ggplot(data_filtered, aes(x = Diabetes_binary, y = Percentage, fill = Value)) +
    geom_col(position = position_dodge(), alpha = 0.7) +
    scale_fill_viridis_d() +
    theme_minimal() +
    labs(title = variable,
         x = "", y = "Percentage") +
    theme(legend.position = "none") +
    theme(plot.title = element_text(size = 10), axis.title = element_text(size = 8))
  
  plots[[variable]] <- p
}

grid.arrange(plots$HighBP, plots$HighChol, plots$CholCheck, plots$Smoker, plots$Stroke, plots$HeartDiseaseorAttack, ncol=3)

grid.arrange(plots$PhysActivity, plots$Fruits, plots$Veggies, plots$HvyAlcoholConsump, plots$AnyHealthcare, plots$NoDocbcCost, ncol=3)

grid.arrange(plots$DiffWalk, plots$Sex, ncol=3)

```

En estas graficas de barras podemos concluir que casi todas las variables pueden estar muy relacionadas con pacientes con diabetes. Algunas influyen a tener a diabetes como: HighBP, HighChol, CholCheck, Smoker, Stroke, HeartDiseaseorAttack, DiffWalk y Sex (hombre más propenso). Con esto podemos decir que unos tener una mala salud como una presión arterial alta, colestererol o ser fumador influyen en tener diabetes. El ictus y el ataque cardiaco podría estar relacionado con las variables anteriores por lo que son tambien variables susceptibles de ser predictoras de tener diabetes. En el caso de los chequeo del colesterol en los ultimos 5 años aumenta en pacientes diabeticos. Esto es debido que al ser diagnosticados estos pacientes requieren un mayor seguimiento por parte del personal sanitario.

Hay otros casos que las variables influyen en no tener diabetes como: PhysHlth, Fruits, Veggies. Es decir que hacer ejercicio fisico, comer frutas y verduras ayudan a evitar tener diabetes.

Hay tres variables que no varían mucho entre pacientes sanos y diabéticos que son  HvyAlcoholConsump, AnyHealthcare y NoDocbcCost. 

Vamos a mirar ahora las variables con varias clases.

```{r}
# Incluye Diabetes_binary en el pivot_longer, si es necesario
data_long <- pivot_longer(diabetesBRFSS2015[,c(1,15, 20, 21, 22)], cols = -c(Diabetes_binary), 
                          names_to = "Variable", values_to = "Value")

plots <- list()

for(variable in unique(data_long$Variable)) {
  # Asegura que Diabetes_binary sea factor si aún no lo es
  data_long$Diabetes_binary <- factor(data_long$Diabetes_binary)
  
  # Calcula el porcentaje de Diabetes_binary por cada Value en la variable actual
  data_filtered <- data_long %>%
    filter(Variable == variable) %>%
    group_by(Value, Diabetes_binary) %>%
    summarize(Frequency = n(), .groups = 'drop') %>%
    mutate(Percentage = Frequency / sum(Frequency) * 100) %>%
    ungroup()
  
  # Crea el gráfico
  p <- ggplot(data_filtered, aes(x = Value, y = Percentage, fill = Diabetes_binary)) +
    geom_col(position = position_dodge(), alpha = 0.7) +
    scale_fill_manual(values = c("Healthy" = "#00AFBB", "Diabetic" = "#FC4E07")) +
    theme_minimal() +
    labs(title = variable,
         x = "", y = "Percentage") +
    theme(legend.position = "none") +
    theme(axis.title = element_text(size = 9))
  
  # Guarda el gráfico en la lista
  plots[[variable]] <- p
}


grid.arrange(plots$GenHlth, plots$Age, plots$Education, plots$Income, ncol=2)
```

Aquí ya vemos algunas distribuciones interesantes. En estos gráficos vemos en color azul los pacientes sanos y en rojo los pacientes diabéticos. 

En general las personas sanas se sienten mejor que los pacientes diabéticos ya que el número 1 en la escala es estado excelente. Con respecto a la edad, vemos que la mayoría de personas entrevistadas para este estudio se situan entre los 50 y 72 años (7-11). La diabetes puede clasificarse principalmente en dos tipos: Diabetes de tipo 1  y diabetes de tipo 2, cada una de ellas con diferentes edades típicas de aparición. La diabetes de tipo 1, menos frecuente, suele diagnosticarse en niños, adolescentes o adultos jóvenes, pero puede aparecer a cualquier edad. Por otro lado, la diabetes de tipo 2 es más frecuente en adultos mayores de 45 años, aunque cada vez se diagnostica en personas más jóvenes, incluidos adolescentes y adultos jóvenes, debido al aumento de las tasas de obesidad. Parece lógica esta distribución de los datos ya que el mayor número de muestras se situan en esta franja crítica de aparición de la enfermedad, aunque para saber si la variable edad es un factor de riesgo, se debería tener el mismo número de muestras a cada edad. Considero por tanto que esta variable contiene un sesgo importante para nuestro modelo.

Otro sesgo importante que veo en estos datos, se muestra en las gráficas de distribución de eduacation e income. Vemos que la mayoría de las personas encuestadas para este estudio son personas con alto status socioeconómico ya que tienen altos niveles de estudios y un porcentaje alto de ingresos económicos, sobre todo en la franja de más de 75.000 USD. Al preguntar a este tipo de personas, otras variables se ven muy sesgadas a la hora de optimizar nuestro modelo como por ejemplo: NoDocbcCost (no poder costearse un médico), AnyHealthcare (tenencia de seguro médico) o PhysActivity (actividad física). 

Vamos a estudiar ahora las variables continuas del dataset.

```{r}
p1 <- ggplot(data=diabetesBRFSS2015, aes(x=BMI, group=Diabetes_binary, fill=Diabetes_binary)) +
  geom_boxplot(alpha=.4) +
  scale_fill_viridis_d()+
  theme_minimal()+
  theme(legend.position = "none",
        axis.text.y=element_blank(), 
        axis.ticks.y=element_blank()) 

p2 <- ggplot(data=diabetesBRFSS2015, aes(x=MentHlth, group=Diabetes_binary, fill=Diabetes_binary)) +
    geom_boxplot(alpha=.4) +
   scale_fill_viridis_d()+
  theme_minimal()+ 
  xlab("MentHlth (days)") +
  theme(axis.text.y=element_blank(), 
        axis.ticks.y=element_blank()) 

p3 <- ggplot(data=diabetesBRFSS2015, aes(x=PhysHlth, group=Diabetes_binary, fill=Diabetes_binary)) +
    geom_boxplot(alpha=.4) +
   scale_fill_viridis_d()+
  theme_minimal()+ 
  xlab("PhysHlth (days)")+
    theme(legend.position = "none",
          axis.text.y=element_blank(), 
        axis.ticks.y=element_blank()) 

grid.arrange(p1, p2, p3, ncol = 1)
```


## Pregunta 4. Supón que los posibles sesgos pudieran estar representados por los predictores del dataset. ¿Podríamos determinar visual y numéricamente, con ayuda de PCA si los hay?

En el caso de este dataset hay 19 de 22 variables categóricas por lo que no considero que sea buena idea hacer una PCA para estos datos.

## Pregunta 5. ¿Qué algoritmo de entre los que conoces aplicarías según el problema para el que lo has identificado?

Para el dataset diabetesBRFSS2015, que hemos identificado como adecuado para problemas de clasificación, específicamente para determinar la presencia de diabetes, el algoritmo Naive Bayes puede ser una opción atractiva para empezar el modelado. 

El algoritmo Naive Bayes es un clasificador probabilístico basado en el teorema de Bayes con la "ingenua" suposición de independencia entre las características. Es especialmente popular en tareas de clasificación de texto pero también puede ser efectivo en datasets de salud como este.

Naive Bayes es conocido por ser un modelo simple y rápido para entrenar lo que lo hace atractivo para datasets grandes o para una exploración inicial de los datos. Funciona bien con variables categóricas lo cual es relevante para el dataset diabetesBRFSS2015, que contiene varias de estas variables, aunque también funciona con variables numéricas.

La suposición de independencia entre las características raramente se cumple en la práctica lo que puede afectar la precisión del modelo. Dado que es un modelo probabilístico puede no ser tan preciso en la clasificación cuando las relaciones entre las características son complejas o altamente correlacionadas. 

A menudo considerado como un punto de partida para la clasificación binaria, la regresión logística puede manejar relaciones lineales entre las características y la variable objetivo, pero puede quedarse corta frente a relaciones más complejas que Naive Bayes podría capturar a través de su enfoque probabilístico. 


## Pregunta 6. ¿Sería necesario aplicar alguna transformación a los datos? ¿Por qué? Si la respuesta es afirmativa, explica qué tendría de beneficioso hacerlo.

Aunque el algoritmo Naive Bayes es menos sensible al escalado de características que otros algoritmos, normalizar o estandarizar los datos numéricos puede ser útil para mantener la consistencia en la interpretación de los resultados. 

```{r}
diabetesBRFSS2015$BMI <- scale(diabetesBRFSS2015$BMI)
diabetesBRFSS2015$MentHlth <- scale(diabetesBRFSS2015$MentHlth)
diabetesBRFSS2015$PhysHlth <- scale(diabetesBRFSS2015$PhysHlth)
```


## Pregunta 7. Aplica el algoritmo designado en la pregunta 5 y optimiza el estadístico o estadísticos que consideres pertinente para medir su rendimiento mediante la selección de un modelo óptimo. Comenta los resultados


En primer, lugar vamos a generar los datos de entrenamiento y test de los datos de diabetesBRFSS2015 para entrenar el algoritmo de Naive-bayes. Vemos que el porcentaje de paciente sano y diabético en entrenamiento y test se mantienen.

```{r}
set.seed(1234)
diabetes.TrainIdx.80 <- createDataPartition(diabetesBRFSS2015$Diabetes_binary,
                                       p=0.8, #Genera un 80% para train, 20% para test
                                       list = FALSE, #Dame los resultados en una matriz
                                       times = 1) #Genera solamente una partición 80/20

trainSet.diabetes <- diabetesBRFSS2015[diabetes.TrainIdx.80, ]
testSet.diabetes <- diabetesBRFSS2015[-diabetes.TrainIdx.80, ]

table(trainSet.diabetes$Diabetes_binary)
table(testSet.diabetes$Diabetes_binary)
```

Dentro del paquete caret encontramos el modelo naive_bayes y los hiperparámetros específicos que se pueden ajustar dependen del tipo de modelo naive_bayes que se está utilizando. Por lo general, naive_bayes es conocido por su simplicidad y tiene relativamente pocos hiperparámetros en comparación con otros modelos de aprendizaje automático más complejos. Sin embargo, hay algunas variaciones del algoritmo que ofrecen diferentes hiperparámetros para el ajuste, especialmente cuando se trata de modelar la distribución de los datos.

- usekernel: Este hiperparámetro determina si se deben usar kernels para estimar la densidad de probabilidad de las variables numéricas. Es útil cuando se asume que la distribución de los datos no sigue necesariamente una distribución normal. Los valores habituales: TRUE o FALSE. El valor predeterminado suele ser FALSE, lo que significa que no se usan kernels y se asume una distribución normal (Gaussiana) para las variables numéricas. Si se establece en TRUE, se usan kernels para una aproximación más flexible de la distribución de los datos.

- laplace: Este hiperparámetro se utiliza cuando usekernel es TRUE. Se refiere al factor de suavizado (factor de Laplace) aplicado a las estimaciones de densidad del kernel para evitar el problema de la probabilidad cero. Los valores habituales son un valor numérico, generalmente pequeño (por ejemplo, 0, 0.01, 0.1). Un valor más grande aumenta el suavizado.

- adjust: También relacionado con el uso de kernels (cuando usekernel es TRUE), este hiperparámetro ajusta el ancho de banda del kernel utilizado en la estimación de la densidad. Los valores habituales son un valor numérico mayor que 0. Un valor de 1 significa no ajustar el ancho de banda, valores menores de 1 lo reducen y valores mayores de 1 lo aumentan. Ajustar el ancho de banda puede ayudar a afinar cómo se modela la distribución de las variables numéricas.

```{r}
modelLookup(("naive_bayes"))
```

Debido a que computacionalmente naive_bayes no es costoso, he decidido optar por un entrenamiento de validación cruzada repetitiva con 3 repeticiones y 10 particiones. Permito el análisis en paralelo para aumentar el rendimiento.

```{r}
set.seed(1234)
NBControl.diabetes <- trainControl(method = "repeatedcv",
                           number = 10 ,
                           repeats = 3,
                           returnResamp = "final",
                           seeds = NULL,
                           allowParallel = T)
```

En un primer intento de buscar el mejor modelo realizo una parrilla de hiperparámetros con los siguientes valores dando 68 posibilidades combinatorias:
- laplace: 0, 0.2, 0.4, 0.6, 0.8, 1
- usekernel: T, F
- adjust: 0.2, 0.6, 1, 1.4, 1.8, 2.2

```{r}
set.seed(1234)
y = trainSet.diabetes$Diabetes_binary
x = trainSet.diabetes[,-1]

mygrid.diabetes <- expand.grid(laplace = seq(0, 1, 0.2),
                      usekernel = c(T, F),
                      adjust = seq(0.2, 2.2, 0.4))

myfit.diabetes.cv10.rp3.nb <- train(Diabetes_binary ~ ., data = trainSet.diabetes, 
               method = "naive_bayes",
               metric = "Accuracy",
               trControl = NBControl.diabetes,
               tuneGrid=mygrid.diabetes
               )

(myfit.diabetes.cv10.rp3.nb.best <- subset(myfit.diabetes.cv10.rp3.nb$results, laplace == myfit.diabetes.cv10.rp3.nb$bestTune$laplace & usekernel == myfit.diabetes.cv10.rp3.nb$bestTune$usekernel & adjust == myfit.diabetes.cv10.rp3.nb$bestTune$adjust))
plot(myfit.diabetes.cv10.rp3.nb)

```

El mejor modelo obtenido es laplace = 0, sin utilizar kernel y adjust= 0.2, aunque no haría falta el parámetro adjust ya que no se utilizará kernel.

Como vemos en las siguiente gráficas, casi todas las iteraciones de nuestro mejor modelo de entrenamiento nos da como resultado un accuracy de entorno 0.717 y un Kappa de 0.433. Estos valores son bastante estables en cada una de las iteraciones ya que la distribución de los resultados es muy estable. Los resultados de accuracy, rondan entre 0.7 y 0.73, y los de kappa entre 0.4 y 0.44.

```{r}
resample <- myfit.diabetes.cv10.rp3.nb$resample
p1 <- ggplot(data = resample, aes(x = Accuracy)) +
  geom_density(size = 1.5,color = "#8B1A1A") +
  theme_minimal() +
  labs(title = "Accuracy",
       x = "Accuracy")+
  geom_vline(xintercept = median(resample$Accuracy),
             linetype = "dashed") +
  annotate("text", x = 0.72, y = 5, 
           label = paste("Accuracy", "\n", round(median(resample$Accuracy), 3)), color = "#8B1A1A", size = 4) +
  xlim(c(0.69, 0.73))
  
p2 <- ggplot(data = resample, aes(x = Kappa)) +
  geom_density(size = 1.5, color = "#f1e6b2") +
  theme_minimal() +
  labs(title = "Kappa",
       x = "Kappa")+
  geom_vline(xintercept = median(resample$Kappa),
             linetype = "dashed") +
   annotate("text", x = 0.435, y = 5, 
           label = paste("Kappa", "\n", round(median(resample$Kappa), 3)), color = "#c0b88e", size = 4) +
  xlim(c(0.39, 0.46))

grid.arrange(p1, p2, ncol = 2)
```

En la siguiente tabla vemos las variables más importantes para nuestro modelo y vemos que la más importante de todas en considerar tener buena salud, seguida de el indice de masa corporal y la tension vascular alta.

```{r}
varImp(myfit.diabetes.cv10.rp3.nb)
```

A continuación, vamos a predecir los resultados de la variable dependiente con el conjunto de test generado anteriormente.

```{r, warning=FALSE}
preds.diabetes <- predict(myfit.diabetes.cv10.rp3.nb$finalModel, newdata = testSet.diabetes)

conf.diabetes <- confusionMatrix(preds.diabetes, testSet.diabetes$Diabetes_binary, positive = "Diabetic")
conf.diabetes
```


La matriz muestra que 5748 individuos fueron correctamente identificados como saludables por el modelo (verdaderos positivos). 4090 individuos saludables fueron incorrectamente clasificados como diabéticos (Falsos negativos). 1321 individuos diabéticos fueron incorrectamente clasificados como saludables (falsos positivos). 2979 individuos fueron correctamente identificados como diabéticos (verdaderos negativos).

- Accuracy: El 61.73% de todas las predicciones fueron correctas. Este valor indica la proporción general de predicciones correctas del modelo.
- Intervalo de Confianza del 95% para la Precisión: Entre 60.92% y 62.53%, lo que proporciona un rango estimado de precisión del modelo en la población general.
- Tasa de No Información (No Information Rate): Es el mayor de las proporciones de las clases de referencia, que en este caso es 50%. Este valor se utiliza para comparar la precisión del modelo; una precisión significativamente mayor que la tasa de no información indica un modelo útil.
- P-value de la Precisión Mayor que la Tasa de No Información: Menor que 2.2e-16, lo que sugiere que el modelo es significativamente mejor que una conjetura al azar.
- Kappa de Cohen: De 0.2345, reflejando una concordancia débil a moderada más allá de la coincidencia por casualidad.
- Prueba de McNemar Valor-P: Menor que 2.2e-16, lo que indica una diferencia significativa entre los falsos positivos y los falsos negativos.

- Sensibilidad: El 42.14% de los individuos diabéticos reales fueron correctamente identificados, lo que sugiere una capacidad moderadamente baja del modelo para detectar la clase "Diabetic". 
- Especificidad: El 81.31% de los individuos saludables reales fueron correctamente identificados, indicando una alta capacidad del modelo para detectar la clase "Healthy".
- Valor Predictivo Positivo :  El 69.28% de los individuos clasificados como diabéticos eran realmente diabéticos.
- Valor Predictivo Negativo:El 58.43% de los individuos clasificados como saludables eran realmente saludables.
- Prevalencia de la Clase 'Diabetic': Se asumió un 50%, lo que indica que las clases están balanceadas en el conjunto de datos utilizado para esta matriz de confusión.
- Tasa de Detección para 'Diabetic'**: El 40.66% del total de individuos fueron correctamente identificados como saludables.
- Prevalencia de Detección para "Diabetic": El 30% del total de predicciones fueron para la clase 'Diabetic'.
- Precisión Balanceada: El 61.73%, igual a la precisión general, dado que la prevalencia de las clases es equilibrada.

Aunque el modelo tiene una buena especificidad para detectar individuos saludables, su sensibilidad y valor predictivo negativos son moderados, lo que indica una capacidad limitada para clasificar correctamente a los individuos sanos y para que las predicciones de la clase 'Diabetic' sean precisas. El coeficiente Kappa muestra una concordancia débil, lo que sugiere que hay margen de mejora en la capacidad del modelo para clasificar las instancias más allá de lo que se esperaría por azar. La prueba de McNemar destaca una discrepancia significativa entre los falsos positivos y falsos negativos, lo que puede indicar un sesgo en cómo el modelo realiza sus predicciones. 

A continuación muestro la matriz de confusión gráficamente para una mejor interpretación.

```{r, warning=FALSE}

preds.diabetes <- predict(myfit.diabetes.cv10.rp3.nb$finalModel, newdata = testSet.diabetes)

conf.diabetes <- confusionMatrix(preds.diabetes, testSet.diabetes$Diabetes_binary)
conf.diabetes_tibble <- as_tibble(conf.diabetes$table)
plot_confusion_matrix(conf.diabetes_tibble, 
                      target_col = "Reference", 
                      prediction_col = "Prediction",
                      counts_col = "n")
```

Vamos a ver a continuación sin este modelo ha incurrido en underfitting.

```{r, warning=FALSE}
# Evaluar el rendimiento en el conjunto de entrenamiento
trainPredictions <- predict(myfit.diabetes.cv10.rp3.nb$finalModel, trainSet.diabetes)
conf_train_diabetes <- confusionMatrix(trainPredictions, trainSet.diabetes$Diabetes_binary)

# Evaluar el rendimiento en el conjunto de prueba
testPredictions <- predict(myfit.diabetes.cv10.rp3.nb$finalModel, testSet.diabetes)
conf_test_diabetes <- confusionMatrix(testPredictions, testSet.diabetes$Diabetes_binary)

conf_train_diabetes$overall
conf_test_diabetes$overall
```
Al calcular el accuracy de los valores predichos con los datos de entrenamiento y el accuracy de los datos de test, observamos que en los datos de entrenamiento la precisión es ligeramente menor que los datos de test indicando que nuestro modelo ha incurrido en underfitting por lo que deberiamos considerar otro modelo o ver alternativas de preprocesamiento.

En resumen, este modelo predice muchos falsos negativos. En casos medicos como este, un falso positivo o falsos negativos pueden incurrir en graves consecuencias.

