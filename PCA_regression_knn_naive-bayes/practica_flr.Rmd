---
title: "Práctica 1 Machine Learning 2023/2024"
subtitle: "Master en Bioinformática, Universidad de Murcia"
author: "Fernando Lucas Ruiz (fernando.lucas@um.es)"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    highlight: kate
    number_sections: no
    theme: spacelab
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Task to perform

In this practice, we aim to analyze 3 datasets (BostonHousingData, LetterRecognition, and Diabetes in the BRFSS). In each of them, we will answer a series of questions. We will practice everything from variable association to PCA, the use of basic algorithms such as linear regression, logistic regression, KNN, and Naive Bayes, and preprocessing in three different datasets.

First, we will answer the questions of the first dataset, BostonHousingData.

## Libraries

```{r, message=FALSE, warning=FALSE}
library(tidyverse)  
library(gridExtra)  
library(viridis)  # For the viridis color palette  
library(caret)  
library(kknn)  
library(naivebayes)  
library(pheatmap)  # heatmap  
library(REdaS)  # For Bartlett’s Sphericity Test  
library(factoextra)  # PCA Graphics  
library(REdaS)  # PCA Analysis  
library(plotly)  # 3D PCA Graphics  
library(fastDummies)  # dummy variable encoding  
library(cvms)  # confusion matrix  
library(formattable)  # tables  

library(doParallel)  # for parallelizing processes  
num_cores <- detectCores()  
registerDoParallel(cores=num_cores)  
```

# BostonHousingData

The data in this dataset is found in the R package mlbench.

```{r}
library(mlbench)
data(BostonHousing)
```

## Size and type

```{r}
str(BostonHousing)
```

The dataset contains 506 rows and 14 variables.

All variables are numeric except the variable "chas" or Charles River dummy variable.

Let’s take a first look at the numeric data to check for nulls, view distributions, search for categorical variables, etc.

```{r}
Hmisc::describe(BostonHousing)
```

There are no missing values, and the variables have disparate distributions.

## Representation

This dataset includes information on different areas of Boston, detailed through various parameters. Each record in this dataset represents an area of Boston, with characteristics that influence the estimation of the median value of homes in that area. The dependent variable is "medv," which refers to the median value of homes inhabited by their owners in thousands of dollars.

The dataset variables are:

1. [crim]{.underline}: Per capita crime rate by town.
2. [zn]{.underline}: Proportion of residential land zoned for lots over 25,000 square feet.
3. [indus]{.underline}: Proportion of non-retail business acres per town.
4. [chas]{.underline}: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
5. [nox]{.underline}: Nitrogen oxide concentration (parts per 10 million).
6. [rm]{.underline}: Average number of rooms per dwelling.
7. [age]{.underline}: Proportion of owner-occupied units built before 1940.
8. [dis]{.underline}: Weighted distances to five Boston employment centers.
8. [rad]{.underline}: Index of accessibility to radial highways.
10. [tax]{.underline}: Full-value property-tax rate per $10,000.
11. [ptratio]{.underline}: Pupil-teacher ratio by town.
12. [b]{.underline}: 1000(Bk - 0.63)^2 where Bk is the proportion of Black residents by town.
13. [lstat]{.underline}: Percentage of lower-status population.
14. [medv]{.underline}: Median value of owner-occupied homes in thousands of dollars.

To examine the data in more detail, we will represent them in graphs to see their distributions and to draw preliminary conclusions.

```{r}
plots <- list()  
for (i in 1:14) {  
  p <- ggplot(BostonHousing, aes_string(x = names(BostonHousing)[i])) +  
    geom_density(adjust=1.5, alpha=.7, fill="#f1e6b2") +  
    theme_minimal() +  
    labs(title = names(BostonHousing)[i],  
         x = names(BostonHousing)[i])   

  plots[[colnames(BostonHousing)[i]]] <- p  
}  

plots$chas <- ggplot(BostonHousing, aes(x=chas)) +  
  geom_bar(alpha=.7, fill="#f1e6b2", color = "black")+  
  theme_minimal() +  
  labs(title = "chas",  
       x = "chas")  

grid.arrange(plots$crim , plots$zn, plots$indus, plots$nox, ncol=2)  
grid.arrange(plots$rm, plots$age, plots$dis, plots$rad, ncol=2)  
grid.arrange(plots$tax, plots$ptratio, plots$b, plots$lstat, ncol = 2)  
plots$chas  
```

1. [Crim]{.underline}: We observe that the crime rate is generally low, although some areas in the dataset reach very high crime rates, up to more than 80, likely representing problematic neighborhoods in Boston.

2. [Zn]{.underline}: This graph indicates that there are not many residential areas, although two intermediate peaks around 23 and 80 might represent residential neighborhoods.

3. [Indus]{.underline}: There are two zones in Boston with non-retail business areas.

4. [Nox]{.underline}: Most areas of Boston have a density of 0.4 and 0.6 ppm of nitrogen oxide, although some areas reach up to 0.9 ppm.

5. [Rm]{.underline}: The average number of rooms in Boston homes is about 6 rooms per dwelling.

6. [Age]{.underline}: As we can see, most homes in Boston were built before 1940. The tail on the left suggests that new houses are being built in Boston.

7. [Dis]{.underline}: This graph indicates that most areas are near employment centers, suggesting they may be downtown, although the right tail suggests that residential areas may be located in the outskirts.

8. [Rad]{.underline}: This graph shows that most areas are well-connected to highways, but there is a certain density of areas far from highways, which could indicate either residential or marginalized areas.

9. [Tax]{.underline}: This suggests that there is a normal distribution of property tax rates, except for certain areas that pay higher taxes, likely indicating luxury residential zones.

10. [Ptratio]{.underline}: Most areas have a student-to-teacher ratio around 20. The distribution broadens on the left tail because private schools tend to have lower ratios.

11. [B]{.underline}: This graph may suggest that there are few areas where most residents are Caucasian.

12. [Lstat]{.underline}: The lower-status population percentage is around 8%, but the right tail suggests that there are areas where the percentage increases, likely marginal zones.

13. [Chas]{.underline}: Most areas are far from the Charles River.

## Type of problem

In regression, the dependent or target variable is continuous, meaning it can take any value within a range. In the case of BostonHousing, the variable "medv" is clearly an example of a continuous variable, as housing prices can vary across a broad spectrum, theoretically without specific limits within the observed data ranges.

The primary goal of a regression problem is to predict specific values of the dependent variable based on one or more independent variables (predictors). In this context, we are interested in predicting the value of homes (medv) based on other characteristics of the dataset, such as the crime rate (crim), the average number of rooms per dwelling (rm), etc.

Let's examine the distribution of the "medv" data to get an initial picture of this variable.

```{r}
p1 <- ggplot(BostonHousing, aes(x = medv)) +
    geom_density(adjust=1.5, alpha=.7, fill="#f1e6b2") +
    theme_minimal() +
    labs(title = "medv", x ="") +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

p2<- ggplot(BostonHousing, aes(x=medv))+
  geom_boxplot(alpha=.7, fill="#f1e6b2")+
    theme_minimal() +
  labs(y ="")

grid.arrange(p1, p2, ncol=1)
```

Most homes are priced around 20,000 to 23,000 USD. We notice a few interesting cases where several homes deviate from the normal trend, with some observations having a value of 50,000 USD.

Let’s now examine how the "medv" variable correlates with the predictor variables.

```{r, warning=FALSE}
plots <- list()
for (i in 1:13) {
  # Crea una gráfica para cada variable
  p <- ggplot(BostonHousing, aes_string(x = names(BostonHousing)[i], y = names(BostonHousing)[14])) +
  geom_point(color ="#8B1A1A", alpha = 0.7) +
    labs(title = names(BostonHousing)[i],
         x = names(BostonHousing)[i], y = names(BostonHousing)[14]) +
#    scale_color_viridis_c(alpha = 0.7) +
    xlab(names(BostonHousing)[i]) +
    theme_minimal() +
    ylab("medv") +
    guides(color = FALSE) +
    theme(axis.title.x=element_blank(),
        axis.ticks.x=element_blank())

  plots[[colnames(BostonHousing)[i]]] <- p
}

plots$chas <- ggplot(BostonHousing, aes(y = medv, x = chas, fill = chas)) +
  geom_boxplot() +
  geom_jitter(color ="#8B1A1A", size = 1, alpha = 0.7) +
  theme_minimal() +
  scale_fill_viridis_d() + 
  theme(legend.position = "none")

grid.arrange(plots$crim , plots$zn, plots$indus, plots$nox, ncol=2)
grid.arrange(plots$rm, plots$age, plots$dis, plots$rad, ncol=2)
grid.arrange(plots$tax, plots$ptratio, plots$b, plots$lstat, ncol = 2)
plots$chas
```

A summary of the scatter plots:

1. [Crim]{.underline}: Neighborhoods with higher crime rates tend to have lower housing prices.
2. [Zn]{.underline}: There is a slight positive relationship in this variable.
3. Indus: As the proportion of non-retail businesses increases, housing prices fall.
4. [Nox]{.underline}: There is a slight tendency for higher nitrogen oxide levels to be associated with lower housing prices. This suggests that areas farther from the city have higher market values.
5. [Rm]{.underline}: There is a clear positive correlation between the number of rooms and housing prices.
6. [Age]{.underline}: It appears that in more modern neighborhoods, housing prices are slightly higher than in older areas.
7. [Dis]{.underline}: Areas farther from the city seem to have higher housing prices.
8. [Rad]{.underline}: It seems that some neighborhoods with low highway access have lower housing prices, but some high-value homes also have limited highway access, suggesting luxury residential areas.
9. [Tax]{.underline}: There is a slight negative correlation in this variable.
10. [Ptratio]{.underline}: Neighborhoods with higher housing prices tend to have lower student-teacher ratios.
11. [B]{.underline}: There is no clear relationship.
12. [Lstat]{.underline}: This relationship is quite clear, as lower-status populations tend to live in areas with lower housing prices.
13. [Chas]{.underline}: The imbalance between classes does not allow us to draw a firm conclusion, although it seems that areas near the Charles River tend to have higher housing prices.

```{r}
correlations <- cor(BostonHousing[,c(-4)])

cor_df <- data.frame(correlations["medv", ])

cor_df$Variable <- rownames(cor_df)
cor_df <- cor_df %>% filter(Variable != "medv")
names(cor_df)[1] <- "Correlation"
cor_df <- cor_df %>% arrange(desc(Correlation))

cor_df$Variable <- factor(cor_df$Variable, levels = cor_df$Variable[order(cor_df$Correlation, decreasing = TRUE)])

ggplot(cor_df, aes(x = Variable, y = Correlation, fill= Correlation)) +
  geom_col() +
  coord_flip() +
   labs(title = "Correlations with Medv", x = "", y = "Correlation", fill = "medv") +
  scale_fill_viridis_c() 
```

In this correlation plot, we can see that the variable most correlated with "medv" is "lstat," which measures the percentage of lower-status individuals, with a negative correlation. Other variables like "ptratio," "indus," and "tax" also show correlations. The variable most positively correlated with "medv" is the number of rooms (rm), followed by "zn" (proportion of residential land).

## Biases

Principal component analysis (PCA) is a statistical technique that reduces data dimensionality while maintaining as much variation as possible. It achieves this by identifying directions (principal components) in which the data vary the most. In addition to dimensionality reduction, PCA can provide insights into the underlying structure of the data, including potential biases.

To determine if our data is suitable for PCA, we must pass Bartlett's sphericity test and the Kaiser-Meyer-Olkin (KMO) test.

For both tests, we reject the null hypothesis, meaning our data is suitable for PCA. Additionally, the KMO test gives a high adequacy criterion of 0.86 for PCA.

```{r}
bart_spher(BostonHousing[,c(-4,-14)])
KMOS(BostonHousing[,c(-4,-14)])
```

To apply PCA correctly, it is essential to determine if our data needs proper scaling.

```{r}
datos_long_boston <- pivot_longer(BostonHousing, cols = -chas, names_to = "Variable", values_to = "Valor")

ggplot(datos_long_boston, aes(x = Variable, y = Valor, fill = Variable)) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha = 0.6) +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, hjust = 0.5),  
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),  
    axis.text.y = element_text(size = 8),  
    axis.title = element_text(size = 10),  
    axis.title.x = element_text(margin = margin(t = 10), hjust = 0.5),  
    axis.title.y = element_text(margin = margin(r = 10), hjust = 0.5)  
  ) +
  ggtitle("Variables") +
  xlab("Variables") +
  ylab("Count")
```

We should pay attention to variables such as "tax" and "b," which have significantly higher values compared to the other variables. This imbalance in value magnitudes can influence PCA results, making scaling necessary to ensure a fair interpretation of the principal components.

```{r}
pca_result_boston <- prcomp(BostonHousing[,c(-4,-14)], scale = TRUE)
```

```{r}
var_exp <- pca_result_boston$sdev^2
prop_var_exp <- var_exp / sum(var_exp)
cum_var_exp <- cumsum(prop_var_exp)

df_var_exp <- data.frame(Comp = 1:length(prop_var_exp), VarExp = prop_var_exp)
df_cum_var_exp <- data.frame(Comp = 1:length(cum_var_exp), CumVarExp = cum_var_exp)

# Scree plot con los datos no escalados
ggplot(df_var_exp[1:10,], aes(x = Comp, y = VarExp)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_line(aes(group = 1), color = "blue") +
  geom_point(color = "blue") +
  theme_minimal() +
  labs(x = "Principal components", y = "Variance", title = "Scree Plots") +
  ylim(c(0,1)) +
  geom_line(data = df_cum_var_exp[1:10,], aes(x = Comp, y = CumVarExp), color="#8B1A1A") +
  geom_point(data = df_cum_var_exp[1:10,], aes(x = Comp, y = CumVarExp), color = "red") +
  geom_bar(data = df_cum_var_exp[1:10,], aes(x = Comp, y = CumVarExp), stat = "identity", fill = "red", alpha= 0.25) +
  annotate("text", x = 9, y = 0.85, label = "Cumulative Scree \n Plot", color = "#8B1A1A", size = 4) +
  geom_text(data = df_cum_var_exp[1:10,], aes(x=Comp, y = CumVarExp +0.04, label = round(CumVarExp, 2)))
```

The first five principal components explain 85% of the total variance, allowing us to proceed with this dataset to investigate biases through a heatmap that visualizes correlations. Notably, the first principal component alone explains more than 50% of the variance, highlighting its significant contribution to understanding the data's structure.

The following heatmap shows the correlation of the principal components with the variables.

```{r}
sesgos <- cor(pca_result_boston$x[,1:5], BostonHousing[,c(-4,-14)])

pheatmap(sesgos,
         clustering_method = "ward.D2",
         color = colorRampPalette(c("#00AFBB", "white", "#FC4E07"))(255), # Paleta de colores
         cellwidth = 20,
         cellheight = 20,
         breaks = seq(-1, 1, length.out = 256),
         scale = "none", 
         annotation_legend = TRUE,
         angle_col = 45, cutree_cols = 3)
```

This heatmap suggests that the first principal component is highly correlated with variables such as "lstat," "rm," and "ptratio." The second component appears to capture variations related to variables like "crim" and "age."

Based on these results, we can tentatively conclude that the first principal component captures socioeconomic biases, as it is strongly associated with variables such as the percentage of lower-status individuals (lstat) and the average number of rooms (rm).

```{r, warning=FALSE}
pca <- as.data.frame(pca_result_boston$x[,1:5], stringsAsFactors=F)
pca <- cbind(BostonHousing, pca)

plots <- list()
for (i in 1:14) {
  # Crea una gráfica para cada variable
  p <- ggplot(pca, aes_string(x = names(pca)[15], y = names(pca)[16], color = names(pca)[i])) +
    geom_point() +
    labs(title = names(pca)[i],
         x = names(pca)[15], y = names(pca)[16], color = names(pca)[i]) +
    scale_color_gradient(low = "#D3D3D3", high = "#FC4E07") +
    geom_point() +
    xlab("PC 1 ") +
    ylab("PC 2")+
    theme_minimal()

  plots[[colnames(pca)[i]]] <- p
}

plots$chas <- ggplot(pca, aes(x = PC1, y = PC2, color = chas)) +  
  geom_point() +
  labs(title = "chas", x = "PC 1", y = "PC 2", color = "CHAS") +
  scale_color_manual(values = c("0" = "#D3D3D3", "1" = "#FC4E07")) +  
  theme_minimal()  

grid.arrange(plots$crim , plots$zn, plots$indus, plots$nox, ncol=2)
grid.arrange(plots$rm, plots$age, plots$dis, plots$rad, ncol=2)
grid.arrange(plots$tax, plots$ptratio, plots$b, plots$lstat, ncol = 2)
grid.arrange(plots$chas, plots$medv, ncol=2, nrow=2)
```

As seen in the heatmap, there is a clear bias in the data as the PCA results form two large clusters. We observe six neighborhoods significantly distanced from the rest, located in the bottom-right corner of the graph.

The rounded cluster, which could represent Boston’s urban core, is characterized by optimal highway accessibility (rad), a higher tax regime (tax), older buildings (age), higher levels of nitrogen oxides (nox), and a greater concentration of industrial zones (indus).

By contrast, the elongated second cluster suggests a transition to the periphery, as indicated by the increased average distance to employment centers (dis). This pattern is reflected in the progressive reduction of industrial areas (indus) towards the outskirts, leading to an increase in residential areas (zn), especially evident in the extension of the elongated cluster. Additionally, the presence of houses with a higher average number of rooms (rm) reinforces the notion of urban expansion.

A revealing detail is the existence of a central neighborhood with an unusually low percentage of Black residents (b). However, the data does not provide more distinctive characteristics for this neighborhood.

Proximity to the Charles River (chas) in the rounded cluster seems to delineate an exclusive area in the city center, where property values (medv) are substantially higher, and houses feature more rooms compared to the rest of the urban center.

Focusing on the six outliers in the bottom-right corner, we see elevated crime rates (crim) and a higher percentage of lower socioeconomic status individuals (lstat). These neighborhoods present marked racial diversity as they are either predominantly Black or White (b) and are located near highways, a characteristic of marginalized areas due to drug trafficking.

Finally, examining the dependent variable medv, we deduce that distance from the urban core correlates with an increase in housing value, which tends to be larger and located in predominantly residential areas. The sector adjacent to the Charles River in the urban core stands out for its high property values. In contrast, areas with higher crime rates tend to have lower housing prices, indicating an inverse relationship between safety and property value.

Now, let's examine the importance of variables for each principal component.

```{r}
p1 <- fviz_cos2(pca_result_boston, choice = "var", axes = 1, top= 5, fill = "#00AFBB", ggtheme = theme_minimal()) + labs(y = "Cos2", title = "Dim 1")
p2 <- fviz_cos2(pca_result_boston, choice = "var", axes = 2, top= 5, fill = "#00AFBB", ggtheme = theme_minimal()) + labs(y = "Cos2", title = "Dim 2")
p3 <- fviz_cos2(pca_result_boston, choice = "var", axes = 3, top= 5, fill = "#00AFBB", ggtheme = theme_minimal()) + labs(y = "Cos2", title = "Dim 3")
p4 <- fviz_cos2(pca_result_boston, choice = "var", axes = 4, top= 5, fill = "#00AFBB", ggtheme = theme_minimal()) + labs(y = "Cos2", title = "Dim 4")

grid.arrange(p1, p2, p3, p4, ncol=2)
```

The cosine squared (Cos2) of a variable in a principal component indicates how much that variable contributes to the variance explained by that component. For each principal component, the five variables with the highest Cos2 values are presented.

We observe that the first and second principal components have several variables explaining their variance. Although the contribution of variables to the variance of the second principal component is notable, it is less significant than the contributions to the first component, as seen by the lower Cos2 values.

In the third principal component, the variable rm has a much higher Cos2 than the others, indicating that it is a more dominant factor in this component.

In the fourth component, ptratio has the highest contribution, but all variables have relatively low values, suggesting that the fourth principal component captures less variance in the original variables than the previous components.

```{r}
pca_rotation <- round(data.frame(pca_result_boston$rotation[,1:4]),2)
pca_rotation <- pca_rotation[order(-pca_rotation[,1]),]

formattable(pca_rotation, 
            list(
  area(col= PC1:PC4) ~ formatter("span", style = x ~ ifelse(x > 0, 
    style(color = "green", font.weight = "bold"),  style(color = "red", font.weight = "bold"))) 
  ))
  
```

The loadings vectors of a PCA represent the contribution of each original variable to the computed principal components. Each column in the data provided represents a principal component, and each row represents a variable. The value of each cell in the loading matrix is the coefficient of the original variable in the corresponding principal component. These coefficients can be interpreted as the importance or weight of that variable for that specific principal component. A high value (positive or negative) indicates that the variable is strongly related to that principal component, while a value close to zero indicates that the variable has little or no effect in that dimension. The signs of the loading values indicate the direction of the relationship between the variable and the principal component, but in the context of PCA, only the magnitude of these values is relevant for determining the importance of the variable. The signs are arbitrary and may change if the PCA is run again due to axis inversion.

In the first principal component, the general trends observed are high positive values for "indus", "nox", "age", "rad", and "tax", which suggests that these variables contribute significantly and in the same direction to the first principal component. These variables may be associated with the urban core or industrial development.

In the second principal component, the variables "zn" and "crim" have the highest loading values by magnitude but in opposite directions. "zn" represents residential zones, while "crim" might be higher in densely populated urban areas.

The third principal component is dominated by "rm" with a high positive value, meaning "rm" is the main contributor to this component.

For the fourth principal component, "ptratio" and "b" have the highest values, but ptratio has a strong negative loading, suggesting an inverse relationship with this component. This dimension might capture aspects of the social or educational environment in the study areas.

Below, I graphically show the results of the first two principal components to see the direction and importance of each variable as described in this section.

```{r}
fviz_pca_biplot(pca_result_boston,
                geom.ind = "point", 
                geom_var = c("arrow", "text"), 
                col.var = "cos2",
                col.ind = "cos2",
                gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
                repel = F, 
                )
```

## Algorithm Selection

Given the nature of the BostonHousing data, I consider a regression algorithm should be used to predict the variable medv with respect to the other variables. Given the dataset's size, the best algorithms for this practice would be k-nearest neighbors (KNN).

KNN is a non-parametric algorithm and can capture non-linear relationships between variables without requiring a specific model formulation. KNN can be useful for identifying local and non-linear patterns that linear regression might miss, especially in areas where properties have distinctive characteristics that only apply to a specific neighborhood.

The KNN algorithm is intuitive and easy to understand as it predicts the value of a new observation by averaging the values of the K nearest neighbors. KNN takes into account the locality of the data, which could be particularly relevant for real estate data where location and neighborhood play a crucial role in housing prices.

## Data Transformation

It is important to pay attention to data preprocessing when using KNN. Since KNN depends on the distance between observations, the variables must be normalized or standardized so that they are on the same scale. Otherwise, variables with larger ranges could disproportionately influence the prediction.

We also need to convert the categorical variable chas to numeric using Dummy encoding. In this case, since there are only two levels, we would simply convert the variable to numeric values between 0 and 1 without adding additional columns.

```{r}
BostonHousing.scaled <- data.frame(scale(BostonHousing[,-4]))
BostonHousing.scaled <- cbind(BostonHousing.scaled, chas = BostonHousing$chas)
BostonHousing.scaled$chas <- as.numeric(BostonHousing.scaled$chas) - 1

datos_long_boston <- pivot_longer(BostonHousing.scaled, cols = -chas, names_to = "Variable", values_to = "Valor")

ggplot(datos_long_boston, aes(x = Variable, y = Valor, fill = Variable)) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha = 0.6) +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, hjust = 0.5),  
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),  
    axis.text.y = element_text(size = 8),  
    axis.title = element_text(size = 10),  
    axis.title.x = element_text(margin = margin(t = 10), hjust = 0.5),  
    axis.title.y = element_text(margin = margin(r = 10), hjust = 0.5)  
  ) +
  ggtitle("Variables") +
  xlab("Variables") +
  ylab("Count")

```

## Algorithm

First, we will generate the training and test data from the scaled BostonHousing data to train the KNN algorithm.

```{r}
set.seed(1234)

boston.TrainIdx.80<- createDataPartition(BostonHousing.scaled$medv,
                                       p=0.8, #Genera un 80% para train, 20% para test
                                       list = FALSE, # resultados en una matriz
                                       times = 1) #Genera solamente una partición 80/20

trainSet.boston <- BostonHousing.scaled[boston.TrainIdx.80, ]
testSet.boston <- BostonHousing.scaled[-boston.TrainIdx.80, ]
```

The kknn function is part of the kknn package, which is integrated with caret. Some of the hyperparameters that can be tuned using the kknn function through caret include:

1. [kmax]: The number of nearest neighbors to consider in the vote or average. It is the main hyperparameter of KNN. A small k can make the model sensitive to noise in the data (overfitting), while a large k can make the model too general (underfitting).

2. [distance]: Defines the distance metric to use for calculating proximity between points. For example, a distance of 2 refers to Euclidean distance, while 1 refers to Manhattan distance. Fractional distances are also possible and can be useful for capturing non-linear structures.

3. [kernel]: Specifies the weighting function to apply to the neighbors. Some options include "rectangular" (all neighbors have equal weight), "triangular", "epanechnikov", "biweight", "triweight", "cos", etc. Weighting can affect how neighbors influence the final prediction, giving more weight to closer neighbors.

It is important to note that hyperparameter tuning should be done carefully. For example, selecting k may require implementing techniques like cross-validation to find a value that balances bias and variance. The choice of distance metric and kernel function should be based on domain knowledge and experimentation, as well as validation.

```{r}
modelLookup(("kknn"))
```

Since we don't have a large sample size, I have chosen repetitive cross-validation with 3 repetitions and 10 partitions. Parallel analysis is enabled to improve performance.

```{r}
set.seed(1234)
kknnControl.boston <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 3,
                           seeds = NULL,
                           returnResamp = "final", 
                           allowParallel = T)
```

In the first attempt to find the best model, I run a hyperparameter grid search with the following values, giving 21 combinatory possibilities:

- kmax: 5, 7, 9, 11

- distance: 1, 2, 3

- kernel: rectangular, triangular, optimal

```{r}
set.seed(1234)
y = trainSet.boston$medv
x = trainSet.boston[,-13]

mygrid.boston = expand.grid(kmax = seq(from=5,to=11,2),
                     distance = seq(1, 3, 1),
                     kernel = c("rectangular","triangular","optimal" 
                                 # ,"epanechnikov", "biweight", "triweight" 
                                 # ,"cos", "inv", "gaussian","rank"
                                ))

myfit.boston.cv10.rp3 <- train(y=y, x = x, 
               method = "kknn",
               metric = "Rsquared",
               trControl = kknnControl.boston,
               tuneGrid=mygrid.boston
               )

(myfit.boston.cv10.rp3.best <- subset(myfit.boston.cv10.rp3$results, kmax == myfit.boston.cv10.rp3$bestTune$kmax & distance == myfit.boston.cv10.rp3$bestTune$distance & kernel == myfit.boston.cv10.rp3$bestTune$kernel))
plot(myfit.boston.cv10.rp3)

```

In this first model, we see that the best hyperparameters are kmax of 5, Manhattan distance of 1, and the optimal kernel. The RMSE and Rsquared values show very good results for this model.

Let's try with other types of kernels to see if we improve the model. The hyperparameters are:

- kmax: 5, 7, 9, 11

- distance: 1

- kernel: optimal, epanechnikov, biweight, triweight, cos, gaussian, rank

```{r}
set.seed(1234)

mygrid.boston = expand.grid(kmax = seq(from=5,to=11,3),
                     distance = 1,
                     kernel = c(#"rectangular","triangular", 
                                "optimal"
                                ,"epanechnikov", "biweight", "triweight"
                                 ,"cos", "gaussian","rank"
                                ))

myfit.boston.cv10.rp3.2 <- train(y=y, x = x, 
               method = "kknn",
               metric = "Rsquared",
               trControl = kknnControl.boston,
               tuneGrid=mygrid.boston,
               #preProcess = c("center", "scale")
               )

(myfit.boston.cv10.rp3.2best <- subset(myfit.boston.cv10.rp3.2$results, kmax == myfit.boston.cv10.rp3.2$bestTune$kmax & distance == myfit.boston.cv10.rp3.2$bestTune$distance & kernel == myfit.boston.cv10.rp3.2$bestTune$kernel))
plot(myfit.boston.cv10.rp3.2)
```

The best possible model is still found with the previous hyperparameters, so we don't improve the model. As seen in the following graphs, almost all iterations of our training model result in Rsquared values around 0.85 and an RMSE of 0.39 without major changes, although there are a few iterations with slightly higher RMSE and lower Rsquared.

```{r, warning=FALSE}
resample <- myfit.boston.cv10.rp3.2$resample
p1 <- ggplot(data = resample, aes(x = RMSE)) +
  geom_density(size = 2,color = "#8B1A1A") +
  theme_minimal() +
  labs(title = "RMSE",
       x = "RMSE")+
  geom_vline(xintercept = median(resample$RMSE),
             linetype = "dashed") +
  annotate("text", x = 0.5, y = 4, 
           label = paste("RMSE", "\n", round(median(resample$RMSE), 2)), color = "#8B1A1A", size = 4) 
  
p2 <- ggplot(data = resample, aes(x = Rsquared)) +
  geom_density(size = 2, color = "#f1e6b2") +
  theme_minimal() +
  labs(title = "Rsquared",
       x = "Rsquared")+
  geom_vline(xintercept = median(resample$Rsquared),
             linetype = "dashed") +
   annotate("text", x = 0.75, y = 6, 
           label = paste("Rsquared", "\n", round(median(resample$Rsquared), 2)), color = "#c0b88e", size = 4) 

grid.arrange(p1, p2, ncol = 2)
```

In the following table, we see the most important variables for our model, with "lstat" and "rm" being the most important. "Chas" does not influence our model since the data is highly imbalanced.

```{r}
var.import.boston <- varImp(myfit.boston.cv10.rp3.2)
var.import.boston.ordered <- var.import.boston$importance %>%
  arrange(desc(Overall))

formattable(var.import.boston.ordered, 
            list(
  Overall = color_tile("white", "#FC4E07")) 
  )
```

We see that the predicted values fall in similar locations to the train and test values, with no apparent outliers.

```{r}
preds.boston <- predict(myfit.boston.cv10.rp3.2$finalModel, newdata = testSet.boston)

df.train <- trainSet.boston
df.test <- testSet.boston
df.train$trainORtest <- "train"
df.test$trainORtest <- "test"
boston_df_plot.boston <- rbind(df.train, df.test)

prediction_test.boston <- cbind(testSet.boston, preds.boston)

plots <- list()
for (i in 1:13){
  col_name <- names(boston_df_plot.boston)[i]
  
  p <- ggplot(boston_df_plot.boston, aes_string(x = col_name, y = "medv")) + 
    geom_point(aes(color = trainORtest)) + 
    geom_point(data = prediction_test.boston, aes_string(x = col_name, y = "preds.boston", color = "'Predicciones'"), show.legend = TRUE) +
    scale_color_manual(values = c("train" = "#f1e6b2", "test" = "#b2d8b2", "Predicciones" = "black"),
                     name = "", labels = c("Predicciones", "Test", "Train")) + 
    theme_minimal()
    
    plots[[col_name]] <- p
}

plots$chas <- ggplot(boston_df_plot.boston, aes(y = medv, x = chas, color = trainORtest)) +
  geom_jitter() +
  geom_jitter(data = prediction_test.boston, aes(x = chas, y = preds.boston, color = "Predictions"), show.legend = TRUE) +
    scale_color_manual(values = c("train" = "#f1e6b2", "test" = "#b2d8b2", "Predictions" = "black"),
                     name = "", labels = c("Predictions", "Test", "Train")) + 
    theme_minimal() +
  geom_vline(xintercept = 0.5, color = "#8B1A1A", linetype = "dashed")

grid.arrange(plots$crim , plots$zn, plots$indus, plots$nox, ncol=2)
grid.arrange(plots$rm, plots$age, plots$dis, plots$rad, ncol=2)
grid.arrange(plots$tax, plots$ptratio, plots$b, plots$lstat, ncol = 2)
plots$chas

```

The analysis of the prediction error relative to the actual medv values in the test set reveals a differentiated trend depending on the housing price range. For houses with a value close to the median of medv, predictions are notably accurate, reflecting low error. However, for houses with medv values higher than average, the model tends to perform less accurately, resulting in higher prediction errors.

```{r}
testSet.boston %>%
  cbind(preds.boston) %>%
  mutate(RMSE = sqrt((preds.boston - medv)^2)) %>%
  ggplot(aes(x = medv, y = RMSE)) +
  geom_line(color ="#8B1A1A") + 
  geom_point(color ="#c0b88e") + 
  theme_minimal() +
  labs(title = "Predictive error test data", x = "Medv test data")
```

Finally, to check for overfitting or underfitting, we calculate the RMSE of the predicted values with the training data and the test data. We observe that the training data error is smaller than the test data error, but the difference is not large, indicating a very low error.

**We find a good model for predicting housing prices with these variables in the city of Boston.**

```{r, warning=FALSE}
preds.boston.train <- predict(myfit.boston.cv10.rp3.2$finalModel, newdata = trainSet.boston)

cat("El error RMSE de los datos de entrenamiento es",sqrt((1/nrow(trainSet.boston)) * sum((trainSet.boston$medv - preds.boston.train)^2)),"\n")

cat("El error RMSE de los datos de evaluación es", sqrt((1/nrow(testSet.boston)) * sum((testSet.boston$medv - preds.boston)^2)),"\n")
```

# LetterRecognition

The data for this dataset is found in the R mlbench package.

```{r}
library(mlbench)
data(LetterRecognition)
```

## Size and Type

```{r}
str(LetterRecognition)
```

The dataset contains 20,000 rows and 17 variables.

All variables are numerical except for the variable lettr, which is the dependent variable.

Here’s a first look at the numerical data to check for missing values, distribution, and categorical variables.

```{r}
Hmisc::describe(LetterRecognition)
```

There are no missing values, and the distributions range from 0 to 15 in all variables except for the dependent variable lettr.

## Representation of Examples

The LetterRecognition dataset in the mlbench package is designed for the task of identifying uppercase letters from the English alphabet based on black and white pixel images. Each letter image is based on 20 different fonts, and each letter from these fonts was randomly distorted to produce a set of 20,000 unique stimuli. These stimuli were converted into 16 primitive numerical attributes, such as statistical moments and edge counts, which were scaled to fit a range of integer values from 0 to 15.

The dataset contains 20,000 observations with 17 variables. The first variable is categorical, with levels from A to Z, representing each of the alphabet letters. The remaining 16 variables are numerical and represent various statistical and geometric characteristics of the letter images.

1. x.box: The horizontal position of the box enclosing the letter (the smallest rectangle that can contain the letter image).
2. y.box: The vertical position of the box enclosing the letter.
3. width: The width of the box in pixels.
4. high: The height of the box in pixels.
5. onpix: The total number of on pixels (i.e., pixels containing part of the letter) in the image.
6. x.bar: The mean horizontal position of all on pixels.
7. y.bar: The mean vertical position of all on pixels.
8. x2bar: The variance of the horizontal position of all on pixels.
9. y2bar: The variance of the vertical position of all on pixels.
10. xybar: The correlation between the horizontal and vertical positions of the on pixels.
11. x2ybr: The mean of the product of x by the vertical position of the on pixels.
12. xy2br: The mean of the product of y by the horizontal position of the on pixels.
13. x.ege: The number of horizontal edges (transitions from light to dark).
14. xegvy: The correlation of x.ege with the variable y.
15. y.ege: The number of vertical edges (transitions from light to dark).
16. yegvx: The correlation of y.ege with the variable x.

## Type of Problem

This dataset is a classification problem. The reason is that the goal is to categorize each digital representation of a letter into one of several predefined classes, which are the letters of the alphabet. Therefore, the dataset is a classic example of a multi-class classification problem, where the machine learning model must predict which of the multiple categories (lettr) corresponds to the given features of an image.

For a classification problem, it’s crucial to examine the distribution of examples among the classes. A balanced distribution of examples ensures that the machine learning model has enough data to learn the distinctive features of each class. Conversely, an imbalanced distribution can lead to a biased model that favors the more prevalent classes.

Below, we show the distributions of the frequencies of each class for the variable lettr. We see that the distribution is fairly homogeneous between classes, with around 750 instances of each letter.

```{r}
table(LetterRecognition$lettr)
```

```{r}
LetterRecognition %>%
  group_by(lettr) %>%
  summarize(Frequency = n()) %>%
  ungroup() %>%
  ggplot(aes(x = lettr, y = Frequency)) +
  geom_segment(aes(x = lettr, xend = lettr, y = 0, yend = Frequency)) +
  geom_point(size = 5, aes(fill = lettr), shape = 21, stroke = 2, alpha = 0.7) +
  scale_fill_viridis_d() +
  theme_minimal() + 
  guides(fill = FALSE) + 
  ylim(c(0, 1000)) +
  labs(title = "Letters") 
```

## Biases

To assess whether our data is suitable for Principal Component Analysis (PCA), we must pass the Bartlett's Test of Sphericity and the Kaiser-Meyer-Olkin (KMO) test.

In both tests, we reject the null hypothesis, indicating that our data is appropriate for PCA. Additionally, the KMO test gives us a criterion of 0.68, which is suitable for PCA.

```{r}
bart_spher(LetterRecognition[,-1])
KMOS(LetterRecognition[,-1])
```

Let’s examine the correlation between the predictor variables to see how they group together. We see that there are two groups of highly positively correlated variables, making them candidates to be important in different principal components. Then there is another cluster with more diverse correlations.

```{r}
correlations <- cor(LetterRecognition[,c(-1)])

pheatmap(correlations,
         clustering_method = "ward.D2",
         color = colorRampPalette(c("#00AFBB", "white", "#FC4E07"))(255), # Paleta de colores
         cellwidth = 12,
         cellheight = 12,
         breaks = seq(-1, 1, length.out = 256),
         scale = "none", 
         annotation_legend = TRUE,
         angle_col = 45, cutree_cols = 3)
```

To correctly apply PCA, it’s essential to determine whether our data needs appropriate scaling.

```{r}
datos_long_letter <- pivot_longer(LetterRecognition, cols = -lettr, names_to = "Variable", values_to = "Valor")

ggplot(datos_long_letter, aes(x = Variable, y = Valor, fill = Variable)) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha = 0.6) +
  scale_color_viridis(discrete = TRUE, alpha = 0.6) + 
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, hjust = 0.5),  
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    axis.text.y = element_text(size = 8),
    axis.title = element_text(size = 10),
    axis.title.x = element_text(margin = margin(t = 10), size = 14, hjust = 0.5),
    axis.title.y = element_text(margin = margin(r = 10), size = 14, hjust = 0.5)
  ) +
  ggtitle("Variables") +
  xlab("Variables") +
  ylab("Value")
```

The distribution is not homogeneous, but we see that all the data is between 0 and 15, so I do not consider it necessary to scale the data since all values are restricted to that range and appear to be on the same scale.

```{r}
pca_result.letter <- prcomp(LetterRecognition[,-1])
```

```{r}
var_exp <- pca_result.letter$sdev^2
prop_var_exp <- var_exp / sum(var_exp)
cum_var_exp <- cumsum(prop_var_exp)

df_var_exp <- data.frame(Comp = 1:length(prop_var_exp), VarExp = prop_var_exp)
df_cum_var_exp <- data.frame(Comp = 1:length(cum_var_exp), CumVarExp = cum_var_exp)

# Scree plot con los datos no escalados
ggplot(df_var_exp[1:10,], aes(x = Comp, y = VarExp)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_line(aes(group = 1), color = "blue") +
  geom_point(color = "blue") +
  theme_minimal() +
  labs(x = "Principal components", y = "Variance", title = "Scree Plot") +
  ylim(c(0,1)) +
  geom_line(data = df_cum_var_exp[1:10,], aes(x = Comp, y = CumVarExp), color="#8B1A1A") +
  geom_point(data = df_cum_var_exp[1:10,], aes(x = Comp, y = CumVarExp), color = "red") +
  geom_bar(data = df_cum_var_exp[1:10,], aes(x = Comp, y = CumVarExp), stat = "identity", fill = "red", alpha= 0.25) +
  annotate("text", x = 9, y = 0.75, label = "Cumulative Scree \n Plot", color = "#8B1A1A", size = 4) +
  geom_text(data = df_cum_var_exp[1:10,], aes(x=Comp, y = CumVarExp +0.04, label = round(CumVarExp, 2)))
```

The first component only explains 29% of the variance. With the second component, it explains up to 44%, so we understand that the characteristics explained by our variables are well distributed, and each explains something important in the classification of our classes.

To see if there are clear biases in our data, let’s correlate the variables with the principal components.

```{r}
sesgos.letter <- cor(pca_result.letter$x[,1:7], LetterRecognition[,-1])

pheatmap(sesgos.letter,
         clustering_method = "ward.D2",
         color = colorRampPalette(c("#00AFBB", "white", "#FC4E07"))(255), # Paleta de colores
         cellwidth = 20,
         cellheight = 20,
         breaks = seq(-1, 1, length.out = 256),
         scale = "none", 
         annotation_legend = TRUE,
         cutree_cols = 3)
```

We see that the variables "x.ege", "y.box", "high", "onpix", "x.box", and "width" are strongly correlated with the first principal component. Curiously, these are the same variables that had a strong correlation with each other in the previous correlation map.

Let’s now examine the distribution of our variables on the first two principal components 1 and 2.

```{r, warning=FALSE}
pca.letter <- as.data.frame(pca_result.letter$x[,1:7], stringsAsFactors=F)
pca.letter <- cbind(LetterRecognition, pca.letter)

plots <- list()
for (i in 1:17) {
  # Crea una gráfica para cada variable
  p <- ggplot(pca.letter, aes_string(x = names(pca.letter)[18], y = names(pca.letter)[19], color = names(pca.letter)[i])) +
    geom_point() +
    labs(x = names(pca.letter)[18], y = names(pca.letter)[19], color = names(pca.letter)[i]) +
    scale_color_gradient(low = "#D3D3D3", high = "#FC4E07") +
    theme_minimal()

  plots[[colnames(pca.letter)[i]]] <- p
}

plots$lettr <- ggplot(pca.letter, aes(x = PC1, y = PC2, color = lettr)) +
    geom_point() +
    labs(x = "PC 1", y = "PC 2", color = "lettr") +
    theme_minimal()

grid.arrange(plots$x.box , plots$y.box, plots$width, plots$high, ncol=2)
grid.arrange(plots$onpix, plots$x.bar, plots$y.bar, plots$x2bar, ncol=2)
grid.arrange(plots$y2bar, plots$xybar, plots$x2ybr, plots$xy2br, ncol = 2)
grid.arrange(plots$x.ege, plots$xegvy, plots$y.ege, plots$yegvx, ncol = 2)
plots$lettr
```

As we can see, no separable clusters are visible in the first two principal components, but we do see that each variable is located in specific areas, grouping together with variables that are more related. For example, the variables "x.box", "y.box", "width", and "high" are in similar areas as they correlate with each other.

Regarding the dependent variable lettr, we see that we can intuitively distinguish each letter from the others in these first two principal components. In the following illustrations, I better show the distribution of each class among the first two principal components.

```{r}
unique_letters <- unique(LetterRecognition$lettr)

plots <- list()

for (letter in unique_letters) {
  pca.letter$highlight <- ifelse(pca.letter$lettr == letter, "Highlighted", "Other")
  
  plots[[letter]] <- ggplot(pca.letter, aes(x = PC1, y = PC2)) +
    geom_point(aes(color = highlight), alpha = ifelse(pca.letter$highlight == "Highlighted", 1, 0.05)) +
    scale_color_manual(values = c("Highlighted" = "#FC4E07", "Other" = "grey")) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    geom_vline(xintercept = 0, linetype = "dashed") +
    labs(title = letter) +
    theme_minimal() +
    guides(color = FALSE, alpha = FALSE)
}

grid.arrange(plots$A, plots$B,plots$C,plots$D,plots$E,plots$F, plots$G, plots$H, plots$I,plots$J,ncol = 5)
grid.arrange(plots$K,plots$L, plots$M,plots$N,plots$O,plots$P,plots$Q,plots$R,plots$S,plots$T,ncol = 5)
grid.arrange( plots$U,plots$V,plots$W,plots$X, plots$Z, ncol = 3)

```

Lastly, another way to represent the distribution of the letter classes but this time among the first three principal components. It becomes clear that classifying the letter classes using these variables is promising.

```{r, warning=FALSE}
plot <- plot_ly(data = pca.letter, x = ~PC1, y = ~PC2, z = ~PC3, color = ~lettr, type = 'scatter3d', mode = 'markers')
plot <- plot %>% layout(title = "3D PCA Letter Recongnition", 
                        scene = list(xaxis = list(title = "PC1"),
                                     yaxis = list(title = "PC2"),
                                     zaxis = list(title = "PC3")))

plot
```

The above 3D graph can be explained by the next graph, where we show the weight of each variable in each dimension. For instance, "y.box" has the highest weight in explaining the variance of the data in the first principal component. The variables "x2ybr" and "y.bar" are important in the second principal component, while "x2bar" and "y2bar" dominate the third.

```{r}
p12 <- fviz_pca_var(pca_result.letter, col.var="cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, axes = c(1,2), label = "var", title="PC1-2") + theme(legend.position = "none")

p23 <- fviz_pca_var(pca_result.letter, col.var="cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, axes = c(2,3), label = "var", title="PC2-3") + theme(legend.position = "none")

grid.arrange(p12, p23, ncol= 2)

```

Now let’s look at the numerical values of the loading vectors for the first three principal components.

```{r}
pca_rotation <- round(data.frame(pca_result.letter$rotation[,1:3]),2)
pca_rotation <- pca_rotation[order(-pca_rotation[,1]),]

formattable(pca_rotation, 
            list(
  area(col= PC1:PC3) ~ formatter("span", style = x ~ ifelse(x > 0, 
    style(color = "green", font.weight = "bold"),  style(color = "red", font.weight = "bold"))) 
  ))
```

The variables with the highest loadings in the first principal component ("x.box", "y.box", "width", "high", "onpix") appear to be related to the physical dimensions of the boxes enclosing the letters and the number of on pixels (onpix), suggesting that PC1 might be capturing information related to the general size and spatial occupation of the letters.

The variables with the highest absolute loadings in the second component are "y.bar" and "x2ybr", both with negative values, and "x.bar" with a significant positive value. This indicates that PC2 might be capturing variations in the spatial distribution of the pixels in the letters, possibly reflecting characteristics like the symmetry or vertical/horizontal distribution of pixels.

The variables "x2bar" and "xybar" have significant loadings in the third component, although in opposite directions. "x2bar" has a positive loading, and xybar has a negative loading. These loadings suggest that the third component might be associated with the dispersion or alignment of the pixels in the letter images, which could be interpreted as features capturing the texture or internal structure of the letters.

Now let’s look at the contribution of each variable to the variance of each principal component.

```{r}
p1 <- factoextra::fviz_cos2(pca_result.letter, choice = "var", axes = 1, top= 5, fill = "#00AFBB", ggtheme = theme_minimal()) + labs(y = "Cos2", title = "Dim 1")
p2 <- factoextra::fviz_cos2(pca_result.letter, choice = "var", axes = 2, top= 5, fill = "#00AFBB", ggtheme = theme_minimal())+ labs(y = "Cos2", title = "Dim 2")
p3 <- factoextra::fviz_cos2(pca_result.letter, choice = "var", axes = 3, top= 5, fill = "#00AFBB", ggtheme = theme_minimal())+ labs(y = "Cos2", title = "Dim 3")

grid.arrange(p1, p2, p3, ncol=3)

```

The five variables that project best onto the first principal component are shown in the first graph. The variable "y.box" has the highest Cos2 value, indicating that it is best represented in this dimension. This means that "y.box" contributes significantly to the variance captured by the first principal component.

The second graph shows the five variables with the highest Cos2 for the second principal component. The variable "x2ybar" has a much higher Cos2 value compared to the other four, suggesting that it has a predominant influence in this second dimension.

The third graph presents the five most important variables in terms of representation in the third principal component. The variable "x2ybar" also appears here with the highest value, followed by 'y2bar', 'xybar', 'x.ege', and 'x2ybr', suggesting that these variables are also important in defining this third principal component.

## Algorithm Selection

KNN is a reasonable choice for the LetterRecognition dataset due to its ability to handle multi-class classifications and its usefulness in situations where classification depends on the similarity of instances in feature space. Its robustness against complex variations in the data makes it suitable for recognizing visual patterns, which is essential in classifying letters based on visual attributes.

Moreover, KNN does not make assumptions about the underlying distribution of the data, which is advantageous in image processing, where distributions can be complex and non-linear. KNN’s ability to adapt locally to the data by adjusting the number of neighbors allows for great flexibility in finding meaningful structures in the feature space for classification.

However, it’s important to note that KNN has its disadvantages. The main drawback is its computational intensity, especially in large datasets with many features, due to the need to calculate the distance between pairs for each test instance against all training instances. Additionally, KNN can be sensitive to irrelevant or redundant features, so good preprocessing and possibly feature selection are crucial steps for its optimal performance.

## Data Transformation

In this case, the data was previously scaled to be within a range of 0 to 15, so I would not consider scaling the data.

On the other hand, I could reduce the dimensionality to 7 or 8 dimensions, as they explain more than 80% of the data variance, in case computational capacity becomes an issue. In this case, I will not reduce the dimensionality and will continue with the untransformed data.

## Algorithm

First, we will generate the training and test data from the letterRecognition dataset to train the KNN algorithm. We see that the percentage of each letter in training and testing remains consistent.

```{r}
set.seed(1234)
letterRecognition.TrainIdx.80<- createDataPartition(LetterRecognition$lettr,
                                       p=0.8, #Genera un 80% para train, 20% para test
                                       list = FALSE, #Dame los resultados en una matriz
                                       times = 1) #Genera solamente una partición 80/20

trainSet.letter <- LetterRecognition[letterRecognition.TrainIdx.80, ]
testSet.letter <- LetterRecognition[-letterRecognition.TrainIdx.80, ]

table(trainSet.letter$lettr)
table(testSet.letter$lettr)
```

In the previous section on the BostonHousing data, I explain the caret kknn package and the hyperparameters to use (kmax, distance, and kernel).

```{r}
modelLookup(("kknn"))
```

For this model, I chose to train the data with non-repeated cross-validation, as the dimensionality of this data is quite large, making it computationally expensive. I also allow parallel processing to improve performance.

```{r}
set.seed(1234)
kknnControl.letter<- trainControl(method = "cv",
                           number = 10 ,
                           returnResamp = "final",
                           seeds = NULL,
                           allowParallel = T)
```

In a first attempt to find the best model, I perform a grid search of hyperparameters with the following values, yielding 9 combinatorial possibilities:

- kmax: 5, 7, 9
- distance: 1, 2, 3
- kernel: optimal

```{r}
set.seed(1234)
y <- trainSet.letter$lettr
x <- trainSet.letter[,-1]

mygrid.letter <- expand.grid(kmax = seq(5,9,2),
                      distance = c(1,2,3),
                      kernel = c(#"rectangular","triangular",
                                 "optimal" 
                                 # ,"epanechnikov", "biweight", "triweight" 
                                 # ,"cos", "inv", "gaussian","rank"
                                ))

myfit.letter.cv10 <- train(y=y, x=x, 
               method = "kknn",
               metric = "Kappa",
               trControl = kknnControl.letter,
               tuneGrid=mygrid.letter
               )

(myfit.letter.cv10.best <- subset(myfit.letter.cv10$results, kmax == myfit.letter.cv10$bestTune$kmax & distance == myfit.letter.cv10$bestTune$distance & kernel == myfit.letter.cv10$bestTune$kernel))
plot(myfit.letter.cv10)

```

In this first model, we see that the best hyperparameters are with a kmax of 9, Manhattan distance of 1, and the optimal kernel. The Accuracy (0.95) and Kappa (0.95) show very good values for this model.

We will try with higher kmax values since the trend for kmax seems to increase. We will also test other types of kernels to see if the model can be improved. The hyperparameters are:

- kmax: 9, 10, 11
- distance: 1
- kernel: rectangular, triangular, optimal

```{r}
mygrid.letter <- expand.grid(kmax = seq(9,11,1),
                      distance = 1,
                      kernel = c("rectangular","triangular",
                                 "optimal" 
                                 # ,"epanechnikov", "biweight", "triweight" 
                                 # ,"cos", "inv", "gaussian","rank"
                                ))

myfit.letter.cv10.2 <- train(y=y, x=x, 
               method = "kknn",
               metric = "Kappa",
               trControl = kknnControl.letter,
               tuneGrid=mygrid.letter
               )

(myfit.letter.cv10.2.best <- subset(myfit.letter.cv10.2$results, kmax == myfit.letter.cv10.2$bestTune$kmax & distance == myfit.letter.cv10.2$bestTune$distance & kernel == myfit.letter.cv10.2$bestTune$kernel))
plot(myfit.letter.cv10.2)
```

In this second model, we see that the best hyperparameters are still a kmax of 9 and the triangular kernel. The Accuracy (0.96) and Kappa (0.96) show very good values for this model, improving the previous results.

We will now try with a fixed kmax of 9 but with different types of kernels to see if the model can be improved. The hyperparameters are:

- kmax: 9,
- distance: 1
- kernel: triangular, epanechnikov, biweight, triweight, cos, inv, gaussian, rank

```{r}
mygrid.letter <- expand.grid(kmax = 9,
                      distance = 1,
                      kernel = c(#"rectangular",
                                 "triangular"
                                 #,"optimal" 
                                  ,"epanechnikov", "biweight", "triweight" 
                                  ,"cos", "inv", "gaussian","rank"
                                ))

myfit.letter.cv10.3 <- train(y=y, x=x, 
               method = "kknn",
               metric = "Kappa",
               trControl = kknnControl.letter,
               tuneGrid=mygrid.letter
               )

(myfit.letter.cv10.3.best <- subset(myfit.letter.cv10.3$results, kmax == myfit.letter.cv10.3$bestTune$kmax & distance == myfit.letter.cv10.3$bestTune$distance & kernel == myfit.letter.cv10.3$bestTune$kernel))
plot(myfit.letter.cv10.3)
```

We still see that other kernels do not improve the model, so the best model for this dataset is with a kmax of 9, Manhattan distance of 1, and the triangular kernel.

As shown in the following graphs, almost all iterations of our best training model result in an accuracy of around 0.96 and a Kappa of 0.96. These values are very high, so we should check if our model has fallen into overfitting.

```{r}
resample <- myfit.letter.cv10.3$resample
p1 <- ggplot(data = resample, aes(x = Accuracy)) +
  geom_density(size = 2,color = "#8B1A1A") +
  theme_minimal() +
  labs(title = "Accuracy",
       x = "Accuracy")+
  geom_vline(xintercept = median(resample$Accuracy),
             linetype = "dashed") +
  annotate("text", x = 0.95, y = 4, 
           label = paste("Accuracy", "\n", round(median(resample$Accuracy), 3)), color = "#8B1A1A", size = 4)  +
  xlim(c(0.93, 0.97))
  
p2 <- ggplot(data = resample, aes(x = Kappa)) +
  geom_density(size = 2, color = "#f1e6b2") +
  theme_minimal() +
  labs(title = "Kappa",
       x = "Kappa")+
  geom_vline(xintercept = median(resample$Kappa),
             linetype = "dashed") +
   annotate("text", x = 0.95, y = 6, 
           label = paste("Kappa", "\n", round(median(resample$Kappa), 3)), color = "#c0b88e", size = 4) +
  xlim(c(0.93, 0.97))

grid.arrange(p1, p2, ncol = 2)
```

In the following table, we see the most important variables for our model for each letter of our dependent variable "lettr." The variable "y2bar" has strong importance in each letter. On the other hand, "y.box" is not important in our model for predicting letters from these variables.

```{r}
import <- varImp(myfit.letter.cv10.3)
import$importance <- round(import$importance[order(-import$importance[,1]),], 2)
formattable(import$importance, list(
  area(col= A:Z) ~ color_bar("lightblue")
  ))
```

Now let's predict the classes of the test data that we generated earlier. After that, we will show the confusion matrix information of these results.

A confusion matrix is a powerful tool used in the field of machine learning to assess the performance of classification models. It is a specific table that allows the visualization of the performance of a classification algorithm, explicitly showing when the classes are confused by the model during prediction.

The confusion matrix compares the predicted labels by the model against the actual (true) labels in the test dataset. The confusion matrix allows the calculation of important metrics such as accuracy, sensitivity, specificity, and predictive value. It also helps to understand the types of errors the model is making (for example, if it is predicting false positives more often than false negatives).

```{r}
preds.letter <- predict(myfit.letter.cv10.3$finalModel, newdata = testSet.letter)

conf.letter <- confusionMatrix(preds.letter, testSet.letter$lettr)
conf.letter
```

The overall accuracy of the model is 95.64%, indicating a high level of precision in classifying instances into their respective categories. The Kappa value is 0.9546, which indicates very good agreement between the model's predictions and the actual values, beyond what would be expected by chance.

Each class also shows high values of sensitivity (the model's ability to correctly identify true positives) and specificity (the model's ability to correctly identify true negatives), demonstrating that the model is capable of correctly classifying most instances in all categories.

Although the model is highly robust and makes very few errors, let's visualize, through the following heatmap, the mistakes the model makes.

```{r}
conf_matrix_long <- as.data.frame(conf.letter$table)

ggplot(data = conf_matrix_long, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  scale_fill_gradient2(low = "white", high = "#FC4E07", mid ="#00AFBB", midpoint = 5, limit = c(0, 10)) +
  theme_minimal() +
  labs(x = "Predicted", y = "Reference", fill = "Count") +
  geom_text(aes(x=Reference, y = Prediction, label=ifelse(Freq > 0, paste(Reference, Prediction), "")), size = 2)+ coord_flip()
```

What we see in the heatmap are the errors made by our model. For example, it made nearly 10 mistakes by predicting a "P" instead of an "F". Another error it made a few times was predicting a "Q" instead of an "O". These are mistakes likely due to the high similarity in handwriting between the letters.

Even with these few mistakes, our model perfectly predicts the different classes with this training model. Let's now check if this model has overfitted.

```{r}
trainPredictions <- predict(myfit.letter.cv10.3$finalModel, trainSet.letter)
conf_train <- confusionMatrix(trainPredictions, trainSet.letter$lettr)

testPredictions <- predict(myfit.letter.cv10.3$finalModel, testSet.letter)
conf_test <- confusionMatrix(testPredictions, testSet.letter$lettr)

conf_train$overall
conf_test$overall
```

By calculating the accuracy of the predicted values with the training data and the accuracy with the test data, we observe that the training data accuracy is higher than the test data, but the difference is not significant, and both accuracies are very high. We have found a very good model for predicting letter types based on the variables collected in this dataset.

# Diabetes BRFSS

The data for this dataset is located in the "diabetesBRFSS2015.csv" file.

```{r}
diabetesBRFSS2015 <- read.csv("diabetesBRFSS2015.csv")
```

## Size and Type

```{r}
str(diabetesBRFSS2015)
```

It is a dataset with 70,692 samples and 22 variables, most of which are numeric, though some data-loading issues may exist. I'll convert the categorical variables into factors.

```{r}
diabetesBRFSS2015[] <- lapply(diabetesBRFSS2015, as.factor)
diabetesBRFSS2015$BMI <- as.numeric(diabetesBRFSS2015$BMI)
diabetesBRFSS2015$MentHlth <- as.numeric(diabetesBRFSS2015$MentHlth)
diabetesBRFSS2015$PhysHlth <- as.numeric(diabetesBRFSS2015$PhysHlth)
diabetesBRFSS2015$Diabetes_binary <- factor(diabetesBRFSS2015$Diabetes_binary, levels= c(0,1), labels = c("Healthy", "Diabetic"))

Hmisc::describe(diabetesBRFSS2015)
```

There are no missing data in this dataset. Some categorical variables are well balanced, while others are not. Some variables have multiple classes, and some may be numerical.

## Representation of Examples

The data in the diabetesBRFSS2015 dataset represents survey information about the general lifestyle of people, including their diabetes diagnosis. Each instance in this dataset represents a person who participated in the study. The primary objective of this dataset is to better understand the relationship between lifestyle and diabetes in the U.S.

The dataset includes 22 variables ranging from demographic information to laboratory test results and survey responses. Below is a summary of some key variables:

1. Diabetes_binary: Binary target variable indicating whether the patient has diabetes (including prediabetes) or not.
2. HighBP: High blood pressure, indicated as 0 = no, 1 = yes.
3. HighChol: High cholesterol, also indicated as 0 = no, 1 = yes.
4. CholCheck: Whether the patient has had a cholesterol check in the past 5 years (0 = no, 1 = yes).
5. BMI: Body Mass Index, a quantitative measure.
6. Smoker: Whether the patient has smoked at least 100 cigarettes in their life (0 = no, 1 = yes).
7. Stroke: Whether the patient has been diagnosed with a stroke (0 = no, 1 = yes).
8. HeartDiseaseorAttack: Presence of coronary heart disease or a heart attack, indicated as 0 = no, 1 = yes.
9. PhysActivity: Physical activity in the last 30 days, excluding work (0 = no, 1 = yes).
10. Fruits: Daily fruit consumption, indicated as 0 = no, 1 = yes.
11. Veggies: Daily vegetable consumption, indicated as 0 = no, 1 = yes.
12. HvyAlcoholConsump: Heavy alcohol consumption (0 = no, 1 = yes).
13. AnyHealthcare: Whether the individual has any healthcare coverage (0 = no, 1 = yes).
14. NoDocbcCost: Whether the individual could not see a doctor in the past 12 months due to cost (0 = no, 1 = yes).
15. GenHlth: General health status, measured on a scale from 1 (excellent) to 5 (very poor).
16. MentHlth: Mental health, indicated by the number of bad days in the past 30 days.
17. PhysHlth: Physical health, indicated by the number of bad days in the past 30 days.
18. DiffWalk: Difficulty walking or climbing stairs (0 = no, 1 = yes).
19. Sex: Gender of the patient (0 = female, 1 = male).
20. Age: Age, categorized into 13 levels. For example, 1 = 18-24, 9 = 60-64, 13 = 80 or older.
21. Education: Education level, categorized on a scale of 1 to 6, where 1 = never attended school, 2 = grades 1 to 8, 3 = grades 9 to 11, and so on.
22. Income: Income, categorized into 8 levels, where 1 = less than $10,000, 5 = less than $35,000, and 8 = $75,000 or more.

These variables provide a comprehensive view of the risk factors, health conditions, and lifestyle of the participants, allowing for detailed analysis of the relationship between these factors and diabetes.

Source: <https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators>

## Type of Problem

This dataset clearly represents a classification problem because the target variable, "Diabetes_binary," is categorical and indicates the presence or absence of diabetes in participants. In classification problems, the goal is to predict a class label for each instance in the dataset based on the observed features. The distinction between having or not having diabetes represents a fundamental classification between two health states, which is a common objective in health data analysis.

Let's examine the distribution of the data for the dependent variable "Diabetes_binary."

```{r}
n<-nrow(diabetesBRFSS2015)

diabetesBRFSS2015 %>%
  group_by(Diabetes_binary) %>%
  summarize(Frequency = n()/n *100) %>%
  ungroup() %>%
  ggplot(aes(x = Diabetes_binary, y = Frequency, fill =Diabetes_binary)) +
  geom_col(position = position_dodge(), alpha = 0.7) +
  scale_fill_viridis_d() +
  theme_minimal() +
  guides(fill = FALSE) +
  ylim(c(0,100)) +
  xlab("")+ ylab("Percentage %") + ggtitle("Percentage of Diabetic and Healthy people")
```

Fifty percent of our data represents healthy individuals, and the other 50% represents diabetic individuals. This is crucial for creating a robust model with high predictive power.

Now, let’s explore the categorical variables concerning the dependent variable to observe the data and check for trends.

```{r}
data_long <- pivot_longer(diabetesBRFSS2015[,c(-5, -15, -16, -17, -20, -21, -22)], cols = -c(Diabetes_binary), 
                          names_to = "Variable", values_to = "Value")

plots <- list()

for(variable in unique(data_long$Variable)) {
  data_long$Diabetes_binary <- factor(data_long$Diabetes_binary)
  
  data_filtered <- data_long %>%
    filter(Variable == variable) %>%
    group_by(Value, Diabetes_binary) %>%
    summarize(Frequency = n(), .groups = 'drop') %>%
    mutate(Percentage = Frequency / sum(Frequency) * 100) %>%
    ungroup()
  
  p <- ggplot(data_filtered, aes(x = Diabetes_binary, y = Percentage, fill = Value)) +
    geom_col(position = position_dodge(), alpha = 0.7) +
    scale_fill_viridis_d() +
    theme_minimal() +
    labs(title = variable,
         x = "", y = "Percentage") +
    theme(legend.position = "none") +
    theme(plot.title = element_text(size = 12), axis.title = element_text(size = 10))
  
  plots[[variable]] <- p
}

grid.arrange(plots$HighBP, plots$HighChol, plots$CholCheck, plots$Smoker, plots$Stroke, plots$HeartDiseaseorAttack, ncol=3)

grid.arrange(plots$PhysActivity, plots$Fruits, plots$Veggies, plots$HvyAlcoholConsump, plots$AnyHealthcare, plots$NoDocbcCost, ncol=3)

grid.arrange(plots$DiffWalk, plots$Sex, ncol=3)

```

From these bar charts, we can conclude that many of the variables are strongly related to diabetes patients. Some variables are associated with a higher likelihood of having diabetes, such as "HighBP," "HighChol," "CholCheck," "Smoker," "Stroke," "HeartDiseaseorAttack," "DiffWalk," and "Sex" (men are more prone). In contrast, variables such as "PhysHlth," "Fruits," and "Veggies" are associated with a lower likelihood of having diabetes, indicating that physical activity and a healthy diet of fruits and vegetables may help prevent diabetes.

Three variables show minimal variation between healthy and diabetic individuals: "HvyAlcoholConsump," "AnyHealthcare," and "NoDocbcCost."

Next, let's examine the variables with multiple classes.

```{r}
# Incluye Diabetes_binary en el pivot_longer, si es necesario
data_long <- pivot_longer(diabetesBRFSS2015[,c(1,15, 20, 21, 22)], cols = -c(Diabetes_binary), 
                          names_to = "Variable", values_to = "Value")

plots <- list()

for(variable in unique(data_long$Variable)) {
  # Asegura que Diabetes_binary sea factor si aún no lo es
  data_long$Diabetes_binary <- factor(data_long$Diabetes_binary)
  
  # Calcula el porcentaje de Diabetes_binary por cada Value en la variable actual
  data_filtered <- data_long %>%
    filter(Variable == variable) %>%
    group_by(Value, Diabetes_binary) %>%
    summarize(Frequency = n(), .groups = 'drop') %>%
    mutate(Percentage = Frequency / sum(Frequency) * 100) %>%
    ungroup()
  
  # Crea el gráfico
  p <- ggplot(data_filtered, aes(x = Value, y = Percentage, fill = Diabetes_binary)) +
    geom_col(position = position_dodge(), alpha = 0.7) +
    scale_fill_manual(values = c("Healthy" = "#00AFBB", "Diabetic" = "#FC4E07")) +
    theme_minimal() +
    labs(title = variable,
         x = "", y = "Percentage") +
    theme(legend.position = "none") +
    theme(axis.title = element_text(size = 9))
  
  # Guarda el gráfico en la lista
  plots[[variable]] <- p
}


grid.arrange(plots$GenHlth, plots$Age, plots$Education, plots$Income, ncol=2)
```

At this point, we observe some interesting distributions. In these graphs, healthy individuals are represented in blue, and diabetic individuals are in red.

In general, healthy people feel better than diabetic patients, as the score of 1 on the scale represents an "excellent" state. Regarding age, we see that most people interviewed for this study are between 50 and 72 years old (categories 7-11). Diabetes can be primarily classified into two types: Type 1 diabetes and Type 2 diabetes, each with different typical ages of onset. Type 1 diabetes, less common, is usually diagnosed in children, adolescents, or young adults, but it can appear at any age. On the other hand, Type 2 diabetes is more frequent in adults over the age of 45, although it is increasingly being diagnosed in younger people, including teenagers and young adults, due to rising obesity rates. This distribution of data makes sense, as most samples fall into this critical age range for the onset of the disease. However, to determine if age is truly a risk factor, we would need an equal number of samples at every age. Therefore, I consider this variable to have a significant bias in our model.

Another significant bias I observe in these data is shown in the distribution graphs for education and income. We see that the majority of people surveyed for this study are individuals with high socioeconomic status, as they have high levels of education and a substantial percentage of high incomes, especially in the over $75,000 range. By surveying this type of people, other variables are highly skewed when optimizing our model, such as "NoDocbcCost" (unable to afford a doctor), "AnyHealthcare" (having health insurance), or "PhysActivity" (physical activity), as all of these are directly or indirectly dependent on the economic status of the person.

Now, let's study the continuous variables in the dataset.

```{r}
p1 <- ggplot(data=diabetesBRFSS2015, aes(x=BMI, group=Diabetes_binary, fill=Diabetes_binary)) +
  geom_boxplot(alpha=.4) +
  scale_fill_viridis_d()+
  theme_minimal()+
  theme(legend.position = "none",
        axis.text.y=element_blank(), 
        axis.ticks.y=element_blank()) 

p2 <- ggplot(data=diabetesBRFSS2015, aes(x=MentHlth, group=Diabetes_binary, fill=Diabetes_binary)) +
    geom_boxplot(alpha=.4) +
   scale_fill_viridis_d()+
  theme_minimal()+ 
  xlab("MentHlth (days)") +
  theme(axis.text.y=element_blank(), 
        axis.ticks.y=element_blank()) 

p3 <- ggplot(data=diabetesBRFSS2015, aes(x=PhysHlth, group=Diabetes_binary, fill=Diabetes_binary)) +
    geom_boxplot(alpha=.4) +
   scale_fill_viridis_d()+
  theme_minimal()+ 
  xlab("PhysHlth (days)")+
    theme(legend.position = "none",
          axis.text.y=element_blank(), 
        axis.ticks.y=element_blank()) 

grid.arrange(p1, p2, p3, ncol = 1)
```

## Biases

In this case, since 19 out of 22 variables are categorical, I don't consider it a good idea to perform a PCA (Principal Component Analysis) on this data.

In the previous section, I discuss the biases I observe when collecting the data and the type of people surveyed in this study.

## Algorithm Choice

For the diabetesBRFSS2015 dataset, which we've identified as suitable for classification problems, specifically to determine the presence of diabetes, the Naive Bayes algorithm could be an attractive option to start the modeling.

Naive Bayes is a probabilistic classifier based on Bayes' Theorem with the "naive" assumption of independence between features. It is especially popular in text classification tasks but can also be effective in health datasets like this one.

Naive Bayes is known for being a simple and fast model to train, which makes it attractive for large datasets or for initial exploration of the data. It works well with categorical variables, which is relevant to the diabetesBRFSS2015 dataset, as it contains several such variables, although it also works with numeric variables.

The assumption of independence between features rarely holds in practice, which can affect the model's accuracy. Since it is a probabilistic model, it may not be as accurate in classification when the relationships between features are complex or highly correlated.

Logistic regression is often considered a starting point for binary classification. It can handle linear relationships between features and the target variable but may fall short compared to Naive Bayes, which can capture more complex relationships through its probabilistic approach.

## Data transformation

The Naive Bayes algorithm is less sensitive to scaling of numerical features compared to other algorithms, so I don't consider modifying the data necessary.

## Algorithm

First, let's generate the training and test data from the diabetesBRFSS2015 dataset to train the Naive Bayes algorithm. We see that the proportion of healthy and diabetic patients remains consistent in both the training and test sets.

```{r}
set.seed(1234)
diabetes.TrainIdx.80 <- createDataPartition(diabetesBRFSS2015$Diabetes_binary,
                                       p=0.8, #Genera un 80% para train, 20% para test
                                       list = FALSE, #Dame los resultados en una matriz
                                       times = 1) #Genera solamente una partición 80/20

trainSet.diabetes <- diabetesBRFSS2015[diabetes.TrainIdx.80, ]
testSet.diabetes <- diabetesBRFSS2015[-diabetes.TrainIdx.80, ]

table(trainSet.diabetes$Diabetes_binary)
table(testSet.diabetes$Diabetes_binary)
```

Within the caret package, we find the Naive Bayes model, and the specific hyperparameters that can be adjusted depend on the type of Naive Bayes model being used. Generally, Naive Bayes is known for its simplicity and has relatively few hyperparameters compared to other, more complex machine learning models. However, some variations of the algorithm offer different hyperparameters for tuning, especially when modeling the data distribution.

- usekernel: This hyperparameter determines whether to use kernels to estimate the probability density of numerical variables. It is useful when we assume that the data distribution does not necessarily follow a normal distribution. Typical values: TRUE or FALSE. The default value is usually FALSE, meaning that kernels are not used, and a normal (Gaussian) distribution is assumed for the numerical variables. If set to TRUE, kernels are used for a more flexible approximation of the data distribution.

- laplace: This hyperparameter is used when usekernel is TRUE. It refers to the Laplace smoothing factor applied to the kernel density estimates to avoid the problem of zero probability. Typical values are small numeric values (e.g., 0, 0.01, 0.1). A larger value increases the smoothing.

- adjust: Also related to kernel use (when usekernel is TRUE), this hyperparameter adjusts the bandwidth of the kernel used in density estimation. Typical values are greater than 0. A value of 1 means no bandwidth adjustment; values less than 1 reduce it, and values greater than 1 increase it. Adjusting the bandwidth can help refine how the distribution of numerical variables is modeled.

```{r}
modelLookup(("naive_bayes"))
```

Since Naive Bayes is not computationally expensive, I have decided to opt for repetitive cross-validation training with 3 repetitions and 10 partitions. I allow parallel analysis to improve performance.

```{r}
set.seed(1234)
NBControl.diabetes <- trainControl(method = "repeatedcv",
                           number = 10 ,
                           repeats = 3,
                           returnResamp = "final",
                           seeds = NULL,
                           allowParallel = T)
```

In an initial attempt to find the best model, I perform a hyperparameter grid search with the following values, providing 68 combinatory possibilities:

-   laplace: 0, 0.2, 0.4, 0.6, 0.8, 1

-   usekernel: T, F

-   adjust: 0.2, 0.6, 1, 1.4, 1.8, 2.2

```{r}
set.seed(1234)
y = trainSet.diabetes$Diabetes_binary
x = trainSet.diabetes[,-1]

mygrid.diabetes <- expand.grid(laplace = seq(0, 1, 0.2),
                      usekernel = c(T, F),
                      adjust = seq(0.2, 2.2, 0.4))

myfit.diabetes.cv10.rp3.nb <- train(Diabetes_binary ~ ., data = trainSet.diabetes, 
               method = "naive_bayes",
               metric = "Accuracy",
               trControl = NBControl.diabetes,
               tuneGrid=mygrid.diabetes
               )

(myfit.diabetes.cv10.rp3.nb.best <- subset(myfit.diabetes.cv10.rp3.nb$results, laplace == myfit.diabetes.cv10.rp3.nb$bestTune$laplace & usekernel == myfit.diabetes.cv10.rp3.nb$bestTune$usekernel & adjust == myfit.diabetes.cv10.rp3.nb$bestTune$adjust))
plot(myfit.diabetes.cv10.rp3.nb)

```

The best model obtained is laplace = 0, without using a kernel, and adjust = 0.2, although the adjust parameter is not necessary since a kernel is not being used.

As we can see from the following graphs, most iterations of our best training model yield an accuracy around 0.717 and a Kappa statistic of 0.433. These values are quite stable in each iteration as the distribution of the results is very consistent. Accuracy results range between 0.7 and 0.73, and Kappa results between 0.4 and 0.44.

```{r}
resample <- myfit.diabetes.cv10.rp3.nb$resample
p1 <- ggplot(data = resample, aes(x = Accuracy)) +
  geom_density(size = 1.5,color = "#8B1A1A") +
  theme_minimal() +
  labs(title = "Accuracy",
       x = "Accuracy")+
  geom_vline(xintercept = median(resample$Accuracy),
             linetype = "dashed") +
  annotate("text", x = 0.72, y = 5, 
           label = paste("Accuracy", "\n", round(median(resample$Accuracy), 3)), color = "#8B1A1A", size = 4) +
  xlim(c(0.69, 0.73))
  
p2 <- ggplot(data = resample, aes(x = Kappa)) +
  geom_density(size = 1.5, color = "#f1e6b2") +
  theme_minimal() +
  labs(title = "Kappa",
       x = "Kappa")+
  geom_vline(xintercept = median(resample$Kappa),
             linetype = "dashed") +
   annotate("text", x = 0.435, y = 5, 
           label = paste("Kappa", "\n", round(median(resample$Kappa), 3)), color = "#c0b88e", size = 4) +
  xlim(c(0.39, 0.46))

grid.arrange(p1, p2, ncol = 2)
```

In the following table, we see the most important variables for our model. The most important is general health, followed by BMI (Body Mass Index) and high blood pressure.

```{r}
import <- varImp(myfit.diabetes.cv10.rp3.nb)
import$importance <- round(import$importance[order(-import$importance[,1]),], 2)
formattable(import$importance, list(
  area(col= Healthy:Diabetic) ~ color_bar("lightblue")
  ))
```

Next, we will predict the results of the dependent variable using the test set we generated earlier.

```{r, warning=FALSE}
preds.diabetes <- predict(myfit.diabetes.cv10.rp3.nb$finalModel, newdata = testSet.diabetes)

conf.diabetes <- confusionMatrix(preds.diabetes, testSet.diabetes$Diabetes_binary, positive = "Diabetic")
conf.diabetes
```

The confusion matrix shows that:

- 5748 individuals were correctly identified as healthy by the model (true positives).

- 4090 healthy individuals were incorrectly classified as diabetic (false negatives).

- 1321 diabetic individuals were incorrectly classified as healthy (false positives).

- 2979 individuals were correctly identified as diabetic (true negatives).

- Accuracy: 61.73% of all predictions were correct. This value indicates the overall proportion of correct predictions by the model.

- 95% Confidence Interval for Accuracy: Between 60.92% and 62.53%, which provides an estimated range of accuracy in the general population.

- No Information Rate: The largest proportion of the reference classes is 50%. This value is used to compare the model’s accuracy; an accuracy significantly higher than the No Information Rate indicates a useful model.

- P-value for Accuracy Greater Than No Information Rate: Less than 2.2e-16, suggesting the model is significantly better than random guessing.

- Kappa: 0.2345, reflecting weak to moderate agreement beyond chance.

- McNemar’s Test P-value: Less than 2.2e-16, indicating a significant difference between false positives and false negatives.

- Sensitivity: 42.14% of the actual diabetic individuals were correctly identified, indicating a moderately low ability of the model to detect the "Diabetic" class.

- Specificity: 81.31% of the actual healthy individuals were correctly identified, showing the model's high capacity to detect the "Healthy" class.

- Positive Predictive Value: 69.28% of individuals classified as diabetic were actually diabetic.

- Negative Predictive Value: 58.43% of individuals classified as healthy were actually healthy.

- Prevalence of the "Diabetic" Class: 50%, indicating balanced classes in the dataset used for this confusion matrix.

- Detection Rate for "Diabetic": 40.66% of all individuals were correctly identified as diabetic.

- Detection Prevalence for "Diabetic": 30% of the total predictions were for the "Diabetic" class.

- Balanced Accuracy: 61.73%, the same as the overall accuracy, given the balanced class prevalence.

Although the model has good specificity for detecting healthy individuals, its sensitivity and negative predictive value are moderate, indicating limited capacity to correctly classify individuals as healthy. The Kappa coefficient shows weak agreement, suggesting room for improvement in the model's ability to classify instances beyond what would be expected by chance. McNemar's test highlights a significant discrepancy between false positives and false negatives, which could indicate a bias in how the model makes predictions.

Next, let's visualize the confusion matrix for better interpretation.

```{r, warning=FALSE}
preds.diabetes <- predict(myfit.diabetes.cv10.rp3.nb$finalModel, newdata = testSet.diabetes)

conf.diabetes <- confusionMatrix(preds.diabetes, testSet.diabetes$Diabetes_binary)
conf.diabetes_tibble <- as_tibble(conf.diabetes$table)
plot_confusion_matrix(conf.diabetes_tibble, 
                      target_col = "Reference", 
                      prediction_col = "Prediction",
                      counts_col = "n")
```

Now, let's check whether this model has suffered from underfitting.

```{r, warning=FALSE}
# Evaluar el rendimiento en el conjunto de entrenamiento
trainPredictions <- predict(myfit.diabetes.cv10.rp3.nb$finalModel, trainSet.diabetes)
conf_train_diabetes <- confusionMatrix(trainPredictions, trainSet.diabetes$Diabetes_binary)

# Evaluar el rendimiento en el conjunto de prueba
testPredictions <- predict(myfit.diabetes.cv10.rp3.nb$finalModel, testSet.diabetes)
conf_test_diabetes <- confusionMatrix(testPredictions, testSet.diabetes$Diabetes_binary)

conf_train_diabetes$overall
conf_test_diabetes$overall
```

When calculating the accuracy of the predicted values with the training data and the test data, we observe that the accuracy is slightly lower in the training data compared to the test data, indicating that our model has suffered from underfitting. This suggests that we should consider using another model or explore preprocessing alternatives.

In summary, this model predicts many false negatives. In medical cases like this, false positives or false negatives can have serious consequences.
