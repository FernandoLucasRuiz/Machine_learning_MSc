---
title: "Práctica 2 Machine Learning 2023/2024"
subtitle: "Master en Bioinformática, Universidad de Murcia"
author: "Fernando Lucas Ruiz (fernando.lucas@um.es)"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    highlight: kate
    number_sections: no
    theme: spacelab
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Tarea a realizar

En esta práctica, el objetivo es desarrollar modelos predictivos aplicando los algoritmos de Random Forest y Máquinas de Vectores de Soporte (SVM) al dataset de Diabetes del BRFSS. Adicionalmente, implementaremos técnicas de clustering para explorar la presencia de subgrupos dentro de los datos mediante aprendizaje no supervisado. Esta aproximación busca no solo predecir incidencias de la enfermedad, sino también identificar posibles subclases de poblaciones que compartan características similares.

## Librerias

```{r, message=FALSE, warning=FALSE}
library(caret)
library(randomForest)
library(tidyverse)
library(fastDummies)  # Para realizar el one-hot encoding
library(pROC)         # Curvas ROC
library(cvms)         # matriz de confusion
library(ggrepel)      # para notación en gráficas
library(umap)         # UMAP
library(cluster)      # Análisis silhouette
library(gridExtra)    # Gráficas dobles
library(pROC)         # Cálculo Área bajo la curva

library(doParallel) # para paralelizar procesos
num_cores <- detectCores() - 1
registerDoParallel(cores=num_cores)
```

## Cargar los datos

Los datos para este análisis se encuentran en el archivo CSV "diabetesBRFSS2015.csv". Al cargar estos datos, es necesario ajustar manualmente los tipos de datos, dado que no se identifica automáticamente si las variables son categóricas, numéricas o booleanas. Por lo tanto, realizo la conversión de tipos de variables adecuada para cada caso, asegurando así la correcta manipulación y análisis posterior.

```{r}
diabetesBRFSS2015 <- read.csv("diabetesBRFSS2015.csv")
diabetesBRFSS2015[] <- lapply(diabetesBRFSS2015, as.factor)
diabetesBRFSS2015$BMI <- as.numeric(diabetesBRFSS2015$BMI)
diabetesBRFSS2015$MentHlth <- as.numeric(diabetesBRFSS2015$MentHlth)
diabetesBRFSS2015$PhysHlth <- as.numeric(diabetesBRFSS2015$PhysHlth)
diabetesBRFSS2015$Age <- as.numeric(diabetesBRFSS2015$Age)
diabetesBRFSS2015$Diabetes_binary <- factor(diabetesBRFSS2015$Diabetes_binary, levels= c(0,1), labels = c("Healthy", "Diabetic"))
str(diabetesBRFSS2015)
```

```{r}
sum(is.na(diabetesBRFSS2015))
```

## Partición de los datos

En primer lugar, vamos a dividir el conjunto de datos "diabetesBRFSS2015" en subconjuntos de entrenamiento y prueba para entrenar el algoritmo de Random Forest. Nos aseguraremos de que la proporción de pacientes sanos y diabéticos se mantenga consistente entre los datos de entrenamiento y prueba, garantizando así la representatividad y la validez del modelo.

```{r}
set.seed(1234)
diabetes.TrainIdx.80 <- createDataPartition(diabetesBRFSS2015$Diabetes_binary,
                                       p=0.8, #Genera un 80% para train, 20% para test
                                       list = FALSE, # Los resultados en una matriz
                                       times = 1) #Genera solamente una partición 80/20

trainSet.diabetes <- diabetesBRFSS2015[diabetes.TrainIdx.80, ]
testSet.diabetes <- diabetesBRFSS2015[-diabetes.TrainIdx.80, ]

table(trainSet.diabetes$Diabetes_binary)
table(testSet.diabetes$Diabetes_binary)
```

# Random Forest

Es un algoritmo de aprendizaje supervisado utilizado tanto para clasificación como para regresión. Este algoritmo se basa en la idea de combinar múltiples árboles de decisión durante el proceso de entrenamiento para mejorar la precisión y evitar el sobreajuste, problemas comunes cuando se utiliza un solo árbol de decisión. Esto se consigue mediante la selección aleatoria de atributos en cada arbol de decisión. Esto se hace mediante muestreo por reemplazo, por lo que los atributos cambian en cada decisión de cada arbol. Esta estrategia de selección aleatoria de características contribuye a la diversidad entre los árboles y aumenta la robustez del modelo.

Para realizar una predicción, en el caso de la clasificación, cada árbol da un resultado de qué clase ha predicho, y la clase con más votos es la predicción del modelo. Para la regresión, el promedio de las predicciones de todos los árboles es el resultado final.

Para este trabajo vamos a utilizar el paquete "randomForest" de "Caret" y para este paquete no se necesita un preprocesamiento de los datos categóricos por One-hot encoding ya que el propio paquete lo solventa. Tampoco es necesario imputar valores nulos (en nuestro caso no tenemos valores nulos).

## Selección hiperparámetros

Este paquete de Random Forest (rf) solamente tiene un hiperparámetro que podemos modificar que es mtry. Representa el número de variables que se deben considerar en cada división de un árbol. Por defecto, este valor se establece como la raíz cuadrada del número total de variables en el caso de la clasificación, y un tercio del número total de variables en el caso de la regresión. Este hiperparámetro es crucial porque controla el grado de aleatoriedad en la construcción de árboles. Un mtry bajo aumenta la diversidad entre los árboles, reduciendo la varianza pero aumentando el sesgo. Un mtry alto hace lo contrario.

También podemos modificar el número de árboles en el bosque (ntree). Este valor determina cuántos árboles individuales se construirán antes de hacer una predicción final basada en la agregación de sus resultados. Generalmente, más árboles mejoran la precisión del modelo hasta cierto punto, pero también incrementan el tiempo de computación y pueden llegar a un punto de rendimientos decrecientes.

Para buscar el mejor modelo realizo una parrilla de hiperparámetros con los siguientes valores dando 12 posibilidades combinatorias:

-   mtry: 2, 3, 4, 5

-   trees: 500, 1000, 1500, 2000

Para mtry, según las recomendaciones para modelos clasificatorios habría que probar desde 2 hasta $\sqrt{n\_atributos}$.

```{r}
modelLookup(("rf"))
```

## Entrenamiento

Para este modelo, he optado por entrenar los datos utilizando validación cruzada sin repetición. Dado que la dimensionalidad de estos datos es considerablemente alta, este método es más costoso a nivel computacional, pero es esencial para mantener la consistencia de los datos y evitar problemas de sobreajuste (overfitting) o subajuste (underfitting). Además, he habilitado el análisis en paralelo para optimizar el rendimiento.

```{r}
set.seed(1234)
RFControl.diabetes <- trainControl(method = "cv",
                           number = 10 , # 10 particiones
                           returnResamp = "final", # devuelve la métrica final
                           seeds = NULL,
                           allowParallel = T) # permitir la paralelización
```

Debido a las limitaciones computacionales de mi portátil para manejar estas cargas, he optado por escribir un script que se ejecutará en un entorno Bash, gestionado por el gestor de colas SLURM de Dayhoff.

Inicialmente, he creado un R script denominado "train_model_rf.R", que incluye un bucle para variar el número de árboles en el Random Forest en cada iteración. Este script está diseñado para adaptarse y modificar la configuración del modelo eficientemente.

Posteriormente, he desarrollado un script de Bash llamado "modelo_script_rf.sh", que se encarga de invocar al script de R anteriormente mencionado para su ejecución. Finalmente, lanzo este Shell Script utilizando el comando "sbatch" dentro del sistema de gestión de colas SLURM, facilitando así una ejecución eficiente y optimizada.

```{r, eval=FALSE}
# R Script para generar los modelos de Random Forest para ejecutarlos en dayhoff

library(caret)
library(tidyverse)
library(doParallel) # para paralelizar procesos
num_cores <- detectCores()
registerDoParallel(cores=num_cores)

diabetesBRFSS2015 <- read.csv("diabetesBRFSS2015.csv")
diabetesBRFSS2015[] <- lapply(diabetesBRFSS2015, as.factor)
diabetesBRFSS2015$BMI <- as.numeric(diabetesBRFSS2015$BMI)
diabetesBRFSS2015$MentHlth <- as.numeric(diabetesBRFSS2015$MentHlth)
diabetesBRFSS2015$PhysHlth <- as.numeric(diabetesBRFSS2015$PhysHlth)
diabetesBRFSS2015$Age <- as.numeric(diabetesBRFSS2015$Age)
diabetesBRFSS2015$Diabetes_binary <- factor(diabetesBRFSS2015$Diabetes_binary, levels= c(0,1), labels = c("Healthy", "Diabetic"))


set.seed(1234)
diabetes.TrainIdx.80 <- createDataPartition(diabetesBRFSS2015$Diabetes_binary,
                                            p=0.8, #Genera un 80% para train, 20% para test
                                            list = FALSE, #Dame los resultados en una matriz
                                            times = 1) #Genera solamente una partición 80/20

trainSet.diabetes <- diabetesBRFSS2015[diabetes.TrainIdx.80, ]
testSet.diabetes <- diabetesBRFSS2015[-diabetes.TrainIdx.80, ]

RFControl.diabetes <- trainControl(method = "cv",
                                   number = 10 ,
                                   returnResamp = "final",
                                   seeds = NULL,
                                   allowParallel = T)

mygrid.diabetes <- expand.grid(mtry = c(2:round(sqrt(ncol(diabetesBRFSS2015)))))
trees <- seq(500, 2000, 500) 

rf.cv.10 <- list() # lista vacia para meter los resultados de cada bucle
for (tree in trees){ # bucle en cada pasada hace un modelo con un número de arboles distintos
  modelo <- train(Diabetes_binary~.,data=trainSet.diabetes,
                  method="rf",
                  tuneGrid=mygrid.diabetes,
                  trControl=RFControl.diabetes, 
                  ntree = tree
  )
  rf.cv.10[[paste(tree, "trees")]] <- modelo # metemos el modelo en la lista
}

saveRDS(rf.cv.10, "rf.cv.10.RDS") # guardamos los modelos
```

```{bash, eval=FALSE}
# Shell script para generar los modelos de Random Forest para ejecutarlos en dayhoff.

#!/bin/bash

#SBATCH --job-name=rf_training    # Nombre del trabajo
#SBATCH --output=rf_%u.%x.%j.out        # Salida estandar (stdout)
#SBATCH --error=rf_%u.%x.%j.err         # Salida de error (stderr)
#SBATCH --cpus-per-task=4       #
#SBATCH --chdir=/home/alumno14/machine_learning

module load R                     
Rscript /home/alumno14/machine_learning/train_model_rf.R  
```


## Análisis

Una vez tengamos los resultados los cargamos de nuevo para analizarlos.

```{r}
rf.cv.10 <- readRDS("rf.cv.10.RDS")
```

Como podemos ver, el modelo con unas métricas mejores contiene 2000 arboles y 5 variables en cada división del arbol tanto para la métrica de Accuracy como de Kappa.

```{r, fig.height=5, fig.height=5}
# Hacemos dataframe con las métricas de cada modelo
datos_combinados <- rbind(
  data.frame(mtry = rf.cv.10$`500 trees`$results$mtry, MetricValue = rf.cv.10$`500 trees`$results$Accuracy, Trees = "500 trees", Metric = "Accuracy"),
  data.frame(mtry = rf.cv.10$`500 trees`$results$mtry, MetricValue = rf.cv.10$`500 trees`$results$Kappa, Trees = "500 trees", Metric = "Kappa"),
  data.frame(mtry = rf.cv.10$`1000 trees`$results$mtry, MetricValue = rf.cv.10$`1000 trees`$results$Accuracy, Trees = "1000 trees", Metric = "Accuracy"),
  data.frame(mtry = rf.cv.10$`1000 trees`$results$mtry, MetricValue = rf.cv.10$`1000 trees`$results$Kappa, Trees = "1000 trees", Metric = "Kappa"),
  data.frame(mtry = rf.cv.10$`1500 trees`$results$mtry, MetricValue = rf.cv.10$`1500 trees`$results$Accuracy, Trees = "1500 trees", Metric = "Accuracy"),
  data.frame(mtry = rf.cv.10$`1500 trees`$results$mtry, MetricValue = rf.cv.10$`1500 trees`$results$Kappa, Trees = "1500 trees", Metric = "Kappa"),
  data.frame(mtry = rf.cv.10$`2000 trees`$results$mtry, MetricValue = rf.cv.10$`2000 trees`$results$Accuracy, Trees = "2000 trees", Metric = "Accuracy"),
  data.frame(mtry = rf.cv.10$`2000 trees`$results$mtry, MetricValue = rf.cv.10$`2000 trees`$results$Kappa, Trees = "2000 trees", Metric = "Kappa")
)

datos_combinados %>%
  ggplot(aes(x = mtry, y = MetricValue, color = Trees)) +
  geom_point(size = 4) +
  geom_line(linetype = "dashed") +
  theme_minimal() +
  labs(x = "Mtry", y = "Valor de la Métrica", title = "Comparación de Precisión y Kappa del Random Forest") +
  facet_wrap(~ Metric, scales = "free_y")

```

El "mean decrease Gini" es una métrica utilizada en modelos de Random Forest para evaluar la importancia de las variables. Se basa en la impureza de Gini, que indica la frecuencia con la que un elemento sería incorrectamente etiquetado si la asignación fuera aleatoria, basada en la distribución de etiquetas del conjunto. Este valor se calcula promediando las reducciones de impureza de Gini que una variable aporta en todos los árboles del modelo. Una alta puntuación en esta métrica señala que la variable es crucial para el modelo, al contribuir significativamente a la precisión y reducir la incertidumbre en las predicciones.

Cuando nos fijamos en las variables mas importantes en nuestro modelo vemos que el indice de masa corporal es el que tiene más impacto seguido de un indice de presión arterial alto.

```{r, fig.height=8, fig.height=8}
rf.cv.10$`2000 trees`$finalModel$importance %>%
  as.data.frame() %>%
  arrange(MeanDecreaseGini) %>%   # ordenamos por indice Gini
  mutate(name=factor(rownames(.), levels=rownames(.))) %>%   # This trick update the factor levels
  ggplot( aes(x=name, y=MeanDecreaseGini)) +
    geom_segment( aes(xend=name, yend=0)) +
    geom_point( size=4, color="orange") +
    coord_flip() +
    theme_bw() +
    xlab("")
```

# SVM

Las Máquinas de Vectores de Soporte (SVM, por sus siglas en inglés, Support Vector Machines) son un conjunto de métodos de aprendizaje supervisado utilizados para clasificación, regresión y detección de outliers. Las SVM trabajan construyendo un hiperplano o conjunto de hiperplanos en un espacio de alta dimensión. 

Para problemas de clasificación como es el que tenemos en estos datos, SVM intenta encontrar el hiperplano que mejor separa las clases en el espacio de características. El mejor hiperplano es aquel que tiene la mayor distancia a los puntos de datos más cercanos de cada clase, conocidos como vectores de soporte. Por otro lado, los datos lejanos a los hiperplanos no son tomados en cuenta. En problemas de regresión, SVM intenta encontrar un hiperplano que se ajuste lo mejor posible a los datos dentro de un margen definido, intentando capturar la mayor cantidad de puntos posibles dentro de este margen.

Una de las características más poderosas de SVM es su capacidad de utilizar funciones kernel para transformar el espacio de características a una dimensión más alta donde las clases se pueden separar linealmente, permitiendo a SVM manejar datos que no son linealmente separables en su forma original. Tambien, las SVM son particularmente eficaces en espacios de alta dimensión. A través del uso del kernels, SVM puede adaptarse a diferentes tipos de datos y relaciones funcionales entre las características y las etiquetas.

Por otro lado, elegir la función de kernel adecuada y configurar los parámetros del kernel puede ser complicado y requiere de una selección cuidadosa a través de la experimentación y validación cruzada. SVM tiende a ser menos eficiente a medida que el tamaño del conjunto de datos aumenta.

## Preprocesamiento

Las SVM requieren un procesamiento previo de los datos para optimizar el rendimiento del algoritmo. Este procesamiento incluye la aplicación de la técnica de One-hot encoding a las variables categóricas, transformándolas en un formato que el modelo pueda procesar eficazmente. Adicionalmente, es crucial escalar todos los datos antes de alimentarlos al algoritmo SVM, ya que este necesita que las variables estén normalizadas para mejorar su eficiencia y precisión.

Guardo los datos procesados para poder ejecutarlos en dayhoff mediante SLURM.

```{r}
diabetes.processed <- data.frame(matrix(ncol = 0, nrow = nrow(diabetesBRFSS2015))) # Hago un dataframe vacio para meter los datos procesados

# paso un bucle por mis datos de diabetes y si las columnas son categóricas les paso un one-hot encoding
for (colname in names(diabetesBRFSS2015)) { 
  if (is.factor(diabetesBRFSS2015[[colname]]) || is.character(diabetesBRFSS2015[[colname]])) {
    
    dummy_df <- dummy_cols(diabetesBRFSS2015[colname],
                           remove_selected_columns = TRUE, # quitar las variables iniciales
                           remove_first_dummy = TRUE) # quitar la variable 0
    
    diabetes.processed <- cbind(diabetes.processed, dummy_df) 
    
  } else {
    
    diabetes.processed[[colname]] <- diabetesBRFSS2015[[colname]] # si son numéricas o de otra clase, las añadimos igual
  }
}

# Escalar las variables y añadir la variable a predecir de diabetes
diabetes.processed <- data.frame(cbind("Diabetes_binary_Diabetic" = diabetes.processed$Diabetes_binary_Diabetic, scale(diabetes.processed[,-1])))

diabetes.processed$Diabetes_binary_Diabetic <- factor(diabetes.processed$Diabetes_binary_Diabetic, levels= c(0,1), labels = c("Healthy", "Diabetic"))

str(diabetes.processed)

write_csv(x = diabetes.processed, file = "diabetes.processed.csv")

```

## Partición de los datos preprocesados

Vamos a dividir el conjunto de datos procesados "diabetesBRFSS2015" en subconjuntos de entrenamiento y prueba para entrenar el algoritmo de SVM. Nos aseguraremos de que la proporción de pacientes sanos y diabéticos se mantenga constante entre los datos de entrenamiento y los de prueba, garantizando así la representatividad y equidad del modelo.

```{r}
set.seed(1234)
diabetes.TrainIdx.80.processed <- createDataPartition(diabetes.processed$Diabetes_binary_Diabetic,
                                       p=0.8, #Genera un 80% para train, 20% para test
                                       list = FALSE, #Dame los resultados en una matriz
                                       times = 1) #Genera solamente una partición 80/20

trainSet.diabetes.processed <- diabetes.processed[diabetes.TrainIdx.80.processed, ]
testSet.diabetes.processed <- diabetes.processed[-diabetes.TrainIdx.80.processed, ]

table(trainSet.diabetes.processed$Diabetes_binary_Diabetic)
table(trainSet.diabetes.processed$Diabetes_binary_Diabetic)
```

## Entrenamiento

Voy a realizar una evaluación comparativa entre diferentes kernels para determinar cuál ofrece el mejor desempeño con nuestro conjunto de datos, dado que no está claro si las relaciones entre las variables son lineales. Para esto, utilizaré los métodos svmLinear, svmRadial y svmPoly del paquete kernlab, integrado en Caret.

Dada la alta dimensionalidad de estos datos, he optado por emplear validación cruzada sin repetición en el entrenamiento de los modelos. Aunque este enfoque incrementa la carga computacional, es fundamental para asegurar la consistencia de los datos y prevenir el riesgo de overfitting o underfitting. Además, he habilitado el procesamiento paralelo para mejorar la eficiencia y acelerar el rendimiento de los análisis.

```{r}
set.seed(1234)
svm.Control.diabetes <- trainControl(method = "cv",
                           number = 10 , # 10 particiones
                           returnResamp = "final", # devuelve la métrica final
                           seeds = NULL,
                           allowParallel = T) # permitir la paralelización
```

Al igual que el Random Forest, voy a lanzar con SLURM en dayhoff un trocito de código para generar estos modelos. 

### SVM linear

El kernel lineal en SVM es una de las funciones de kernel más simples y se utiliza para aprender clasificadores o regresiones lineales en el espacio de entrada original. Utiliza el producto escalar entre dos puntos en su espacio original para determinar su similitud. El kernel lineal no transforma los datos a un espacio de mayor dimensionalidad. Esto lo hace menos flexible para capturar relaciones no lineales, pero más eficiente en términos de cómputo y menos propenso a sobreajustar cuando las relaciones subyacentes en los datos son, de hecho, lineales. Es particularmente adecuado para datos que ya son linealmente separables o cuando se tiene un número muy alto de características en comparación con las muestras. 

#### Selección de hiperparámetros

Solamente tiene un hiperparámetro a modular que es el coste. El coste controla el compromiso entre clasificar correctamente los puntos de entrenamiento y maximizar el margen de decisión del hiperplano. Es decir, determina la tolerancia al error y la regularización del modelo. Un valor más alto de C pone más énfasis en clasificar todos los puntos de entrenamiento correctamente, lo cual puede llevar a un modelo más complejo y potencialmente sobreajustado. Por el contrario, un valor más bajo de C enfatiza la simplicidad del modelo, lo que puede aumentar el número de errores de clasificación en el conjunto de entrenamiento pero puede mejorar la generalización.

Elijo una combinación de 4 costes para el grid: 
- 0.01, 0.1, 1, 10

```{r}
modelLookup(("svmLinear"))
```

```{r, eval=FALSE}
# R Script para generar los modelos de SVM linear para ejecutarlos en dayhoff

library(caret)
library(tidyverse)
library(doParallel) # para paralelizar procesos
num_cores <- detectCores()
registerDoParallel(cores=num_cores)

diabetes.processed <- read.csv("diabetes.processed.csv")

diabetes.TrainIdx.80.processed <- createDataPartition(diabetes.processed$Diabetes_binary_Diabetic,
                                            p=0.8, #Genera un 80% para train, 20% para test
                                            list = FALSE, #Dame los resultados en una matriz
                                            times = 1) #Genera solamente una partición 80/20

trainSet.diabetes.processed <- diabetes.processed[diabetes.TrainIdx.80.processed, ]
testSet.diabetes.processed <- diabetes.processed[-diabetes.TrainIdx.80.processed, ]

svm.Control.diabetes <- trainControl(method = "cv",
                                     number = 10 ,
                                     returnResamp = "final",
                                     seeds = NULL,
                                     allowParallel = T)

tuneGrid.svm.linear <- expand.grid(C = c(0.01, 0.1, 1, 10))

svm.linear.model <- train(Diabetes_binary_Diabetic ~ ., data = trainSet.diabetes.processed,
                          method = "svmLinear",
                          trControl = svm.Control.diabetes)

saveRDS(svm.linear.model, "svm.linear.model.RDS")
```

```{bash, eval=FALSE}
# Shell Script para generar los modelos de SVM linear para ejecutarlos en dayhoff

#!/bin/bash

#SBATCH --job-name=svm_training    # Nombre del trabajo
#SBATCH --output=svm_%u.%x.%j.out        # Salida estandar (stdout)
#SBATCH --error=svm_%u.%x.%j.err         # Salida de error (stderr)
#SBATCH --cpus-per-task=4	#
#SBATCH --chdir=/home/alumno14/machine_learning

module load R                     
Rscript /home/alumno14/machine_learning/train_model_svm_linear.R   
```

### SVM radial

El kernel SVM radial trata con datos que no son linealmente separables. Este kernel transforma los datos a un espacio dimensional más alto donde es posible encontrar un hiperplano que separe mejor las clases. Este kernel es extremadamente útil cuando la relación entre las variables de clase no es lineal, proporcionando una manera poderosa de capturar estructuras complejas en los datos, lo que a menudo resulta en un mejor rendimiento de clasificación comparado con el kernel lineal, especialmente en casos donde las relaciones entre variables son intrincadas y más difíciles de modelar con hiperplanos lineales.

#### Selección de hiperparámetros

Ahora vamos a probar el svm Radial. Al igual que el kernel linear tenemos el parámetro de coste. En este caso podemos modular otro parámetro llamado sigma. Este hiperparámetro controla la escala de la distancia en el espacio de características transformado. Un valor pequeño de sigma significa que el efecto de un punto es más localizado, lo que lleva a fronteras de decisión que pueden ajustarse más estrechamente alrededor de los puntos de datos individuales, aumentando el riesgo de sobreajuste. Por otro lado, un valor grande de sigma extiende la influencia de los puntos de entrenamiento, lo que puede conducir a un modelo más suavizado o generalizado, pero posiblemente a costa de no capturar suficientemente la complejidad de los datos.

Para este kernel he elegido los siguientes hiperparámetros:
- C: 0.001, 0.01, 0.1, 1, 10
- sigma: 0.001, 0.01, 0.1, 1, 10

```{r}
modelLookup(("svmRadial"))
```

```{r, eval=FALSE}
# R Script para generar los modelos de SVM linear para ejecutarlos en dayhoff

library(caret)
library(tidyverse)
library(doParallel) # para paralelizar procesos
num_cores <- detectCores()
registerDoParallel(cores=num_cores)

diabetes.processed <- read.csv("diabetes.processed.csv")

diabetes.TrainIdx.80.processed <- createDataPartition(diabetes.processed$Diabetes_binary_Diabetic,
                                            p=0.8, #Genera un 80% para train, 20% para test
                                            list = FALSE, #Dame los resultados en una matriz
                                            times = 1) #Genera solamente una partición 80/20

trainSet.diabetes.processed <- diabetes.processed[diabetes.TrainIdx.80.processed, ]
testSet.diabetes.processed <- diabetes.processed[-diabetes.TrainIdx.80.processed, ]

svm.Control.diabetes <- trainControl(method = "cv",
                                     number = 10 ,
                                     returnResamp = "final",
                                     seeds = NULL,
                                     allowParallel = T)

tuneGrid.svm.radial <- expand.grid(C = c(0.001, 0.01, 0.1, 1, 10), sigma = c(0.001, 0.01, 0.1, 1, 10))

svm.radial.model <- train(Diabetes_binary_Diabetic ~ ., data = trainSet.diabetes.processed,
                          method = "svmRadial",
                          tuneGrid = tuneGrid.svm.radial,
                          trControl = svm.Control.diabetes)

saveRDS(svm.radial.model, "svm.radial.model.RDS")
```

```{bash, eval=FALSE}
# Shell Script para generar los modelos de SVM linear para ejecutarlos en dayhoff

#!/bin/bash

#SBATCH --job-name=svm_training    # Nombre del trabajo
#SBATCH --output=svm_%u.%x.%j.out        # Salida estandar (stdout)
#SBATCH --error=svm_%u.%x.%j.err         # Salida de error (stderr)
#SBATCH --cpus-per-task=4	#
#SBATCH --chdir=/home/alumno14/machine_learning

module load R                     
Rscript /home/alumno14/machine_learning/train_model_svm_radial.R   
```

### SVM polynomial

Finalmente, entrenamos al modelo con un SVM polinomial (svmPoly). Al igual que el kernel Radial, este método es particularmente útil para datos que no son linealmente separables, ya que el kernel polinomial puede mapear los datos originales a un espacio de características de mayor dimensión donde las clases pueden ser separadas más fácilmente por un hiperplano. 

#### Selección de hiperparámetros

Este kernel tiene tres hiperparámetros a modificar:

El degree define el grado del polinomio utilizado. Un grado más alto significa que el kernel será capaz de modelar interacciones más complejas entre las características.Un grado mayor generalmente permite capturar patrones más complejos en los datos, pero también puede hacer que el modelo sea más propenso al sobreajuste, especialmente si no hay suficientes datos de entrenamiento. Por lo general, se empieza con un grado bajo (como 2 o 3) y se incrementa si es necesario, basándose en la validación cruzada.

El scale ajusta la escala de las características antes de aplicar el kernel polinomial. En otras palabras, controla la influencia de las características individuales en la función del kernel.

El coste es común en todos los kernel.

Para este caso no voy a cambiar los hiperparámetros ya que se va a generar una gran cantidad de posibilidades con los hiperparámetros por defecto que lanza el propio Caret.

```{r}
modelLookup(("svmPoly"))
```

```{r, eval=FALSE}
# R Script para generar los modelos de SVM linear para ejecutarlos en dayhoff

library(caret)
library(tidyverse)
library(doParallel) # para paralelizar procesos
num_cores <- detectCores()
registerDoParallel(cores=num_cores)

diabetes.processed <- read.csv("diabetes.processed.csv")

diabetes.TrainIdx.80.processed <- createDataPartition(diabetes.processed$Diabetes_binary_Diabetic,
                                            p=0.8, #Genera un 80% para train, 20% para test
                                            list = FALSE, #Dame los resultados en una matriz
                                            times = 1) #Genera solamente una partición 80/20

trainSet.diabetes.processed <- diabetes.processed[diabetes.TrainIdx.80.processed, ]
testSet.diabetes.processed <- diabetes.processed[-diabetes.TrainIdx.80.processed, ]

svm.Control.diabetes <- trainControl(method = "cv",
                                     number = 10 ,
                                     returnResamp = "final",
                                     seeds = NULL,
                                     allowParallel = T)

svm.poly.model <- train(Diabetes_binary_Diabetic ~ ., data = trainSet.diabetes.processed,
                        method = "svmPoly",
                        trControl = svm.Control.diabetes)



saveRDS(svm.poly.model, "svm.poly.model.RDS")
```

```{bash, eval=FALSE}
# Shell Script para generar los modelos de SVM linear para ejecutarlos en dayhoff

#!/bin/bash

#SBATCH --job-name=svm_training    # Nombre del trabajo
#SBATCH --output=svm_%u.%x.%j.out        # Salida estandar (stdout)
#SBATCH --error=svm_%u.%x.%j.err         # Salida de error (stderr)
#SBATCH --cpus-per-task=4	#
#SBATCH --chdir=/home/alumno14/machine_learning

module load R                     
Rscript /home/alumno14/machine_learning/train_model_svm_poly.R 
```

## Análisis

Cargo los datos de los modelos ejecutados en dayhoff.

```{r}
svm.linear.model <- readRDS("svm.linear.model.RDS")
svm.radial.model <- readRDS("svm.radial.model.RDS")
svm.poly.model <- readRDS("svm.poly.model.RDS")
```

Aunque parece muy estable según el coste, para el modelo lineal el mejor modelo encontrado es con un coste de 0.1, con un Accuracy rondando el 0.75.

```{r}
svm.linear.model$bestTune
```
```{r}
ggplot(svm.linear.model$results, aes(x = C, y = Accuracy)) +
  scale_x_log10()+
  theme_minimal()+ 
  geom_point(size= 3, color ="#c0b88e") +
  geom_line(linetype = "dashed", color = "#8B1A1A") +
  labs(x = "Cost", y = "Accuracy", title = "SVM linear", color = "Cost")
```

El mejor modelo encontrado en el SVM Radial es con un coste de 10 y un sigma de 0.001. Lo podemos ver también en la gráfica. Con un valor de Sigma bajo se comporta mejor que con los altos, aunque con un coste bajo, el modelo se comporta bastante deficiente. También, El accuracy ronda el 0,75. 

```{r}
svm.radial.model$bestTune
```

```{r}
ggplot(svm.radial.model$results, aes(x = sigma, y = Accuracy, color = as.factor(C))) +
  scale_x_log10()+
  theme_minimal()+ 
  geom_point(size= 3) +
  geom_line(linetype = "dashed") +
  labs(x = "Sigma", y = "Accuracy", title = "SVM radial", color = "Cost")
```

Para el kernel polinomial, los mejores hiperparámetros son 2 grados, escala 0.01 y coste de 0.25. Los valores son bastante estables, a excepción del modelo con escala 0.1 y 3 grados, que baja considerablemente su rendimiento.

```{r}
svm.poly.model$bestTune
```

```{r}
ggplot(svm.poly.model$results, aes(x = degree, y = Accuracy, color = as.factor(scale))) +
  facet_wrap(~C) +
  theme_minimal()+ 
  geom_point(size= 3) +
  geom_line(linetype = "dashed") +
  labs(x = "Degree", y = "Accuracy", title = "SVM polinomial", color = "Scale")
```

# Comparación de modelos
## Accuracy y Kappa
Primero, para comparar los modelos hay que juntar todos los modelos mediante la función de Caret resamples(). A simple vista en los valores numéricos no se aprecia una gran diferencia entre las métricas de Accuracy y Kappa de los modelos.

```{r}
models <- list(RF=rf.cv.10$`2000 trees`, 
               svm.linear=svm.linear.model, 
               svm.radial = svm.radial.model, 
               svm.poly = svm.poly.model)
models.resample <- resamples(models)
summary(models.resample)
```

No se aprecian diferencias significativas entre los modelos al analizar los boxplots. Como podemos observar, los resultados son bastante similares entre sí, aunque el modelo de Random Forest muestra un desempeño ligeramente inferior en comparación con los demás. Esto sugiere que las diferencias en la eficacia de los modelos son mínimas para este conjunto de datos.

```{r}
p1 <- bwplot(models.resample, main="bwplot", xlim= c(0.7, 0.8), metric = "Accuracy")

p2 <- bwplot(models.resample, main="bwplot", xlim= c(0.45, 0.55), metric = "Kappa")

grid.arrange(p1, p2, ncol= 2)
```

El siguiente paso es determinar si existe una diferencia significativa en el desempeño entre los modelos, evaluando si alguno supera a los otros. Al observar los resultados en la diagonal inferior, tanto para el índice Kappa como para la Accuracy, no encontramos diferencias significativas entre los modelos. Por lo tanto, no podemos rechazar la hipótesis nula de que todos los modelos tienen un rendimiento equivalente. La diagonal superior indica la diferencia en valor de la mediana de cada modelo, y como vemos las diferencias son mínimas.

```{r}
comparacion <- diff(models.resample, adjustment = "none", metric = c("Accuracy", "Kappa"))
summary(comparacion)
```

## Specificity y Sensitivity.

Vamos a ver si alguno de los modelos ofrece un mejor desempeño en términos de Specificity o Sensitivity. Para ello, calcularé estos parámetros en cada uno de los modelos.

Es importante mencionar que, debido a que inicialmente no apliqué la técnica de One-hot encoding en el modelo de Random Forest en los datos iniciales, necesito implementarla ahora para las predicciones. Esto se debe a que el modelo de Random Forest realiza internamente un One-hot encoding, y la salida de las variables están separadas en distintas columnas categóricas. Por eso, los datos de entrenamiento tienen que pasarse por un One-hot encoding para realizar la predicción.

```{r, warning=FALSE}
# A todas las variables categóricas del conjunto de entrenamiento
dummys <- trainSet.diabetes %>%
  select_if(is.factor) %>%
  dummy_columns(remove_selected_columns = TRUE, # quitar las variables primigenias
                remove_first_dummy = TRUE) # quitar la variable 0

colnames(dummys) <- gsub("_", "", colnames(dummys)) # Quitar las "_" para que se parezca a los nombres del modelo

trainSet.diabetes.dummy <- trainSet.diabetes %>% 
  select(-is.factor) %>%
  cbind(dummys) # unir las variables dummys al conjunto modificado de entrenamiento

trainSet.diabetes.dummy$DiabetesbinaryDiabetic <- factor(trainSet.diabetes.dummy$DiabetesbinaryDiabetic, levels= c(0,1), labels = c("Healthy", "Diabetic"))
```

Aunque las diferencias no son muy pronunciadas, se observa que los modelos con mayor Sensitivity tienden a mostrar menor Specificity Basándonos en las métricas de Accuracy, Kappa, Sensitivity y Specificity, no podemos concluir definitivamente que un modelo sea superior a otro. Esta compensación entre Sensibilidad y Especificidad sugiere que la elección del modelo podría depender del contexto específico y de las prioridades de la aplicación.

```{r, eval=FALSE}
# predecir el comportamiento de los modelos
predict.rf.model <- predict(object = rf.cv.10$`2000 trees`$finalModel, data = trainSet.diabetes)

predict.svm.linear.model <- predict(svm.linear.model, data = trainSet.diabetes)

predict.svm.radial.model <- predict(svm.radial.model, data = trainSet.diabetes)

predict.svm.poly.model <- predict(svm.poly.model, data = trainSet.diabetes)

predicciones1 <- list(predict.rf.model = predict.rf.model, predict.svm.linear.model = predict.svm.linear.model, predict.svm.radial.model = predict.svm.radial.model, predict.svm.poly.model = predict.svm.poly.model)

saveRDS(predicciones1, "predicciones1.RDS")
```

```{r}
predicciones1 <- readRDS("predicciones1.RDS")
predict.rf.model <- predicciones1$predict.rf.model
predict.svm.linear.model <- predicciones1$predict.svm.linear.model
predict.svm.radial.model <- predicciones1$predict.svm.radial.model
predict.svm.poly.model <- predicciones1$predict.svm.poly.model

# Matrices de confusión
conf.rf <- confusionMatrix(predict.rf.model, trainSet.diabetes.dummy$DiabetesbinaryDiabetic, positive = "Diabetic")

conf.svm.linear.model <- confusionMatrix(predict.svm.linear.model, as.factor(trainSet.diabetes.processed$Diabetes_binary_Diabetic), positive = "Diabetic")

conf.svm.radial.model <- confusionMatrix(predict.svm.radial.model, as.factor(trainSet.diabetes.processed$Diabetes_binary_Diabetic), positive = "Diabetic")

conf.svm.poly.model <- confusionMatrix(predict.svm.poly.model, as.factor(trainSet.diabetes.processed$Diabetes_binary_Diabetic), positive = "Diabetic")

# hacer dataframe con las métricas de los modelos
df.sens.spec <- data.frame(Sensitivity = c(conf.rf$byClass['Sensitivity'], conf.svm.linear.model$byClass['Sensitivity'],conf.svm.radial.model$byClass['Sensitivity'],conf.svm.poly.model$byClass['Sensitivity']),
           Specificity = c(conf.rf$byClass['Specificity'], conf.svm.linear.model$byClass['Specificity'], conf.svm.radial.model$byClass['Specificity'], conf.svm.poly.model$byClass['Specificity']),
           Model = c("RF", "SVM linear", "SVM Radial", "SVM Polinomial")
           )

# pivotar el dataframe para plotearlo
df.long <- tidyr::pivot_longer(df.sens.spec, 
                               cols = c("Sensitivity", "Specificity"), 
                               names_to = "Metric", 
                               values_to = "Value")


ggplot(df.long, aes(x = Model, y = Value, color = Metric)) +
  geom_point(size = 3) +  
  scale_color_manual(values = c("Sensitivity" = "red", "Specificity" = "blue"),
                     labels = c("Sensitivity" = "Sensitivity", "Specificity" = "Specificity")) +
  labs(x = "Model", y = "Value", title = "Sensitivity and Specificity in train data",
       color = "Metric") +  
  theme_minimal() + 
  theme(legend.position = "right") 
```

## Predicción de modelos con los datos de test

Finalmente, procederé a evaluar la capacidad predictiva de cada modelo utilizando el conjunto de datos de prueba.

Al intentar realizar predicciones con el modelo de Random Forest utilizando los datos de prueba, me encuentro con un error que indica que el conjunto de prueba no tiene el mismo número de variables que el modelo entrenado. Este problema surge porque el algoritmo de Random Forest realizó un One-hot encoding interno de las variables categóricas con múltiples clases, generando nuevas variables que no están presentes en el conjunto de datos de prueba.

```{r}
rownames(rf.cv.10$`2000 trees`$finalModel$importance)
```

Por tanto, ahora realizamos el One-hot encoding del dataset de los datos para el test.

```{r, warning=FALSE}
dummys <- testSet.diabetes %>%
  select_if(is.factor) %>%
  dummy_columns(remove_selected_columns = TRUE, 
                remove_first_dummy = TRUE)

colnames(dummys) <- gsub("_", "", colnames(dummys)) # quitamos "_" para que se parezca a las variables del modelo

testSet.diabetes.dummy <- testSet.diabetes %>% 
  select(-is.factor) %>%
  cbind(dummys)

testSet.diabetes.dummy$DiabetesbinaryDiabetic <- factor(testSet.diabetes.dummy$DiabetesbinaryDiabetic, levels= c(0,1), labels = c("Healthy", "Diabetic"))

colnames(testSet.diabetes.dummy)
```

Ahora que he ajustado los conjuntos de datos para asegurar la consistencia en las variables, puedo proceder a realizar predicciones sobre el conjunto de prueba y evaluar la precisión de cada modelo. Para ello, genero las matrices de confusión de los cuatro modelos y analizo las métricas derivadas de estas para determinar cómo cada modelo predice los resultados de manera efectiva. Esta comparativa nos permite visualizar y entender mejor el desempeño de cada modelo en términos de sus errores.

```{r, eval=FALSE}
# Predicción de los datos de prueba y sus matrices de confusión
predict.rf.model <- predict(object = rf.cv.10$`2000 trees`$finalModel, newdata= testSet.diabetes.dummy)

predict.svm.linear.model <- predict(svm.linear.model, newdata = testSet.diabetes.processed)

predict.svm.radial.model <- predict(svm.radial.model, newdata = testSet.diabetes.processed)

predict.svm.poly.model <- predict(svm.poly.model, newdata = testSet.diabetes.processed)

predicciones2 <- list(predict.rf.model = predict.rf.model, predict.svm.linear.model = predict.svm.linear.model, predict.svm.radial.model = predict.svm.radial.model, predict.svm.poly.model = predict.svm.poly.model)

saveRDS(predicciones2, "predicciones2.RDS")
```

```{r, warning=FALSE}
predicciones2 <- readRDS("predicciones2.RDS")
predict.rf.model <- predicciones2$predict.rf.model
predict.svm.linear.model <- predicciones2$predict.svm.linear.model
predict.svm.radial.model <- predicciones2$predict.svm.radial.model
predict.svm.poly.model <- predicciones2$predict.svm.poly.model

conf.rf.model <- confusionMatrix(predict.rf.model, testSet.diabetes.dummy$DiabetesbinaryDiabetic, positive = "Diabetic")

conf.svm.linear.model.test <- confusionMatrix(predict.svm.linear.model, as.factor(testSet.diabetes.processed$Diabetes_binary_Diabetic), positive = "Diabetic")

conf.svm.radial.model.test <- confusionMatrix(predict.svm.radial.model, as.factor(testSet.diabetes.processed$Diabetes_binary_Diabetic), positive = "Diabetic")

conf.svm.poly.model.test <- confusionMatrix(predict.svm.poly.model, as.factor(testSet.diabetes.processed$Diabetes_binary_Diabetic), positive = "Diabetic")

```

Genero el dataframe para visualizar las métricas predictivas.

```{r}
# dataframe con las métricas de las matrices de confusión
df.testdata.metrics <- data.frame(Accuracy = c(conf.rf.model$overall['Accuracy'], conf.svm.linear.model.test$overall['Accuracy'],conf.svm.radial.model.test$overall['Accuracy'],conf.svm.poly.model.test$overall['Accuracy']),
                          
                          Kappa = c(conf.rf.model$overall['Kappa'], conf.svm.linear.model.test$overall['Kappa'],conf.svm.radial.model.test$overall['Kappa'],conf.svm.poly.model.test$overall['Kappa']),
                          
                          Sensitivity = c(conf.rf.model$byClass['Sensitivity'], conf.svm.linear.model.test$byClass['Sensitivity'],conf.svm.radial.model.test$byClass['Sensitivity'],conf.svm.poly.model.test$byClass['Sensitivity']),
                          Specificity = c(conf.rf.model$byClass['Specificity'], conf.svm.linear.model.test$byClass['Specificity'], conf.svm.radial.model.test$byClass['Specificity'], conf.svm.poly.model.test$byClass['Specificity']),
                          
                          Model = c("RF", "SVM linear", "SVM Radial", "SVM Polinomial")
           )
```

Como se observa en la gráfica, los cuatro modelos presentan un desempeño similar, con métricas predictivas robustas (Accuracy ≈ 0.75, Kappa ≈ 0.5, Sensitivity ≈ 0.8 y Specificity ≈ 0.7). 

```{r}
df.long <- tidyr::pivot_longer(df.testdata.metrics, 
                               cols = c("Accuracy", "Kappa","Sensitivity", "Specificity"), 
                               names_to = "Metric", 
                               values_to = "Value")


ggplot(df.long, aes(x = Model, y = Value, color = Metric)) +
  geom_point(size = 3) +  
  scale_color_manual(values = c("Accuracy" = "#17becf", "Kappa" =  "#ff7f0e" ,"Sensitivity" = "#2ca02c", "Specificity" = "#9467bd"),
                     labels = c("Sensitivity" = "Sensitivity", "Specificity" = "Specificity")) +
  labs(x = "Model", y = "Value", title = "Metrics in test predictions",
       color = "Metric") +  
  theme_minimal() + 
  theme(legend.position = "right") 
```

### Curvas ROC

La curva ROC es una herramienta gráfica crucial utilizada para evaluar la capacidad predictiva de un modelo de clasificación a lo largo de varios umbrales de decisión. Esta curva traza la tasa de verdaderos positivos (Sensibilidad) contra la tasa de falsos positivos (1 - Especificidad) para diferentes umbrales de clasificación, proporcionando una representación visual de la relación entre ambos. El AUC es una métrica integral derivada de la curva ROC, que mide la capacidad total del modelo para discriminar entre las clases positivas y negativas. Un AUC de 1.0 representa un modelo perfecto que clasifica correctamente todas las positivas y negativas, mientras que un AUC de 0.5 indica un rendimiento no mejor que el azar.

Vamos a ver cual es el mejor modelo utilizando la métrica de area bajo la curva ya que es un problema de clasificación. Para ello lanzo en dayhoff mediante el gestor de colas SLURM los modelos de Random Forest y los 3 SVM con los mejores hiperparámetros (no incluyo las lineas de codigo porque son muy parecidas a los anteriores). Incluyo en la función trainControl() de las lineas de codigo para entrenar los modelos lo siguiente: 

- summaryFunction = twoClassSummary,
- classProbs=TRUE

Tambien añado en la función train():
- metric = "ROC"

```{r}
rf.cv.10.modelo.probs <- readRDS("rf.cv.10.modelo.probs.RDS")
svm.linear.model.probs <- readRDS("svm.linear.model.probs.RDS")
svm.radial.model.probs <- readRDS("svm.radial.model.probs.RDS")
svm.poly.model.probs <- readRDS("svm.poly.model.probs.RDS")
```

Vemos que las AUC de las 4 curvas ROC son extremadamente parecidas. Los kernels no lineales tienen un poco mejor de AUC que los demás con un AUC de 0,84.

```{r, eval=FALSE}
# predecir el conjunto test
predictions_rf <- predict(rf.cv.10.modelo.probs, testSet.diabetes, type = "prob")[,2]
predictions_svm_linear <- predict(svm.linear.model.probs, testSet.diabetes.processed, type = "prob")[,1]
predictions_svm_radial <- predict(svm.radial.model.probs, testSet.diabetes.processed, type = "prob")[,1]
predictions_svm_poly <- predict(svm.poly.model.probs, testSet.diabetes.processed, type = "prob")[,1]

predicciones3 <- list(predictions_rf = predictions_rf, predictions_svm_linear = predictions_svm_linear, predictions_svm_radial =predictions_svm_radial, predictions_svm_poly =predictions_svm_poly)

saveRDS(predicciones3, "predicciones3.RDS")
```

```{r}
predicciones3 <- readRDS("predicciones3.RDS")
predictions_rf <- predicciones3$predictions_rf
predictions_svm_linear <- predicciones3$predictions_svm_linear
predictions_svm_radial <- predicciones3$predictions_svm_radial
predictions_svm_poly <- predicciones3$predictions_svm_poly

# Calcular las curvas ROC
roc_rf <- roc(testSet.diabetes$Diabetes_binary, predictions_rf)
roc_svm_linear <- roc(testSet.diabetes.processed$Diabetes_binary_Diabetic, predictions_svm_linear)
roc_svm_radial <- roc(testSet.diabetes.processed$Diabetes_binary_Diabetic[], predictions_svm_radial)
roc_svm_poly <- roc(testSet.diabetes.processed$Diabetes_binary_Diabetic, predictions_svm_poly)

# Áreas bajo la curva
auc_rf <- auc(roc_rf)
auc_svm_linear <- auc(roc_svm_linear)
auc_svm_radial <- auc(roc_svm_radial)
auc_svm_poly <- auc(roc_svm_poly)
```

```{r, fig.height=5, fig.width=7, warning=FALSE}
# Convertir a data frames para ggplot
df_rf <- data.frame(TPR = roc_rf$sensitivities, FPR =  roc_rf$specificities, Model = 'Random Forest')
df_svm_linear <- data.frame(TPR = roc_svm_linear$sensitivities, FPR =  roc_svm_linear$specificities, Model = 'SVM Linear')
df_svm_radial <- data.frame(TPR = roc_svm_radial$sensitivities, FPR =  roc_svm_radial$specificities, Model = 'SVM Radial')
df_svm_poly <- data.frame(TPR = roc_svm_poly$sensitivities, FPR =  roc_svm_poly$specificities, Model = 'SVM Poly')

# Combining data
roc_data <- rbind(df_rf, df_svm_linear, df_svm_radial, df_svm_poly)

# Poner las AUC en su gráfica con facet_wrap
roc_data$AUC <- case_when(
  roc_data$Model == "Random Forest" ~ sprintf("AUC RF: %.2f", auc_rf),
  roc_data$Model == "SVM Linear" ~ sprintf("AUC SVM Linear: %.2f", auc_svm_linear),
  roc_data$Model == "SVM Radial" ~ sprintf("AUC SVM Radial: %.2f", auc_svm_radial),
  roc_data$Model == "SVM Poly" ~ sprintf("AUC SVM Poly: %.2f", auc_svm_poly)
)

# Generamos el gráfico
ggplot(roc_data, aes(x = 1 - FPR, y = TPR, color = Model, group = Model)) +
  geom_line(size = 0.5) +  
  scale_color_manual(values = c("Random Forest" = "#377eb8", 
                                "SVM Linear" = "#4daf4a",
                                "SVM Radial" = "#ff7f00",
                                "SVM Poly" = "#984ea3")) +
  labs(title = "Comparative ROC Curves",
       x = "False Positive Rate (1 - Specificity)",
       y = "True Positive Rate (Sensitivity)") +
  theme_minimal() +
  theme(legend.position = "right") +
  geom_text(aes(x = 0.3, y = 0.2, label = AUC), hjust = 0, vjust = 0) +
  facet_wrap(~Model)

```

## Conclusiones de la comparación

Aunque las métricas de todos los modelos son comparables, la Sensitivity es particularmente crítica en contextos médicos dado que representa la capacidad del modelo para identificar correctamente a los individuos enfermos. En cambio, la Specificity, que mide la habilidad del modelo para reconocer a los individuos sanos, es menos crítica en este escenario.

Dado este análisis, optaría por el modelo SVM con kernel polinomial o radial, ya que ambos muestran una Sensitivity superior en comparación con los otros dos modelos. Estos serían más adecuados para aplicaciones médicas donde es vital minimizar los falsos negativos.

# Análisis no supervisado

El algoritmo de k-means es un método de clustering o agrupamiento que se utiliza para dividir un conjunto de datos en grupos o clusters distintos. Su objetivo es agrupar los datos de tal manera que los puntos dentro de cada cluster sean lo más similares posible entre sí, mientras que los puntos en diferentes clusters sean lo más diferentes posible. La "similitud" generalmente se mide en términos de distancia euclidiana entre puntos en el espacio de características de los datos.

Inicialmente, se seleccionan puntos del conjunto de datos como los centroides iniciales de los clusters. Esta selección puede ser aleatoria o basada en algún criterio específico. Luego, se asigna cada punto del conjunto de datos al cluster cuyo centroide sea el más cercano, donde la "cercanía" se determina generalmente por la distancia euclidiana entre el punto y el centroide. Una vez que todos los puntos han sido asignados a algún cluster, se recalcula la posición de cada centroide como el promedio de todos los puntos que han sido asignados a ese cluster.

El proceso se repite, alternando entre la asignación de puntos a clusters y la actualización de centroides, hasta que los centroides ya no cambien significativamente entre iteraciones, lo que indica que el algoritmo ha convergido.

Para este conjunto de datos voy a realizar una reducción de la dimensionalidad de los datos mediante UMAP. El UMAP Es una técnica de reducción de dimensionalidad que se puede utilizar tanto para visualización como para mejorar el rendimiento en tareas de machine learning. 

## UMAP

Como el número de muestras es muy grande, he decidido poner un número de vecinos de 50 para el análisis de la distancia euclidiana.

```{r, eval=FALSE}
local.config <- umap.defaults
local.config$n_components <- 2
local.config$n_neighbors <- 50
local.config$metric<- "euclidean"
set.seed(1234)

diabetes.umap <- umap(diabetes.processed[,-1],random_stage=1234, local.config)

saveRDS(diabetes.umap, "diabetes.umap.RDS")
```

Como se observa en la representación de UMAP, se forman diversos subgrupos de individuos dentro del conjunto de datos analizado.

```{r}
diabetes.umap <- readRDS("diabetes.umap.RDS")
umap.data <- as.data.frame(diabetes.umap$layout)

ggplot(umap.data, aes(x=V1, y=V2)) +  
  geom_point(size=2,  alpha = 0.01, color = "#8B1A1A") +
  guides(colour=guide_legend(override.aes=list(size=6))) +
  xlab("") + ylab("") +
  ggtitle("UMAP") +
  theme_light(base_size=20) +
  scale_colour_brewer(palette = "Set2")

```

## Determinación de número de clusters

Determinar el número correcto de clusters (k) es crucial para obtener resultados significativos, y se requiere métodos adicionales como el método del codo o el indice silhouette. Voy a hacer una partición de los datos debido alto coste computacional y que me da error al calcular el indice silhouette ya que no soporta tamaños tan grandes. 

Para tener una correcta distribución de sanos y diabéticos, junto los datos de las variables junto con los datos obtenidos del UMAP. De esta forma puedo hacer la partición de los datos sin desbalance entre clases de salida. Hago una partición del 70% de los datos.

```{r}
diabetes.processed.umap <- diabetes.processed %>% 
  mutate(umap1 = diabetes.umap$layout[,1], umap2 = diabetes.umap$layout[,2])

set.seed(1234)

diabetes.clustering.umap <- createDataPartition(diabetes.processed.umap$Diabetes_binary_Diabetic,
                                       p=0.7, 
                                       list = FALSE, 
                                       times = 1) 

data.clustering.diabetes.umap <- diabetes.processed.umap[diabetes.clustering.umap, ]
table(data.clustering.diabetes.umap[,1])
```

La regla del codo es una técnica heurística utilizada para determinar el número óptimo de clusters en métodos de clustering como k-means. La idea básica detrás de la regla del codo es identificar el punto en el cual añadir más clusters no proporciona un beneficio significativo en términos de la suma de los cuadrados dentro de los clusters. 

Para ello realizo un bucle en el cual se realizan kmeans con distintos k clusters. Se recogen los "tot.withinss" de cada pasada de kmeans con un k distinto. Esto datos representa la "suma total de cuadrados dentro de los grupos" (Total Within-Cluster Sum of Squares). Es la suma de las distancias al cuadrado entre cada punto de datos y el centroide de su cluster. En otras palabras, es una medida de la variabilidad interna de los clusters. Para un cluster individual, se calcula como la suma de las distancias al cuadrado de todos los puntos dentro del cluster hasta el centroide del cluster.

```{r, eval=FALSE}
set.seed(1234) 

buscandoK <- data.frame(k = "", tot.withinss = "")

for (k in 2:20) {
  diabetes.kmeans <- kmeans(data.clustering.diabetes.umap[,c("umap1", "umap2")], centers=k, iter.max=100, nstart = 25)

  buscandoK <- rbind(buscandoK, c(k, diabetes.kmeans$tot.withinss))
}
saveRDS(buscandoK, "buscandoK.RDS")
```

```{r, warning=FALSE, fig.size = 5, fig.height=5}
buscandoK <- readRDS("buscandoK.RDS")

buscandoK$tot.withinss <- as.numeric(buscandoK$tot.withinss)
buscandoK$k <- as.numeric(buscandoK$k)
buscandoK <- buscandoK[-1,]

ggplot(buscandoK, aes(x =k , y = tot.withinss))+
  geom_point(color = "#79CDCD", size = 3) +
  geom_line(size = 1, color = "#79CDCD") +
  theme_minimal()
```

Otro método para predecir el mejor k cluster en kmeans es el índice Silhouette que proporciona una medida de lo bien que está cada objeto asignado a su cluster. Se escoge el k que preste el indice Silhouette mayor. En este caso es 19 clusters. 

```{r, eval=FALSE}
set.seed(1234) 
buscandoSIL <- data.frame(k = "", median = "")

for (k in 2:20) {

diabetes.kmeans <- kmeans(data.clustering.diabetes.umap[,c("umap1", "umap2")], centers = k, iter.max = 100, nstart = 25)
  
  dist_matrix <- dist(data.clustering.diabetes.umap[,c("umap1", "umap2")])
  
  sil_k <- silhouette(diabetes.kmeans$cluster, dist_matrix)
  summ_sil_k <- summary(sil_k)
  
  buscandoSIL <- rbind(buscandoSIL, c(k, summ_sil_k$avg.width))
}

saveRDS(buscandoSIL, "buscandoSIL.RDS")
```

```{r}
buscandoSIL <- readRDS("buscandoSIL.RDS")

buscandoSIL$median <- as.numeric(buscandoSIL$median)
buscandoSIL$k <- as.numeric(buscandoSIL$k)
buscandoSIL <- buscandoSIL[-1,]

ggplot(buscandoSIL, aes(x =k , y = median))+
  geom_point(color = "#79CDCD", size = 3) +
  geom_line(size = 1, color = "#79CDCD") +
  theme_minimal()+ 
  labs(y = "Silhouette width")
```

## K-means

Ahora aplicaremos el método k-means a los datos procesados con UMAP, utilizando hiperparámetros optimizados para afinar más precisamente la identificación de los clusters. Para el número de clusters he elegido el expuesto en el análisis de silhouette ya que la regla del codo es dificil de apreciarlo. Por lo tanto, utilizo 19 centros.

```{r, warning=FALSE}
set.seed(1234)
km.umap <- kmeans(diabetes.processed.umap[,c("umap1", "umap2")], nstart = 50, iter.max = 300, centers = 19)

diabetes.processed.umap$kmeans <- factor(km.umap$cluster)
km.cent <- diabetes.processed.umap %>% 
  group_by(kmeans) %>% 
  select(umap1, umap2) %>% 
  summarize_all(mean)
```

Como vemos en la figura, se diferencian bien los grupos, aunque se debería afinar con otros métodos de clusterización.

```{r, warning=FALSE}
ggplot(diabetes.processed.umap, aes(x = umap1, y = umap2, colour = kmeans)) + 
  geom_point(alpha = 0.3) + 
  theme_bw()  + 
  geom_label_repel(aes(label = kmeans), data = km.cent, nudge_x = 0.2, color = "black", ) + 
  guides(colour = FALSE)+ 
  labs(title = "UMAP Projection with K-means Clustering")
```

Por último, voy a buscar los clusters más interesantes para estudiar. Para ello calculo los números totales de diabéticos y sanos de cada cluster para ver cual de ellos tiene un mayor número de diabéticos o de sanos, para ver qué variables son las que más definen esos clusters.

Como vemos el cluster número 13 contiene más del doble de individuos diabéticos que sanos por lo que se esperaria que esos individuos tengan unas características propensas para tener la enfermedad. En cambio, el cluster 14 tiene un porcentaje muy bajo de diabéticos, lo que cabe esperar que sean individuos con una vida saludable.

```{r, fig.height=5, fig.width=5}
cluster_summary <- diabetes.processed.umap %>%
  group_by(kmeans, Diabetes_binary_Diabetic) %>%
  summarize(n = n(), .groups = 'drop') %>%
  pivot_wider(names_from = Diabetes_binary_Diabetic, values_from = n, values_fill = list(n = 0)) %>%
  mutate(Proportion = Diabetic / Healthy) %>%
  arrange(desc(Proportion))

cluster_summary$kmeans <- factor(cluster_summary$kmeans, levels = unique(cluster_summary$kmeans))

ggplot(cluster_summary, aes(x = factor(kmeans), y = Proportion, fill = factor(kmeans))) +
  geom_bar(stat = "identity") +
  labs(title = "Diabetics vs Healthy per Cluster",
       x = "Cluster",
       y = "Diabetics vs Healthy",
       fill = "Cluster") +
  scale_fill_viridis_d()+
  theme_minimal() +
  geom_hline(yintercept = 1) +
  annotate("text", x = 8, y = 1.8, label = "Diabetic", color = "#8B1A1A", size = 8) +
  annotate("text", x = 16, y = 0.8, label = "Healthy", color = "#1A8B1A", size = 8) 
```
